# Product Requirements Document: Doc Server

## Executive Summary

The Doc Server is a comprehensive documentation platform that provides AI assistants with semantic search capabilities across diverse technical documentation sources. Originally designed for Rust crate documentation, it has evolved into a multi-type documentation server supporting infrastructure tools, blockchain platforms, and programming resources.

## Current State (As of Handoff)

### âœ… Implemented Infrastructure
- **Database**: `docs` database with harmonized schema (migrated from `rust_docs_vectors`)
- **Content**: 4,000+ documents with embeddings across 3 documentation types
- **Architecture**: PostgreSQL with pgvector, Rust MCP server, Docker development environment
- **Working Tools**: `rust_query` tool implemented and tested in Cursor MCP
- **Development Environment**: One-command setup with full database restoration

### âœ… Documentation Content
- **Rust Crates**: 40+ crates with complete documentation and embeddings
- **BirdEye API**: 600+ endpoints with OpenAPI specifications
- **Solana Documentation**: 400+ documents including core docs, architecture diagrams, ZK cryptography PDFs

### âœ… Technical Implementation
- **MCP Server**: HTTP/SSE server on port 3001 with Toolman integration
- **Database Schema**: Unified tables supporting all planned documentation types
- **Docker Setup**: Complete development environment with PostgreSQL on port 5433
- **Database Dump**: 67MB compressed dump for instant restoration

## Target State

### Core Functionality
A unified documentation server providing:
- **Query Tools**: 10 type-specific query tools for precise documentation access
- **Management Tools**: Dynamic Rust crate management via MCP
- **Connection Reliability**: SSE keep-alive for stable Toolman integration  
- **Cost Efficiency**: 70% embedding cost reduction via OpenAI batching
- **Developer Experience**: One-command development environment setup

### Supported Documentation Types
1. âœ… **Rust Crates** (implemented) - Dynamic management and querying
2. âœ… **BirdEye API** (ingested) - Blockchain API documentation
3. âœ… **Solana** (ingested) - Blockchain platform documentation
4. ðŸ”„ **Jupyter Notebooks** (planned) - Interactive notebook documentation
5. ðŸ”„ **Cilium** (planned) - Networking and security documentation
6. ðŸ”„ **Talos** (planned) - Linux distribution documentation
7. ðŸ”„ **Meteora** (planned) - DEX protocol documentation
8. ðŸ”„ **Raydium** (planned) - DEX protocol documentation
9. ðŸ”„ **eBPF** (planned) - Extended Berkeley Packet Filter documentation
10. ðŸ”„ **Rust Best Practices** (planned) - Language best practices guide

## Functional Requirements

### 1. Query Tools (Primary MCP Interface)

Each documentation type must have its own specific query tool for optimal relevance:

#### âœ… Implemented
- `rust_query` - Query Rust crate documentation with semantic search

#### ðŸ”„ Planned Implementation  
- `birdeye_query` - Query BirdEye blockchain API documentation
- `solana_query` - Query Solana blockchain platform documentation
- `jupyter_query` - Query Jupyter notebook documentation
- `cilium_query` - Query Cilium networking/security documentation
- `talos_query` - Query Talos Linux documentation
- `meteora_query` - Query Meteora DEX documentation
- `raydium_query` - Query Raydium DEX documentation
- `ebpf_query` - Query eBPF documentation
- `rust_best_practices_query` - Query Rust best practices guide

#### Tool Requirements
- **Type-specific naming**: Clear indication of available documentation
- **Semantic search**: Vector similarity search with embeddings
- **Metadata filtering**: Use JSONB metadata for refined results
- **Response formatting**: Structured responses with source attribution

### 2. Management Tools (Rust Only)

Only Rust crates support dynamic management via MCP:
- `add_rust_crate` - Add new Rust crate with automatic documentation extraction
- `remove_rust_crate` - Remove Rust crate and associated documentation
- `list_rust_crates` - List available crates with status information
- `check_rust_status` - Check population status and health of Rust documentation

### 3. Connection Reliability (Priority)

#### Current State
- Basic HTTP/SSE server implementation
- Health check endpoint functional
- Cursor MCP integration working

#### Required Implementation
- **SSE Keep-Alive**: Heartbeat messages every 30 seconds
- **Timeout Detection**: Connection timeout detection at 90 seconds
- **Auto-Reconnection**: Client-side reconnection with exponential backoff
- **Message Buffering**: Buffer messages during disconnection periods
- **Health Monitoring**: Enhanced health endpoints for system monitoring

### 4. Data Management

#### Current Implementation
- âœ… Harmonized schema supporting multiple documentation types
- âœ… JSONB metadata for type-specific information
- âœ… OpenAI text-embedding-3-large embeddings (3072 dimensions)
- âœ… Complete migration from original Rust-only schema

#### Required Enhancements
- **Batch Processing**: OpenAI API batching for cost optimization
- **Rate Limiting**: Compliance with OpenAI rate limits (3000 RPM / 1M TPM)
- **Vector Indexing**: Optimize search performance as data grows
- **Cross-Type Search**: Unified search across all documentation types

## Technical Requirements

### Database Schema (Implemented)

```sql
-- Primary documents table (implemented)
CREATE TABLE documents (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    doc_type VARCHAR(50) NOT NULL CHECK (doc_type IN (
        'rust', 'jupyter', 'birdeye', 'cilium', 'talos', 
        'meteora', 'solana', 'ebpf', 'raydium', 'rust_best_practices'
    )),
    source_name VARCHAR(255) NOT NULL,
    doc_path TEXT NOT NULL,
    content TEXT NOT NULL,
    metadata JSONB DEFAULT '{}',
    embedding vector(3072), -- OpenAI text-embedding-3-large
    token_count INTEGER,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(doc_type, source_name, doc_path)
);

-- Source configuration table (implemented)
CREATE TABLE document_sources (
    id SERIAL PRIMARY KEY,
    doc_type VARCHAR(50) NOT NULL,
    source_name VARCHAR(255) NOT NULL,
    config JSONB NOT NULL DEFAULT '{}',
    enabled BOOLEAN DEFAULT true,
    last_checked TIMESTAMPTZ,
    last_populated TIMESTAMPTZ,
    created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(doc_type, source_name)
);
```

### Performance Requirements

#### Current Performance
- Query response time: ~2-3 seconds (no vector index due to dimension limits)
- Concurrent connections: Tested with basic load
- Memory usage: Moderate with current dataset

#### Target Performance
- **Query Response**: < 2 seconds for semantic search
- **Concurrent Connections**: Support 100+ simultaneous connections
- **Batch Processing**: 100 embeddings per OpenAI request
- **Rate Limiting**: Compliant with OpenAI limits (3000 RPM / 1M TPM)
- **Uptime**: 99.9% availability target

### Integration Requirements

#### Current Integrations
- âœ… **Toolman**: Primary client with HTTP/SSE transport
- âœ… **Cursor**: MCP integration working
- âœ… **PostgreSQL**: Database with pgvector extension
- âœ… **OpenAI**: Embedding generation and LLM queries

#### Required Enhancements
- **Docker**: Production-ready containerization
- **Monitoring**: Health checks and system metrics
- **Logging**: Structured logging with tracing
- **Error Handling**: Comprehensive error recovery

## Implementation Priorities

### Phase 1: System Evaluation (Week 1)
**Priority**: Immediate (Task 35)
- Comprehensive system assessment by new implementation agent
- Verify all completed functionality
- Test development environment and Docker setup
- Validate database restoration and query tools
- Document current state and identify gaps

### Phase 2: Connection Reliability (Week 2)
**Priority**: High (Task 17)
- Implement SSE keep-alive mechanism
- Add connection timeout detection and recovery
- Enhance health monitoring endpoints
- Test with Toolman under various network conditions

### Phase 3: Query Tool Implementation (Week 3-4)
**Priority**: Medium (Tasks 20-25)
- Implement `birdeye_query` and `solana_query` tools
- Complete Rust crate management tools
- Add remaining query tools for all documentation types
- Integration testing with Cursor and Toolman

### Phase 4: Optimization (Week 5-6)
**Priority**: Medium (Tasks 21, 28, 29)
- OpenAI batch processing implementation
- Rate limiting and API protection
- Performance optimization and scaling
- Documentation ingestion pipeline automation

### Phase 5: Advanced Features (Week 7+)
**Priority**: Low (Tasks 26, 30)
- Unified search across all documentation types
- Comprehensive integration testing
- Production deployment preparation
- Monitoring and alerting implementation

## Success Criteria

### Completed Milestones âœ…
1. âœ… All 40 existing Rust crates remain searchable after migration
2. âœ… Zero data loss during schema migration
3. âœ… Working MCP server with HTTP/SSE transport
4. âœ… Development environment with one-command setup
5. âœ… BirdEye and Solana documentation successfully ingested

### Remaining Success Criteria
1. **Connection Reliability**: 90% reduction in connection timeouts
2. **Cost Optimization**: 70% cost reduction via OpenAI batching
3. **Query Tools**: All 10 documentation types with working query tools
4. **Performance**: Query performance within 10% of original system
5. **Integration**: Stable Toolman integration under production load

## Constraints

### Technical Constraints
1. **Database**: Must maintain PostgreSQL with pgvector for embeddings
2. **Embeddings**: OpenAI text-embedding-3-large (3072 dimensions)
3. **Vector Index**: No pgvector index due to 2000-dimension limit
4. **Dynamic Management**: Only Rust crates support MCP-based management
5. **Tool Naming**: Must use specific names, not generic terms

### Business Constraints
1. **Backward Compatibility**: Existing Rust functionality must be preserved
2. **Data Migration**: No data loss during any future schema changes
3. **Cost Management**: Optimize OpenAI API usage through batching
4. **Development Speed**: Prioritize connection reliability over new features

## Dependencies

### Current Dependencies âœ…
- âœ… PostgreSQL with pgvector extension
- âœ… OpenAI API access and authentication
- âœ… Docker and Docker Compose for development
- âœ… Rust toolchain and cargo workspace

### Additional Requirements
- Toolman deployment for production testing
- Monitoring infrastructure for production deployment
- CI/CD pipeline for automated testing and deployment

## Risks and Mitigation

### Technical Risks

1. **Connection Stability Risk**
   - **Risk**: Continued timeout issues with Toolman
   - **Mitigation**: Priority implementation of SSE keep-alive (Task 17)
   - **Status**: High priority for Phase 2

2. **Performance Risk**
   - **Risk**: Slower queries without vector indexing
   - **Mitigation**: Optimize query patterns and consider alternative indexing
   - **Status**: Monitor and optimize in Phase 4

3. **Cost Risk**
   - **Risk**: High OpenAI API costs without batching
   - **Mitigation**: Implement batch processing for 70% cost reduction
   - **Status**: Planned for Phase 4

### Business Risks

1. **Integration Risk**
   - **Risk**: Breaking changes affecting Toolman compatibility
   - **Mitigation**: Extensive testing with each change
   - **Status**: Continuous monitoring required

2. **Data Risk**
   - **Risk**: Data loss during future schema modifications
   - **Mitigation**: Comprehensive backup strategy and staging environment
   - **Status**: Database dump created for preservation

## Future Considerations

### Extensibility
- Architecture supports addition of new documentation types
- Loader pattern allows custom ingestion for various formats
- JSONB metadata provides flexibility for type-specific data

### Scalability
- Horizontal scaling capability with proper load balancing
- Vector search optimization as dataset grows
- Caching layer for frequently accessed content

### Advanced Features
- Cross-documentation linking and relationships
- Multi-language support for international documentation
- Real-time documentation updates and synchronization
- Advanced query features (filtering, faceting, aggregation)

## Definition of Done

### For Each Query Tool
- [ ] Tool implemented and registered in MCP server
- [ ] Semantic search working with vector embeddings
- [ ] Metadata filtering and result ranking
- [ ] Integration tested with Cursor
- [ ] Performance benchmarked and optimized
- [ ] Documentation and examples provided

### For System Reliability
- [ ] SSE keep-alive implemented and tested
- [ ] Connection recovery working under network stress
- [ ] Health monitoring providing detailed system status
- [ ] Error handling graceful and informative
- [ ] Load testing completed with 100+ concurrent connections

### For Production Readiness
- [ ] Docker containers optimized for production
- [ ] Monitoring and alerting configured
- [ ] Security review completed
- [ ] Performance benchmarks documented
- [ ] Deployment automation working
- [ ] Backup and recovery procedures tested