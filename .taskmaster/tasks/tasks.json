{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Comprehensive System Assessment and Migration Planning",
        "description": "Perform a thorough evaluation of the existing Doc Server codebase, infrastructure, and database state. Identify gaps between current implementation and production requirements, particularly focusing on MCP transport migration needs.",
        "details": "Analyze existing Rust crates structure (database, mcp, embeddings, doc-loader, llm), verify database migration status, test existing rust_query tool functionality, examine current HTTP/SSE implementation against Streamable HTTP requirements (MCP 2025-06-18), validate Kubernetes deployment configuration in .github/workflows/deploy-doc-server.yml, check PostgreSQL cluster connectivity with pgvector extension, and document all findings with specific migration requirements. Use cargo test to validate existing test suite, examine transport.rs placeholder for implementation needs.",
        "testStrategy": "Execute comprehensive system tests including database connectivity tests, MCP server health checks, rust_query tool integration tests, GitHub Actions workflow dry runs, and document all test results in a system assessment report. Validate against production Kubernetes cluster accessibility.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 2,
        "title": "Implement Streamable HTTP Transport Foundation",
        "description": "Replace the deprecated HTTP+SSE transport with the new Streamable HTTP transport following MCP 2025-06-18 specification, creating the core transport layer infrastructure.",
        "details": "Implement transport.rs with full Streamable HTTP support using Axum 0.7. Create a unified MCP endpoint supporting both POST and GET methods. Implement proper JSON-RPC message handling with UTF-8 encoding. Add support for Server-Sent Events (SSE) streaming for multiple server messages. Ensure backward compatibility detection logic for clients attempting old HTTP+SSE transport. Reference .reference/transports.md for complete specification details. Use tower-http for CORS and tracing middleware integration.",
        "testStrategy": "Create integration tests in crates/mcp/tests/ validating POST/GET endpoints, SSE stream initialization and message delivery, JSON-RPC request/response cycles, backward compatibility with old transport attempts, and proper error handling for malformed requests. Test with both Cursor and Toolman clients.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Core HTTP Transport Module Structure",
            "description": "Set up the foundational transport module structure in transport.rs with necessary imports, types, and session management infrastructure following the Streamable HTTP specification.",
            "dependencies": [],
            "details": "Create the base transport.rs module with imports for axum 0.7, tower-http, serde_json, and async-stream. Define core types including TransportConfig, SessionManager, McpSession with session ID generation using uuid v4. Implement session storage with HashMap and Arc<RwLock> for thread-safe access. Add MCP-Protocol-Version and Mcp-Session-Id header constants. Create error types for transport-specific failures. Set up logging with tracing for debug and error messages.",
            "status": "pending",
            "testStrategy": "Create unit tests for session creation, storage, and retrieval. Test session ID uniqueness and cryptographic security. Verify thread-safe concurrent session access. Test session expiry and cleanup mechanisms."
          },
          {
            "id": 2,
            "title": "Implement Unified MCP Endpoint Handler",
            "description": "Create the main MCP endpoint handler supporting both POST and GET methods with proper content negotiation and request routing logic.",
            "dependencies": [
              "2.1"
            ],
            "details": "Implement unified_mcp_handler function in transport.rs accepting both POST and GET requests. Add Accept header validation for application/json and text/event-stream. Implement POST request handling for JSON-RPC requests, notifications, and responses with appropriate status codes (202 Accepted, 400 Bad Request). Add GET request handling for SSE stream initialization. Implement content type negotiation logic to determine response format. Add MCP-Protocol-Version header extraction and validation. Integrate session ID management from headers. Create request context structure for passing session and protocol information.",
            "status": "pending",
            "testStrategy": "Test POST requests with valid JSON-RPC messages and verify status codes. Test GET requests with Accept header validation. Verify content negotiation between JSON and SSE responses. Test invalid protocol version handling with 400 Bad Request. Test session ID header processing for both POST and GET."
          },
          {
            "id": 3,
            "title": "Implement SSE Streaming Infrastructure",
            "description": "Build Server-Sent Events streaming capability for handling multiple server messages and maintaining persistent connections.",
            "dependencies": [
              "2.2"
            ],
            "details": "Create SseStream struct using async-stream and tokio-stream for event generation. Implement SSE event formatting with proper data, event, and id fields per SSE specification. Add stream connection management with client tracking and cleanup on disconnect. Implement message queuing for buffering server messages before sending. Add event ID generation for stream resumability support. Create heartbeat mechanism using tokio intervals to keep connections alive. Implement proper UTF-8 encoding for JSON-RPC messages in SSE events. Add stream closing logic after sending JSON-RPC responses.",
            "status": "pending",
            "testStrategy": "Test SSE stream initialization and event formatting. Verify multiple message delivery in correct order. Test stream heartbeat and connection keepalive. Verify proper stream closure after response delivery. Test concurrent stream connections and message routing."
          },
          {
            "id": 4,
            "title": "Add Backward Compatibility Detection",
            "description": "Implement detection and handling logic for clients attempting to use the deprecated HTTP+SSE transport from protocol version 2024-11-05.",
            "dependencies": [
              "2.2",
              "2.3"
            ],
            "details": "Create legacy transport detection in unified handler by checking for missing MCP-Protocol-Version header. Implement fallback logic for old SSE endpoint detection when GET request expects endpoint event. Add compatibility response formatter for old transport format. Create protocol version negotiation helper functions. Implement proper error responses (405 Method Not Allowed, 404 Not Found) for legacy detection flow. Add logging for backward compatibility mode activation. Create configuration option to enable/disable legacy support.",
            "status": "pending",
            "testStrategy": "Test detection of legacy clients missing protocol version header. Verify proper 405/404 responses trigger legacy mode. Test endpoint event emission for old SSE transport. Verify new clients with protocol headers work correctly. Test configuration toggle for legacy support."
          },
          {
            "id": 5,
            "title": "Integrate Transport with MCP Server",
            "description": "Wire the new Streamable HTTP transport into the existing MCP server infrastructure, replacing old endpoints and ensuring proper request flow.",
            "dependencies": [
              "2.1",
              "2.2",
              "2.3",
              "2.4"
            ],
            "details": "Update server.rs to use new unified transport handler from transport.rs. Replace existing /mcp POST endpoint with unified handler supporting both methods. Update /sse endpoint to redirect to unified handler or maintain for backward compatibility. Integrate McpHandler with new transport for JSON-RPC processing. Add transport configuration to McpServerState. Update CORS configuration for new endpoint requirements. Wire session management into server state. Add transport initialization in McpServer::new. Update router creation to use new transport endpoints.",
            "status": "pending",
            "testStrategy": "Test end-to-end request flow through new transport. Verify McpHandler integration with transport layer. Test CORS headers on new endpoints. Verify session persistence across requests. Test both Cursor and Toolman client compatibility."
          }
        ]
      },
      {
        "id": 3,
        "title": "Session Management and Security Implementation",
        "description": "Implement robust session management with Mcp-Session-Id headers and comprehensive security measures including Origin header validation and DNS rebinding protection.",
        "details": "Generate cryptographically secure session IDs using UUID v4 with proper entropy. Implement Mcp-Session-Id header handling in both requests and responses. Add session storage using in-memory HashMap with TTL support (consider Redis for production scaling). Validate Origin headers to prevent DNS rebinding attacks. Ensure localhost binding (127.0.0.1) for local deployments. Implement session expiry with HTTP 404 responses. Add DELETE endpoint support for explicit session termination. Use chrono for timestamp management and uuid crate for ID generation.",
        "testStrategy": "Test session creation during initialization, header propagation across requests, session expiry and cleanup, Origin header validation with various attack vectors, concurrent session handling, and proper error responses for invalid sessions. Validate security measures with penetration testing tools.",
        "priority": "high",
        "dependencies": [
          2
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Session Module and Data Structures",
            "description": "Implement a dedicated session management module with secure UUID v4 generation and session data structures including TTL support",
            "dependencies": [],
            "details": "Create a new `session.rs` module in `/crates/mcp/src/` that defines the Session struct with fields for session_id (UUID v4), created_at (DateTime<Utc>), last_accessed (DateTime<Utc>), ttl (Duration), and associated client data. Implement session ID generation using the uuid crate with proper v4 entropy. Create a SessionManager struct that wraps a thread-safe HashMap (Arc<RwLock<HashMap<String, Session>>>) for in-memory storage. Add methods for create_session(), get_session(), update_last_accessed(), and cleanup_expired_sessions(). Include a comment noting Redis as a future production scaling option.",
            "status": "pending",
            "testStrategy": "Write unit tests for UUID v4 generation uniqueness, session creation with proper timestamps, TTL calculation and expiry logic, thread-safe concurrent access to SessionManager, and proper cleanup of expired sessions"
          },
          {
            "id": 2,
            "title": "Implement Security Validation Layer",
            "description": "Add Origin header validation and DNS rebinding protection with localhost binding enforcement for local deployments",
            "dependencies": [],
            "details": "Create a `security.rs` module in `/crates/mcp/src/` implementing middleware for Origin header validation. Define allowed origins configuration (defaulting to localhost variants). Implement validate_origin() function that checks incoming Origin headers against allowed list and prevents DNS rebinding attacks. Add localhost binding validation in http_server.rs to ensure server binds to 127.0.0.1 instead of 0.0.0.0 for local deployments. Create security configuration struct with options for strict mode, allowed origins list, and custom validation rules. Integrate security checks as Axum middleware layer.",
            "status": "pending",
            "testStrategy": "Test Origin validation with various attack vectors including DNS rebinding attempts, verify localhost binding enforcement, test with missing/malformed Origin headers, validate CORS configuration compatibility, and test with both allowed and denied origins"
          },
          {
            "id": 3,
            "title": "Add Mcp-Session-Id Header Handling",
            "description": "Implement bidirectional Mcp-Session-Id header processing in HTTP requests and responses following MCP specification",
            "dependencies": [
              "3.1",
              "3.2"
            ],
            "details": "Modify the SSE handler in transport.rs to extract Mcp-Session-Id from request headers and validate session existence. Update mcp_handler in server.rs to check for session headers and attach to response. Implement session propagation through the request lifecycle using Axum's State and Extension mechanisms. Add header injection in all HTTP responses including SSE streams. Handle missing session scenarios by creating new sessions during initialization. Ensure proper header casing (Mcp-Session-Id) per MCP spec.",
            "status": "pending",
            "testStrategy": "Test header extraction from various HTTP methods (GET/POST), verify header propagation in SSE streams, test session creation on missing headers, validate header persistence across multiple requests, and test concurrent requests with different session IDs"
          },
          {
            "id": 4,
            "title": "Implement Session Lifecycle Management",
            "description": "Add session expiry handling with HTTP 404 responses and DELETE endpoint for explicit session termination",
            "dependencies": [
              "3.1",
              "3.3"
            ],
            "details": "Implement background task using tokio::spawn for periodic session cleanup based on TTL. Add session expiry check middleware that returns HTTP 404 for expired sessions before processing requests. Create DELETE /session/{id} endpoint in server.rs for explicit session termination. Implement graceful cleanup of associated resources when sessions expire or are deleted. Add session renewal logic on valid requests to update last_accessed timestamp. Configure default TTL (30 minutes) with environment variable override option.",
            "status": "pending",
            "testStrategy": "Test automatic expiry after TTL timeout, verify 404 responses for expired sessions, test DELETE endpoint functionality, validate resource cleanup on termination, test session renewal on activity, and verify concurrent deletion handling"
          },
          {
            "id": 5,
            "title": "Integration Testing and Security Audit",
            "description": "Comprehensive integration testing of session management with security validation and performance benchmarking",
            "dependencies": [
              "3.1",
              "3.2",
              "3.3",
              "3.4"
            ],
            "details": "Create integration tests in `/crates/mcp/tests/session_integration_tests.rs` covering full session lifecycle from creation to expiry. Test security measures including Origin validation, DNS rebinding prevention, and session hijacking attempts. Benchmark concurrent session handling performance with 1000+ simultaneous sessions. Validate proper error responses and status codes for all edge cases. Test SSE connection handling with session management. Document security considerations and deployment guidelines in a SECURITY.md file.",
            "status": "pending",
            "testStrategy": "Run penetration testing tools against endpoints, test with various client libraries for compatibility, validate memory usage under load, test session persistence across server restarts (future Redis implementation), and verify audit logging of security events"
          }
        ]
      },
      {
        "id": 4,
        "title": "Protocol Version Negotiation and Headers",
        "description": "Implement MCP protocol version negotiation and header compliance for proper client-server communication and backward compatibility.",
        "details": "Add MCP-Protocol-Version header parsing and validation. Support protocol versions 2025-06-18 (current), 2025-03-26 (fallback), and 2024-11-05 (legacy). Implement version negotiation during initialization phase. Return HTTP 400 for unsupported protocol versions. Add Accept header validation for application/json and text/event-stream. Implement proper Content-Type headers in responses. Store negotiated version in session state for subsequent requests.",
        "testStrategy": "Test version negotiation with various client versions, validate header presence and format, test fallback behavior for missing headers, verify proper error responses for invalid versions, and ensure backward compatibility with older clients.",
        "priority": "medium",
        "dependencies": [
          2
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Protocol Version Registry and Validator",
            "description": "Implement a centralized protocol version registry that defines supported MCP protocol versions (2025-06-18, 2025-03-26, 2024-11-05) with validation logic and version comparison utilities.",
            "dependencies": [],
            "details": "Create a new module protocol_version.rs in crates/mcp/src/. Define a ProtocolVersion enum with variants for each supported version (V2025_06_18, V2025_03_26, V2024_11_05). Implement FromStr trait for parsing version strings from headers. Add version comparison methods for determining compatibility. Create a ProtocolRegistry struct to manage supported versions with methods for checking if a version is supported, finding the best matching version, and determining fallback versions. Include constants for current, fallback, and legacy protocol versions. Add comprehensive error types for version-related failures.",
            "status": "pending",
            "testStrategy": "Test version string parsing for valid and invalid formats, validate version comparison logic between different versions, test registry lookup for supported and unsupported versions, verify fallback version selection algorithm, and ensure proper error messages for invalid versions."
          },
          {
            "id": 2,
            "title": "Implement HTTP Header Extraction and Validation",
            "description": "Create header extraction middleware for parsing and validating MCP-Protocol-Version and Accept headers from incoming HTTP requests using Axum extractors.",
            "dependencies": [
              "4.1"
            ],
            "details": "Create headers.rs module with custom Axum extractors. Implement McpProtocolVersionHeader extractor using TypedHeader or custom FromRequestParts implementation. Add AcceptHeader extractor to validate application/json and text/event-stream content types. Create ContentTypeValidator to ensure proper Content-Type headers in responses. Implement header validation logic that returns HTTP 400 Bad Request for invalid or missing required headers. Add header constants and helper functions for header manipulation. Integrate with tracing for logging header processing.",
            "status": "pending",
            "testStrategy": "Test header extraction with valid MCP-Protocol-Version values, verify rejection of requests with unsupported protocol versions, test Accept header validation for both JSON and SSE, verify proper 400 Bad Request responses for invalid headers, and test header parsing edge cases."
          },
          {
            "id": 3,
            "title": "Add Version Negotiation to Initialize Handler",
            "description": "Enhance the initialize request handler in handlers.rs to implement protocol version negotiation following the MCP specification lifecycle requirements.",
            "dependencies": [
              "4.1",
              "4.2"
            ],
            "details": "Modify handle_initialize method in McpHandler to extract and validate the client's requested protocol version from the initialize request params. Implement version negotiation logic: if server supports the requested version, respond with same version; otherwise respond with server's latest supported version. Store the negotiated protocol version in a session state structure. Update the InitializeResult response to include the negotiated protocolVersion field. Add validation to ensure the client-requested version is parseable and handle version mismatch scenarios. Log version negotiation decisions for debugging.",
            "status": "pending",
            "testStrategy": "Test version negotiation with matching client-server versions, test fallback when client requests unsupported version, verify server responds with its latest supported version when needed, test edge cases like missing version in request, and validate that negotiated version is stored in session."
          },
          {
            "id": 4,
            "title": "Implement Session State Management with Protocol Version",
            "description": "Create session state management system that stores negotiated protocol versions and enforces version consistency across subsequent requests.",
            "dependencies": [
              "4.3"
            ],
            "details": "Create session.rs module with SessionState struct containing negotiated_version field and session metadata. Implement SessionStore using Arc<RwLock<HashMap>> for thread-safe session storage keyed by session ID. Add session creation during initialization with generated UUID session ID. Implement session retrieval and validation for subsequent requests. Add session expiry mechanism with configurable TTL. Create middleware to extract session ID from Mcp-Session-Id header and attach session state to request context. Ensure all handlers have access to session state for version-aware processing.",
            "status": "pending",
            "testStrategy": "Test session creation with protocol version storage, verify session retrieval by ID, test session expiry and cleanup, validate thread-safe concurrent session access, test session validation on subsequent requests, and verify version consistency enforcement."
          },
          {
            "id": 5,
            "title": "Add Response Header Management and Version-Specific Formatting",
            "description": "Implement response header management to ensure proper Content-Type headers and version-specific response formatting based on negotiated protocol version.",
            "dependencies": [
              "4.2",
              "4.4"
            ],
            "details": "Create response.rs module with ResponseBuilder that adds appropriate headers based on request context. Implement Content-Type header setting for application/json and text/event-stream responses. Add Mcp-Session-Id header to responses during initialization. Create version-specific response formatters that adjust JSON structure based on negotiated protocol version. Implement middleware to automatically add required headers to all responses. Add support for protocol version downgrade warnings in responses. Ensure CORS headers are properly maintained alongside MCP-specific headers.",
            "status": "pending",
            "testStrategy": "Test Content-Type headers for JSON and SSE responses, verify Mcp-Session-Id header in initialization responses, test version-specific response formatting for each protocol version, validate header presence in all response types, and test integration with existing CORS middleware."
          }
        ]
      },
      {
        "id": 5,
        "title": "Streamable HTTP Response Enhancement and Optional SSE Support",
        "description": "Implement enhancements to the MCP 2025-06-18 Streamable HTTP transport so that servers can optionally upgrade a normal JSON response into an SSE stream when multiple messages need to be emitted. All functionality must remain fully compatible with the default POST/GET interaction model; SSE is a best-effort optimisation, not the primary mechanism.",
        "status": "pending",
        "dependencies": [
          3
        ],
        "priority": "medium",
        "details": "1. Unified Endpoint Integration\n   • Extend the existing /mcp endpoint (introduced in Task 2) to dynamically choose between a single JSON response body (default) and an SSE stream (optional) based on application logic.\n   • Negotiate the response mode using the request’s Accept header: clients MAY add both application/json and text/event-stream; the server chooses the final media type.\n\n2. Event & Replay Mechanics (SSE mode only)\n   • Generate monotonically increasing, globally unique event IDs (u64) scoped to the lifetime of the node.\n   • Maintain a per-connection ring-buffered history (configurable, default = 1 000 events) to enable selective replay.\n   • Honour the Last-Event-ID header on reconnection, replaying missed events in order; if the ID is too old (purged), start from the oldest retained event; if it is newer than the newest, respond with 204 No Content.\n   • Purge buffers automatically when a stream completes or after idle_timeout.\n\n3. Streaming Implementation\n   • Use tokio-stream + axum::response::Sse to push events efficiently.\n   • Convert JSON-RPC responses into properly formatted SSE events (fields: id, event: mcp, data).\n   • For single-message scenarios, fall back to a normal JSON response body (most common path).\n\n4. Connection Health\n   • Detect lost TCP connections via keep-alive probes and clean up associated resources.\n   • Emit periodic comment-style ping events (:\\n) every ping_interval to keep browser clients alive when in SSE mode.\n\n5. Configuration Surface (Config::streamable_http)\n   • buffer_size, idle_timeout, ping_interval, max_connection_age.\n\n6. Tracing & Metrics\n   • Counters/histograms: active_streams, replayed_events, dropped_events, buffer_occupancy.\n\nNOTE: Do not re-implement routing or generic JSON-RPC parsing—build on the foundation delivered in Task 2. SSE support must remain strictly optional; JSON behaviour for legacy clients must be preserved without behavioural change.",
        "testStrategy": "1. Functional\n   • Issue POST requests with Accept: application/json and verify the server returns a normal JSON body without SSE headers.\n   • Issue POST or GET with Accept: text/event-stream and a server configuration that triggers streaming; validate correct SSE framing, including id, event, and data fields.\n   • Simulate network drop, reconnect with Last-Event-ID=N, and assert the first replayed event has id N+1.\n   • Verify boundary handling when Last-Event-ID is older than earliest retained or newer than newest retained.\n\n2. Ordering & Uniqueness\n   • Stress-test with 1 000 000 sequential events across concurrent streams; assert strictly increasing IDs without gaps.\n\n3. Memory & Cleanup\n   • Soak test with 10 000 concurrent streams; measure RSS; ensure buffers are released after disconnection or idle_timeout.\n\n4. Compatibility\n   • Confirm behaviour remains 100 % unchanged for legacy clients that advertise only application/json.\n\n5. Concurrency & Isolation\n   • Mix streaming and non-streaming workloads; ensure no cross-talk between buffers; inspect with tokio-console for task lifetimes.",
        "subtasks": [
          {
            "id": 1,
            "title": "Add Accept header negotiation and SSE fallback logic to /mcp handler",
            "description": "",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement global event ID generator and per-stream ring buffer",
            "description": "",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Handle Last-Event-ID parsing and selective replay",
            "description": "",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Integrate tokio-stream + axum::response::Sse with keep-alive pings",
            "description": "",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Add metrics and tracing for streaming operations",
            "description": "",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Write integration tests for streaming, replay, and fallback paths",
            "description": "",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 6,
        "title": "Enhanced Database Connection and Migration Validation",
        "description": "Establish production database connectivity, validate schema migration, and implement connection pooling with proper health checks.",
        "details": "Configure SQLx connection pool with production PostgreSQL cluster credentials. Validate pgvector extension (3072 dimensions) availability. Verify documents and document_sources tables with proper constraints. Implement connection retry logic with exponential backoff. Add database health checks for Kubernetes probes. Configure pool size based on expected load (min: 5, max: 100). Use dotenvy for environment configuration. Implement proper connection timeout handling (30s default).",
        "testStrategy": "Test connection establishment with production cluster, validate vector operations with 3072-dimension arrays, stress test connection pool under load, verify automatic reconnection after database restart, and validate migration state with schema comparisons.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Configure SQLx Connection Pool with Production Settings",
            "description": "Enhance the existing DatabasePool implementation to support production-grade configuration with environment variables and configurable pool settings",
            "dependencies": [],
            "details": "Update crates/database/src/connection.rs to read pool configuration from environment variables (POOL_MIN_CONNECTIONS=5, POOL_MAX_CONNECTIONS=100, POOL_ACQUIRE_TIMEOUT=30). Implement builder pattern for pool configuration with sensible defaults. Add support for connection string parsing with production PostgreSQL cluster credentials. Configure connection timeout handling (30s default) and implement proper connection string validation.",
            "status": "pending",
            "testStrategy": "Create unit tests to verify pool configuration from environment variables, test connection establishment with various pool sizes, validate timeout behavior with mock delays, and ensure proper error handling for invalid connection strings"
          },
          {
            "id": 2,
            "title": "Implement Connection Retry Logic with Exponential Backoff",
            "description": "Add robust connection retry mechanism to handle temporary database unavailability during startup or network issues",
            "dependencies": [
              "6.1"
            ],
            "details": "Create a new retry module in crates/database/src/retry.rs implementing exponential backoff strategy. Add configurable retry parameters (max_retries: 5, initial_delay: 1s, max_delay: 30s, multiplier: 2.0). Integrate retry logic into DatabasePool::new() method. Add proper logging for each retry attempt with delay information. Handle different error types appropriately (connection vs authentication errors).",
            "status": "pending",
            "testStrategy": "Test retry behavior with simulated connection failures, verify exponential backoff timing, test max retry limit enforcement, validate proper error propagation after exhausting retries, and ensure successful connection after transient failures"
          },
          {
            "id": 3,
            "title": "Validate pgvector Extension and Schema Migration",
            "description": "Enhance migration system to validate pgvector extension with 3072-dimension support and verify database schema integrity",
            "dependencies": [
              "6.1"
            ],
            "details": "Update crates/database/src/migrations.rs to check pgvector version and validate 3072-dimension vector support. Add schema validation queries to verify documents and document_sources tables with all constraints. Implement migration state tracking to detect partial migrations. Add rollback capability for failed migrations. Create comprehensive schema verification including indexes, constraints, and enum types.",
            "status": "pending",
            "testStrategy": "Test pgvector operations with 3072-dimension arrays, verify constraint enforcement on tables, test migration idempotency, validate rollback procedures work correctly, and ensure schema matches expected state"
          },
          {
            "id": 4,
            "title": "Implement Database Health Check Endpoints",
            "description": "Create comprehensive health check system for Kubernetes readiness and liveness probes",
            "dependencies": [
              "6.1",
              "6.2"
            ],
            "details": "Add /health/ready endpoint in crates/mcp/src/server.rs checking database connectivity and migration status. Implement /health/live endpoint for basic service liveness. Add detailed health check in DatabasePool including connection pool metrics (active/idle connections). Implement health check caching (5s TTL) to reduce database load. Return appropriate HTTP status codes (200 for healthy, 503 for unhealthy) with JSON response including details.",
            "status": "pending",
            "testStrategy": "Test health endpoints return correct status codes, verify database connectivity detection, test health check behavior during database restart, validate response format and timing, and ensure Kubernetes compatibility"
          },
          {
            "id": 5,
            "title": "Add Connection Pool Monitoring and Metrics",
            "description": "Implement connection pool observability with metrics and logging for production monitoring",
            "dependencies": [
              "6.1",
              "6.4"
            ],
            "details": "Add connection pool metrics tracking (active_connections, idle_connections, wait_time, acquisition_errors). Implement periodic pool status logging (every 60s) at INFO level. Add connection lifecycle hooks for detailed tracing. Create pool saturation alerts when connections exceed 80% capacity. Export metrics in Prometheus-compatible format at /metrics endpoint. Add connection leak detection with automatic cleanup.",
            "status": "pending",
            "testStrategy": "Verify metrics accuracy under various load conditions, test connection leak detection, validate metric export format, test alert triggering at threshold, and ensure minimal performance impact from monitoring"
          }
        ]
      },
      {
        "id": 7,
        "title": "OpenAI Embedding Client with Batch Processing",
        "description": "Implement optimized OpenAI embedding generation using text-embedding-3-large model with batch processing for 50% cost reduction.",
        "details": "Implement batch processing using OpenAI Batch API with JSONL format. Configure text-embedding-3-large with 3072 dimensions (full) or 1024 (optimized). Create batch queue with 20,000 line chunks for optimal performance. Implement rate limiting (3000 RPM / 1M TPM) with token bucket algorithm. Add retry logic with exponential backoff for failed batches. Store API keys securely using environment variables. Implement cost tracking and reporting. Use reqwest 0.12 for HTTP client with rustls-tls.",
        "testStrategy": "Test batch creation and submission, validate embedding dimensions and quality, measure cost reduction compared to individual requests, test rate limiting under high load, verify retry mechanisms, and benchmark processing time (target < 20 minutes for 20k embeddings).",
        "priority": "medium",
        "dependencies": [
          6
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 8,
        "title": "Implement BirdEye Query Tool",
        "description": "Create the birdeye_query tool for querying BirdEye blockchain API documentation with semantic search and metadata filtering.",
        "details": "Implement BirdeyeQueryTool struct following the pattern in tools.rs. Add semantic search using pgvector similarity (<=> operator). Parse BirdEye-specific metadata (api_version, endpoint, method, parameters, response_schema). Implement result ranking with relevance scores. Format responses with endpoint details and example usage. Add parameter validation for query and limit fields. Register tool in MCP handler with proper definition. Cache frequently accessed endpoints for performance.",
        "testStrategy": "Test query accuracy for various BirdEye endpoints, validate metadata filtering by API version and category, test response formatting and completeness, verify integration with MCP server, and benchmark query performance (< 2 seconds).",
        "priority": "medium",
        "dependencies": [
          6,
          7
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create BirdEyeQueryTool struct and basic implementation",
            "description": "Define the BirdEyeQueryTool struct following the existing RustQueryTool pattern in tools.rs with required fields for database pool, embedding client, and optional caching mechanism",
            "dependencies": [],
            "details": "Create BirdEyeQueryTool struct in crates/mcp/src/tools.rs with fields for db_pool (DatabasePool), embedding_client (EmbeddingClient), and a cache using HashMap<String, (String, chrono::DateTime<Utc>)> for frequently accessed endpoints. Implement new() constructor method that initializes database pool and embedding client. Add helper method to validate cache entries based on TTL (15 minutes). Create placeholder methods for semantic_search and metadata filtering that will be implemented in subsequent subtasks.",
            "status": "pending",
            "testStrategy": "Create unit tests to verify struct instantiation, cache initialization, and TTL validation logic. Test that the new() constructor properly initializes database and embedding connections."
          },
          {
            "id": 2,
            "title": "Implement semantic search with pgvector similarity",
            "description": "Add birdeye_vector_search method to DocumentQueries and implement semantic search functionality using pgvector's <=> operator for BirdEye documents",
            "dependencies": [
              "8.1"
            ],
            "details": "In crates/database/src/queries.rs, create birdeye_vector_search method similar to rust_vector_search but filtering for doc_type='birdeye'. Implement proper vector similarity search using embedding <=> $1 ORDER BY embedding <=> $1 syntax when real embeddings are available. For now, use fallback query filtering by doc_type and metadata fields. In BirdEyeQueryTool, implement semantic_search method that generates embeddings for the query using embedding_client, calls birdeye_vector_search, and calculates relevance scores based on similarity distance.",
            "status": "pending",
            "testStrategy": "Test vector search with various BirdEye endpoint queries, verify correct SQL generation with pgvector operators, test relevance score calculation, and ensure fallback works when embeddings are not available."
          },
          {
            "id": 3,
            "title": "Parse and filter BirdEye-specific metadata",
            "description": "Implement metadata extraction and filtering for BirdEye API fields including api_version, endpoint, method, parameters, and response_schema",
            "dependencies": [
              "8.2"
            ],
            "details": "Enhance semantic_search method to parse metadata JSONB fields specific to BirdEye documents (api_version, endpoint, method, parameters, response_schema) as shown in ingest_birdeye_simple.py. Add filtering capabilities to narrow results by API version (e.g., 'v1', 'v2'), HTTP method (GET, POST), or endpoint category. Extract and validate these fields from the metadata column during result processing. Implement helper methods parse_birdeye_metadata() and filter_by_metadata() to handle JSON parsing and filtering logic.",
            "status": "pending",
            "testStrategy": "Test metadata parsing for various BirdEye document formats, validate filtering by api_version and method fields, test handling of missing or malformed metadata, verify parameter extraction and validation."
          },
          {
            "id": 4,
            "title": "Format responses with endpoint details and examples",
            "description": "Implement result formatting that presents BirdEye API documentation with endpoint details, parameter descriptions, and usage examples",
            "dependencies": [
              "8.3"
            ],
            "details": "Create format_birdeye_response() method that formats search results with structured output including endpoint URL, HTTP method, required/optional parameters from metadata, response schema details, and example API calls. Extract parameter descriptions and types from the parameters field in metadata. Generate example curl commands or code snippets based on endpoint configuration. Format responses similar to the existing Rust documentation formatting but tailored for API documentation presentation. Include relevance scores in the output for transparency.",
            "status": "pending",
            "testStrategy": "Test response formatting for different endpoint types (GET vs POST), verify example generation accuracy, test handling of complex parameter structures, validate that all required fields are included in formatted output."
          },
          {
            "id": 5,
            "title": "Register tool in MCP handler and add caching",
            "description": "Register BirdEyeQueryTool in the MCP handler, implement Tool trait, add parameter validation, and implement endpoint caching for performance",
            "dependencies": [
              "8.4"
            ],
            "details": "Implement Tool trait for BirdEyeQueryTool with definition() method returning proper JSON schema including name='birdeye_query', description, and inputSchema with query and limit parameters. Add execute() method with parameter validation for query (required string) and limit (optional integer 1-20). In handlers.rs, instantiate and register BirdEyeQueryTool in McpHandler::new() similar to RustQueryTool. Implement cache_endpoint() and get_cached_endpoint() methods to store frequently accessed endpoints with 15-minute TTL. Update semantic_search to check cache before database queries for common endpoints.",
            "status": "pending",
            "testStrategy": "Test tool registration in MCP handler, validate parameter validation for query and limit fields, test cache hit/miss scenarios, verify cache expiration after TTL, test integration with MCP server endpoints, benchmark query performance to ensure < 2 second response time."
          }
        ]
      },
      {
        "id": 9,
        "title": "Implement Solana Query Tool",
        "description": "Create the solana_query tool for querying Solana blockchain platform documentation including architecture diagrams and ZK cryptography content.",
        "details": "Implement SolanaQueryTool with support for multiple content formats (markdown, PDF, BOB diagrams, MSC charts). Add specialized handling for technical diagrams and cryptography documentation. Parse Solana-specific metadata (category, format, section, complexity, topic). Implement context-aware result presentation. Support cross-referenced documentation linking. Add special handling for PDF content extraction. Register in MCP server with comprehensive tool definition.",
        "testStrategy": "Test queries across different content types, validate PDF and diagram content retrieval, test metadata filtering by complexity and topic, verify cross-reference resolution, and ensure proper formatting of technical content.",
        "priority": "medium",
        "dependencies": [
          6,
          7
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create SolanaQueryTool struct implementation",
            "description": "Implement the SolanaQueryTool struct following the established pattern in tools.rs with database pool and embedding client dependencies",
            "dependencies": [],
            "details": "Create a new SolanaQueryTool struct in crates/mcp/src/tools.rs with db_pool and embedding_client fields. Implement a new() constructor that initializes the embedding client and accepts a DatabasePool. Follow the exact pattern used in RustQueryTool for consistency with the existing codebase architecture.",
            "status": "pending",
            "testStrategy": "Create unit tests validating SolanaQueryTool struct initialization, verify embedding client creation succeeds, and test error handling when database pool is unavailable"
          },
          {
            "id": 2,
            "title": "Implement Solana-specific semantic search with metadata filtering",
            "description": "Add semantic_search method to SolanaQueryTool with support for querying Solana documents and filtering by metadata fields like format, complexity, and topic",
            "dependencies": [
              "9.1"
            ],
            "details": "Implement semantic_search method that queries documents where doc_type='solana'. Parse and utilize Solana-specific metadata including category (architecture-diagrams, sequence-diagrams, zk-cryptography), format (markdown, pdf, bob, msc), section, complexity, and topic fields. Add proper pgvector similarity search using the <=> operator. Support result ranking with relevance scores and handle different content formats appropriately.",
            "status": "pending",
            "testStrategy": "Test queries for different content types (markdown, PDF, BOB diagrams, MSC charts), validate metadata filtering by complexity levels and topics, verify pgvector similarity search returns relevant results, and test handling of cross-referenced documentation"
          },
          {
            "id": 3,
            "title": "Add specialized content formatting for technical diagrams and PDFs",
            "description": "Implement content formatting logic that handles BOB diagrams, MSC charts, and PDF metadata with context-aware presentation",
            "dependencies": [
              "9.2"
            ],
            "details": "Create formatting logic that detects content type from metadata and applies appropriate formatting. For BOB/MSC diagrams, preserve ASCII art structure and add descriptive headers. For PDF documents, display metadata summary with file location, size, and content description. For markdown content, format with proper headers and code blocks. Include cross-reference links when available in metadata.",
            "status": "pending",
            "testStrategy": "Test formatting output for BOB diagram files, verify MSC chart content is properly preserved, validate PDF metadata display includes all required fields, and ensure markdown content maintains proper structure"
          },
          {
            "id": 4,
            "title": "Implement Tool trait for SolanaQueryTool with MCP definition",
            "description": "Implement the Tool trait with comprehensive tool definition and execute method for MCP integration",
            "dependencies": [
              "9.3"
            ],
            "details": "Implement the Tool trait's definition() method returning JSON with tool name 'solana_query', description mentioning Solana blockchain documentation including architecture diagrams and ZK cryptography. Define inputSchema with query (string), limit (integer 1-20), format filter (optional string: markdown/pdf/bob/msc), and complexity filter (optional string). Implement execute() method that validates parameters, calls semantic_search, and returns formatted results.",
            "status": "pending",
            "testStrategy": "Test tool definition JSON structure validity, verify execute method parameter validation for query and limit fields, test optional format and complexity filters, and validate error handling for invalid parameters"
          },
          {
            "id": 5,
            "title": "Register SolanaQueryTool in MCP handler",
            "description": "Add SolanaQueryTool registration to McpHandler::new() method to make it available through the MCP server",
            "dependencies": [
              "9.4"
            ],
            "details": "In crates/mcp/src/handlers.rs, modify McpHandler::new() to instantiate SolanaQueryTool using the provided db_pool. Register the tool in the tools HashMap with key 'solana_query'. Ensure proper error handling during tool initialization. Update any relevant documentation or comments to reflect the new tool availability.",
            "status": "pending",
            "testStrategy": "Test that solana_query appears in tools/list response, verify tool can be called through MCP server endpoints, test integration with existing HTTP transport layer, and validate concurrent access with other query tools"
          }
        ]
      },
      {
        "id": 10,
        "title": "Implement Rust Crate Management Tools",
        "description": "Create dynamic Rust crate management tools (add_rust_crate, remove_rust_crate, list_rust_crates, check_rust_status) for MCP-based crate administration.",
        "details": "Implement add_rust_crate with automatic docs.rs fetching and parsing. Create remove_rust_crate with cascade deletion of documents and embeddings. Build list_rust_crates with pagination and status information. Implement check_rust_status for health monitoring and statistics. Add transaction support for atomic operations. Implement crate version management and update detection. Use cargo metadata parsing for dependency analysis. Add rate limiting for docs.rs API calls.",
        "testStrategy": "Test adding popular crates (tokio, serde, axum), validate document extraction completeness, test removal with orphan cleanup, verify transaction rollback on errors, test concurrent management operations, and validate status reporting accuracy.",
        "priority": "medium",
        "dependencies": [
          6
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Rust Crate Management Tool Structures and Database Schema",
            "description": "Design and implement the core data structures for crate management tools and extend the database schema to support crate-specific operations",
            "dependencies": [],
            "details": "Create RustCrateManager struct in crates/mcp/src/tools.rs with fields for database pool, embedding client, and HTTP client for docs.rs API. Extend database schema with a 'crates' table containing fields: id, name, version, description, documentation_url, last_updated, status (active/inactive/updating), and metadata JSONB. Add indexes for name and version lookups. Create CrateInfo and CrateStatus structs in crates/database/src/models.rs. Implement transaction helper methods in DatabasePool for atomic operations.",
            "status": "pending",
            "testStrategy": "Create unit tests for struct initialization, verify database migration creates crates table correctly, test transaction rollback behavior with simulated failures"
          },
          {
            "id": 2,
            "title": "Implement add_rust_crate Tool with docs.rs Integration",
            "description": "Build the add_rust_crate tool with automatic documentation fetching from docs.rs and parsing capabilities",
            "dependencies": [
              "10.1"
            ],
            "details": "Implement AddRustCrateTool in crates/mcp/src/tools.rs following the Tool trait pattern. Add docs.rs API client in crates/doc-loader/src/loaders.rs with rate limiting (max 10 requests/minute) using tokio::time::interval. Parse HTML documentation using scraper crate to extract modules, structs, functions, and examples. Store parsed documentation in documents table with proper metadata (crate_name, version, module_path). Generate embeddings for documentation chunks. Implement version checking to detect updates. Add the tool to McpHandler registry in handlers.rs.",
            "status": "pending",
            "testStrategy": "Test with popular crates (tokio, serde, axum), verify HTML parsing extracts complete documentation structure, test rate limiting with rapid sequential requests, validate metadata extraction accuracy"
          },
          {
            "id": 3,
            "title": "Implement remove_rust_crate Tool with Cascade Deletion",
            "description": "Create the remove_rust_crate tool with proper cascade deletion of associated documents and embeddings",
            "dependencies": [
              "10.1",
              "10.2"
            ],
            "details": "Implement RemoveRustCrateTool with transaction support for atomic deletion. Query and delete all documents where metadata->>'crate_name' matches the target crate. Delete associated embeddings from embeddings table using document IDs. Remove crate entry from crates table. Implement orphan cleanup to identify and remove dangling embeddings. Add soft-delete option with status='inactive' for recoverable deletions. Log all deletion operations for audit trail.",
            "status": "pending",
            "testStrategy": "Test removal of crates with varying document counts, verify no orphaned embeddings remain, test transaction rollback on partial failure, validate soft-delete and recovery functionality"
          },
          {
            "id": 4,
            "title": "Implement list_rust_crates Tool with Pagination",
            "description": "Build the list_rust_crates tool with pagination support and comprehensive status information display",
            "dependencies": [
              "10.1",
              "10.2"
            ],
            "details": "Implement ListRustCratesTool with configurable pagination (default 20 items/page). Return crate information including name, version, document count, last updated timestamp, and status. Add filtering options by status (active/inactive/updating) and search by crate name pattern. Include statistics: total documents, total embeddings, average documents per crate. Implement sorting options (name, version, last_updated, document_count). Format output as structured JSON with metadata about pagination (current_page, total_pages, total_items).",
            "status": "pending",
            "testStrategy": "Test pagination with various page sizes, verify filtering by different status values, test search functionality with partial name matches, validate statistics calculation accuracy"
          },
          {
            "id": 5,
            "title": "Implement check_rust_status Tool and Dependency Analysis",
            "description": "Create the check_rust_status tool for health monitoring and implement cargo metadata parsing for dependency analysis",
            "dependencies": [
              "10.1",
              "10.2",
              "10.3",
              "10.4"
            ],
            "details": "Implement CheckRustStatusTool to report overall system health including database connectivity, total crates/documents/embeddings count, storage usage statistics, and last update timestamps. Add cargo metadata parser using std::process::Command to extract crate dependencies from Cargo.toml files. Implement update detection by comparing local versions with docs.rs latest versions. Create health check endpoints for monitoring integration. Add metrics collection for tool usage patterns and query performance. Generate dependency graph visualization data in JSON format.",
            "status": "pending",
            "testStrategy": "Test status reporting under various load conditions, verify cargo metadata parsing with complex dependency trees, test update detection with outdated crate versions, validate performance metrics accuracy"
          }
        ]
      },
      {
        "id": 11,
        "title": "Implement Additional Query Tools Suite",
        "description": "Create the remaining query tools for jupyter, cilium, talos, meteora, raydium, ebpf, and rust_best_practices documentation types.",
        "details": "Follow established QueryTool pattern for each documentation type. Implement type-specific metadata parsing and filtering. Add specialized response formatting per documentation type. Ensure consistent error handling and validation. Register all tools in MCP server with proper definitions. Implement tool-specific caching strategies. Add performance monitoring per tool. Create shared utility functions for common operations.",
        "testStrategy": "Test each tool with type-specific queries, validate metadata extraction and filtering, verify response formatting consistency, test error handling for edge cases, and benchmark individual tool performance.",
        "priority": "low",
        "dependencies": [
          8,
          9
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create shared utility module for query tools",
            "description": "Implement a common utilities module with shared functions for metadata parsing, response formatting, and performance monitoring that all query tools can use",
            "dependencies": [],
            "details": "Create crates/mcp/src/query_utils.rs module with: parse_metadata_field() generic function for extracting typed fields from JSONB, format_document_response() for consistent markdown formatting across all tools, calculate_relevance_score() for similarity-based ranking, create_performance_monitor() for tracking query execution times, and validate_query_params() for standard parameter validation. Export these utilities from lib.rs for use by all QueryTool implementations.",
            "status": "pending",
            "testStrategy": "Unit test each utility function with various metadata formats and edge cases, verify performance monitoring accuracy with timed operations, test format consistency across different document types"
          },
          {
            "id": 2,
            "title": "Implement Jupyter and Cilium query tools",
            "description": "Create JupyterQueryTool and CiliumQueryTool following the established RustQueryTool pattern with type-specific metadata handling",
            "dependencies": [
              "11.1"
            ],
            "details": "In crates/mcp/src/tools.rs, implement JupyterQueryTool with notebook-specific metadata parsing (kernel, language, cell_types), and CiliumQueryTool with network policy metadata (policy_type, namespace, endpoints). Add corresponding vector_search methods in crates/database/src/queries.rs filtering by doc_type='jupyter' and doc_type='cilium'. Use shared utilities for common operations. Register both tools in McpHandler::new() with appropriate tool definitions including input schemas.",
            "status": "pending",
            "testStrategy": "Test Jupyter tool with notebook-specific queries, validate Cilium network policy searches, verify metadata extraction for both types, test integration with MCP handler"
          },
          {
            "id": 3,
            "title": "Implement Talos and Meteora query tools",
            "description": "Create TalosQueryTool and MeteoraQueryTool with specialized metadata parsing for Kubernetes and DeFi documentation",
            "dependencies": [
              "11.1"
            ],
            "details": "Implement TalosQueryTool in crates/mcp/src/tools.rs with Kubernetes-specific metadata (resource_type, api_version, namespace), and MeteoraQueryTool with DeFi protocol metadata (pool_type, liquidity_params, reward_structure). Add talos_vector_search and meteora_vector_search methods to DocumentQueries. Implement custom response formatting for technical specifications and protocol parameters. Register tools in handlers.rs with comprehensive tool definitions.",
            "status": "pending",
            "testStrategy": "Test with Kubernetes resource queries for Talos, verify DeFi protocol searches for Meteora, validate specialized formatting for technical content, ensure proper error handling"
          },
          {
            "id": 4,
            "title": "Implement Raydium and eBPF query tools",
            "description": "Create RaydiumQueryTool and EbpfQueryTool with domain-specific metadata parsing and response formatting",
            "dependencies": [
              "11.1"
            ],
            "details": "Build RaydiumQueryTool with AMM and liquidity pool metadata support (amm_version, pool_address, fee_structure), and EbpfQueryTool with kernel programming metadata (program_type, kernel_version, hook_points). Add corresponding vector search methods in queries.rs. Implement specialized formatting for code examples and technical diagrams. Add caching strategy using tokio::sync::RwLock for frequently accessed technical documentation. Register in MCP handler.",
            "status": "pending",
            "testStrategy": "Test AMM pool queries for Raydium, verify eBPF program type filtering, test cache hit rates for repeated queries, validate code example formatting"
          },
          {
            "id": 5,
            "title": "Implement RustBestPractices tool and integration testing",
            "description": "Create RustBestPracticesQueryTool and comprehensive integration tests for all seven new query tools",
            "dependencies": [
              "11.2",
              "11.3",
              "11.4"
            ],
            "details": "Implement RustBestPracticesQueryTool with pattern/anti-pattern metadata (practice_category, rust_version, complexity_level). Add rust_best_practices_vector_search to queries.rs. Create integration tests in crates/mcp/tests/ validating all seven tools' query accuracy, metadata filtering, response formatting, error handling, and performance benchmarks (< 2 seconds per query). Verify proper tool registration and MCP protocol compliance for all tools.",
            "status": "pending",
            "testStrategy": "End-to-end testing of all seven tools with representative queries, load testing with concurrent requests, verify MCP tools/list includes all new tools, benchmark query performance across all documentation types"
          }
        ]
      },
      {
        "id": 12,
        "title": "Kubernetes Deployment Configuration",
        "description": "Create and optimize Helm charts for production Kubernetes deployment with proper resource allocation and security policies.",
        "details": "Create Helm chart structure in helm/doc-server/ directory. Define values.yaml with configurable parameters (image, resources, replicas). Create deployment manifest with proper resource limits (CPU: 500m-2000m, Memory: 512Mi-2Gi). Configure service with ClusterIP and optional LoadBalancer. Add ingress rules for external access. Implement ConfigMap for environment variables. Add Secret management for API keys and database credentials. Configure PodDisruptionBudget for high availability.",
        "testStrategy": "Test Helm chart installation with helm lint and dry-run, validate resource allocation under load, test rolling updates with zero downtime, verify secret mounting and environment injection, and test auto-scaling behavior.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Helm Chart Structure and Base Configuration",
            "description": "Initialize the Helm chart directory structure and create the base Chart.yaml file with proper metadata and dependencies",
            "dependencies": [],
            "details": "Create helm/doc-server/ directory structure with standard Helm chart layout (templates/, charts/, values.yaml). Define Chart.yaml with apiVersion: v2, name: doc-server, version and appVersion matching Cargo.toml. Set up .helmignore file to exclude unnecessary files. Create helpers template (_helpers.tpl) for common label and selector functions",
            "status": "pending",
            "testStrategy": "Run helm lint to validate chart structure, execute helm template to verify template rendering without errors, check that all required directories and files are present with correct permissions"
          },
          {
            "id": 2,
            "title": "Define values.yaml with Configurable Parameters",
            "description": "Create comprehensive values.yaml file with all configurable parameters for deployment customization including image settings, resources, and scaling options",
            "dependencies": [
              "12.1"
            ],
            "details": "Define image configuration (repository: ghcr.io/5dlabs/agent-docs/doc-server, tag, pullPolicy). Set resource limits and requests (CPU: 500m-2000m, Memory: 512Mi-2Gi). Configure replicaCount (default: 2), autoscaling parameters (min: 2, max: 10, targetCPU: 70%). Add service configuration (type: ClusterIP, port: 3001). Define ingress settings (enabled: false by default, host, TLS). Include environment variables for DATABASE_URL, PORT (3001), RUST_LOG, and other app configs",
            "status": "pending",
            "testStrategy": "Validate values.yaml schema using helm lint, test value overrides with helm template --set commands, verify default values produce valid Kubernetes manifests"
          },
          {
            "id": 3,
            "title": "Create Deployment and Service Manifests",
            "description": "Implement Kubernetes deployment manifest with proper resource allocation, health checks, and create service manifests for both ClusterIP and optional LoadBalancer",
            "dependencies": [
              "12.2"
            ],
            "details": "Create deployment.yaml with container spec using values from values.yaml, implement readiness/liveness probes on /health endpoint, configure resource limits/requests from values, add security context (runAsNonRoot: true, runAsUser: 1000). Create service.yaml supporting both ClusterIP and LoadBalancer types, expose port 3001 targeting container port. Add proper labels and selectors using helm helpers. Include pod annotations for Prometheus metrics scraping (/metrics)",
            "status": "pending",
            "testStrategy": "Deploy to test namespace and verify pods start successfully, test health check endpoints respond correctly, verify resource limits are enforced, test service connectivity and load balancing"
          },
          {
            "id": 4,
            "title": "Implement ConfigMap, Secret Management and Ingress",
            "description": "Create ConfigMap for environment variables, implement Secret management for sensitive data, and configure Ingress rules for external access",
            "dependencies": [
              "12.3"
            ],
            "details": "Create configmap.yaml for non-sensitive environment variables (RUST_LOG, PORT, monitoring configs). Implement secret.yaml template for API keys and DATABASE_URL with base64 encoding. Create ingress.yaml with conditional rendering based on values.yaml, support multiple hosts and TLS configuration, add annotations for ingress controller (nginx/traefik). Mount ConfigMap and Secrets as environment variables in deployment. Add support for external secrets operator integration",
            "status": "pending",
            "testStrategy": "Verify ConfigMap and Secret mounting in pods, test environment variable injection, validate ingress routing with curl/wget, test TLS termination if configured, verify secret rotation without pod restart"
          },
          {
            "id": 5,
            "title": "Add High Availability and Production Features",
            "description": "Implement PodDisruptionBudget, HorizontalPodAutoscaler, and other production-ready features for high availability and reliability",
            "dependencies": [
              "12.4"
            ],
            "details": "Create poddisruptionbudget.yaml with minAvailable: 1 or maxUnavailable: 1. Implement hpa.yaml for horizontal pod autoscaling based on CPU/memory metrics. Add networkpolicy.yaml for pod-to-pod communication restrictions. Create servicemonitor.yaml for Prometheus Operator integration. Add priorityclass.yaml for pod scheduling priority. Implement pod anti-affinity rules to spread pods across nodes. Add topologySpreadConstraints for even distribution across zones",
            "status": "pending",
            "testStrategy": "Test PDB by draining nodes and verifying minimum pods maintained, simulate load to trigger HPA scaling, verify network policies block/allow traffic correctly, test pod distribution across availability zones, validate rolling updates maintain availability"
          }
        ]
      },
      {
        "id": 13,
        "title": "Container Image Optimization",
        "description": "Optimize Docker image for size, security, and performance with multi-stage builds and minimal attack surface.",
        "details": "Enhance existing Dockerfile with cargo-chef for dependency caching. Implement distroless or alpine-based runtime image. Add security scanning with trivy or snyk. Optimize binary size with strip and upx compression. Implement proper signal handling for graceful shutdown. Add non-root user with minimal permissions. Configure proper health check endpoints. Reduce image layers and optimize build cache. Target image size < 100MB.",
        "testStrategy": "Scan image for vulnerabilities, measure image size and layer count, test graceful shutdown handling, verify health checks in container runtime, and benchmark startup time (< 5 seconds).",
        "priority": "medium",
        "dependencies": [
          2
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement cargo-chef for Dependency Caching",
            "description": "Replace current manual dependency caching approach with cargo-chef to optimize Docker build times and improve layer caching efficiency",
            "dependencies": [],
            "details": "Install cargo-chef in builder stage. Create chef stage with cargo chef prepare for generating recipe.json. Add dependencies stage with cargo chef cook for building dependencies separately. Modify builder stage to use pre-built dependencies from chef. This will significantly reduce rebuild times when only source code changes.",
            "status": "pending",
            "testStrategy": "Measure build time before and after implementation. Test cache effectiveness by modifying only source files and verifying dependencies aren't rebuilt. Validate resulting binary functionality remains unchanged."
          },
          {
            "id": 2,
            "title": "Migrate to Distroless Runtime Image",
            "description": "Replace debian:bookworm-slim base image with gcr.io/distroless/cc-debian12 for minimal attack surface and reduced image size",
            "dependencies": [
              "13.1"
            ],
            "details": "Switch runtime FROM to gcr.io/distroless/cc-debian12. Remove apt-get installations in runtime stage as distroless includes only essential libraries. Ensure libssl and libpq shared libraries are available or statically link them. Update HEALTHCHECK to use built-in /health endpoint since curl won't be available. Set proper USER directive compatible with distroless nonroot user.",
            "status": "pending",
            "testStrategy": "Verify binary runs successfully in distroless environment. Test database connections work properly. Validate health check endpoint accessibility. Scan for CVEs and compare attack surface reduction."
          },
          {
            "id": 3,
            "title": "Add Binary Optimization and Compression",
            "description": "Implement strip and UPX compression to reduce binary size while maintaining performance",
            "dependencies": [
              "13.1"
            ],
            "details": "Add strip command in builder stage after cargo build to remove debug symbols. Install and apply UPX compression with --best flag for maximum compression. Configure Cargo.toml with opt-level=z, lto=true, codegen-units=1 for size optimization. Add panic=abort to reduce binary size further. Target final binary size reduction of 60-70%.",
            "status": "pending",
            "testStrategy": "Compare binary sizes before and after optimization. Benchmark startup time to ensure < 5 seconds. Run performance tests to verify no significant runtime degradation. Test panic handling behavior."
          },
          {
            "id": 4,
            "title": "Implement Graceful Shutdown and Signal Handling",
            "description": "Add proper SIGTERM/SIGINT signal handling in the Rust application for container orchestration compatibility",
            "dependencies": [],
            "details": "Implement tokio::signal handlers in http_server.rs for SIGTERM and SIGINT. Add graceful shutdown logic to close database connections and complete in-flight requests. Set proper timeout for shutdown sequence (30 seconds). Update Dockerfile with STOPSIGNAL SIGTERM. Ensure non-root user can receive and handle signals properly.",
            "status": "pending",
            "testStrategy": "Test graceful shutdown with docker stop command. Verify in-flight requests complete successfully. Validate database connections close cleanly. Test signal handling with kill commands inside container."
          },
          {
            "id": 5,
            "title": "Integrate Security Scanning Pipeline",
            "description": "Add Trivy vulnerability scanning to CI/CD pipeline and create security scanning script for local development",
            "dependencies": [
              "13.2",
              "13.3"
            ],
            "details": "Create scripts/scan_image.sh with Trivy scanning for vulnerabilities and misconfigurations. Add GitHub Action workflow for automated scanning on builds. Configure severity thresholds (CRITICAL/HIGH must be zero). Generate SBOM (Software Bill of Materials) for compliance. Add scanning results to build artifacts. Document security scanning process in documentation.",
            "status": "pending",
            "testStrategy": "Run Trivy scan and verify no CRITICAL/HIGH vulnerabilities. Test SBOM generation completeness. Validate GitHub Action triggers on Dockerfile changes. Verify scan results are properly reported and stored."
          }
        ]
      },
      {
        "id": 14,
        "title": "Production Monitoring and Observability",
        "description": "Implement comprehensive monitoring with Prometheus metrics, structured logging, and distributed tracing for production observability.",
        "details": "Add Prometheus metrics endpoint (/metrics) with custom metrics (query latency, embedding generation time, cache hit rate). Implement structured JSON logging with correlation IDs. Add OpenTelemetry tracing with Jaeger integration. Create custom dashboards in Grafana for key metrics. Implement alerting rules for SLA violations. Add performance profiling endpoints (pprof compatible). Use tracing and tracing-subscriber with json feature.",
        "testStrategy": "Validate metrics endpoint format and values, test log aggregation and searching, verify trace propagation across services, test alert triggering for various scenarios, and validate dashboard accuracy under load.",
        "priority": "medium",
        "dependencies": [
          12
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 15,
        "title": "Load Testing and Performance Optimization",
        "description": "Conduct comprehensive load testing to validate performance targets and optimize system bottlenecks.",
        "details": "Create load testing suite using scripts/load_test_sse.js as base. Test with 100+ concurrent connections using k6 or vegeta. Identify and optimize query bottlenecks with EXPLAIN ANALYZE. Implement query result caching with TTL. Optimize vector similarity search with approximation techniques. Add connection pooling optimization. Profile CPU and memory usage under load. Implement request coalescing for duplicate queries.",
        "testStrategy": "Run sustained load tests (1 hour) with gradual ramp-up, measure p50/p95/p99 latencies, test connection limit handling, verify memory stability under load, and validate cache effectiveness.",
        "priority": "medium",
        "dependencies": [
          14
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 16,
        "title": "CI/CD Pipeline Enhancement",
        "description": "Enhance GitHub Actions workflow for comprehensive testing, security scanning, and automated deployment validation.",
        "details": "Add integration test stage with database fixtures. Implement security scanning (cargo-audit, cargo-deny). Add performance regression testing. Implement blue-green deployment strategy. Add smoke tests post-deployment. Configure branch protection rules. Implement automated rollback on failure. Add dependency update automation with Dependabot. Enhance caching with sccache optimization.",
        "testStrategy": "Test pipeline with various failure scenarios, validate rollback mechanisms, test parallel job execution, verify security scan detection, and measure pipeline execution time (< 10 minutes).",
        "priority": "medium",
        "dependencies": [
          12
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 17,
        "title": "Documentation and API Reference Generation",
        "description": "Create comprehensive documentation including API reference, deployment guides, and integration examples.",
        "details": "Generate API documentation using cargo doc with all features. Create OpenAPI/Swagger specification for HTTP endpoints. Write deployment guide for Kubernetes setup. Document MCP tool usage with examples. Create troubleshooting guide with common issues. Add architecture diagrams using mermaid or plantuml. Document performance tuning parameters. Create client integration examples for Cursor and Toolman.",
        "testStrategy": "Validate documentation completeness with doc tests, test example code execution, verify OpenAPI spec with validators, test deployment guide accuracy, and review with stakeholders.",
        "priority": "low",
        "dependencies": [
          11
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 18,
        "title": "Data Migration and Validation Pipeline",
        "description": "Implement automated data migration pipeline for populating all documentation types with validation and rollback capabilities.",
        "details": "Create migration scripts for each documentation type using doc-loader crate. Implement incremental migration with checkpointing. Add data validation with checksums and count verification. Create rollback procedures for failed migrations. Implement parallel processing for large datasets. Add progress reporting and ETA calculation. Store migration history in database. Handle duplicate detection and merging.",
        "testStrategy": "Test migration with sample datasets, validate data integrity post-migration, test rollback procedures, verify idempotency of migrations, and benchmark migration performance (target: 1000 docs/minute).",
        "priority": "medium",
        "dependencies": [
          10,
          11
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 19,
        "title": "Security Hardening and Compliance",
        "description": "Implement comprehensive security measures including authentication, authorization, encryption, and compliance controls.",
        "details": "Implement JWT-based authentication for MCP endpoints. Add role-based access control (RBAC) for management tools. Enable TLS/SSL for all communications. Implement API key rotation mechanism. Add request signing for integrity validation. Implement audit logging for all operations. Add rate limiting per client/IP. Configure CORS policies properly. Implement input sanitization and validation. Add security headers (CSP, HSTS, X-Frame-Options).",
        "testStrategy": "Conduct penetration testing, test authentication bypass attempts, verify encryption in transit, test rate limiting effectiveness, validate audit log completeness, and perform OWASP compliance check.",
        "priority": "high",
        "dependencies": [
          3
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 20,
        "title": "Production Deployment and Validation",
        "description": "Execute final production deployment with comprehensive validation ensuring all acceptance criteria are met in the live environment.",
        "details": "Execute production deployment using GitHub Actions workflow. Validate all 10 query tools with real queries. Test Rust crate management operations. Verify Streamable HTTP transport with Toolman integration. Confirm 70% cost reduction through batch processing. Validate < 2 second query response times. Test with 100+ concurrent connections. Verify Kubernetes auto-scaling and health checks. Perform acceptance testing with stakeholders. Document production configuration and runbooks.",
        "testStrategy": "Execute comprehensive production validation checklist, perform user acceptance testing, validate SLA compliance, test disaster recovery procedures, verify monitoring and alerting, and obtain stakeholder sign-off on all acceptance criteria.",
        "priority": "high",
        "dependencies": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19
        ],
        "status": "pending",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-08-11T23:21:08.041Z",
      "updated": "2025-08-11T23:21:08.041Z",
      "description": "Tasks for master context"
    }
  }
}