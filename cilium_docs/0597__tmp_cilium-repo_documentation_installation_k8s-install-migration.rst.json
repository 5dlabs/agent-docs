{
  "url": "file:///tmp/cilium-repo/Documentation/installation/k8s-install-migration.rst",
  "content": ".. only:: not (epub or latex or html) \n WARNING: You are looking at unreleased Cilium documentation.\nPlease use the official rendered version released here:\nhttps://docs.cilium.io\n \n .. _cni_migration: \n \n Migrating a cluster to Cilium \n \n Cilium can be used to migrate from another cni. Running clusters can\nbe migrated on a node-by-node basis, without disrupting existing traffic\nor requiring a complete cluster outage or rebuild depending on the complexity of the migration case. \n This document outlines how migrations with Cilium work. You will have a good\nunderstanding of the basic requirements, as well as see an example migration\nwhich you can practice using :ref: Kind <gs_kind> . \n Background \n When the kubelet creates a Pod's Sandbox, the installed CNI, as configured in  /etc/cni/net.d/ ,\nis called. The cni will handle the networking for a pod - including allocating\nan ip address, creating & configuring a network interface, and (potentially)\nestablishing an overlay network. The Pod's network configuration shares the\nsame life cycle as the PodSandbox. \n In the case of migration, we typically reconfigure  /etc/cni/net.d/  to point\nto Cilium. However, any existing pods will still have been configured by the old\nnetwork plugin and any new pods will be configured by the newer CNI. To complete\nthe migration all Pods on the cluster that are configured by the old cni must be\nrecycled in order to be a member of the new CNI. \n A naive approach to migrating a CNI would be to reconfigure all nodes with a new\nCNI and then gradually restart each node in the cluster, thus replacing the CNI\nwhen the node is brought back up and ensuring that all pods are part of the new CNI. \n This simple migration, while effective, comes at the cost of disrupting cluster\nconnectivity during the rollout. Unmigrated and migrated nodes would be split in\nto two \"islands\" of connectivity, and pods would be randomly unable to reach one-another\nuntil the migration is complete. \n Migration via dual overlays \n Instead, Cilium supports a  hybrid  mode, where two separate overlays are established\nacross the cluster. While pods on a given node can only be attached to one network,\nthey have access to both Cilium and non-Cilium pods while the migration is\ntaking place. As long as Cilium and the existing networking provider use a separate\nIP range, the Linux routing table takes care of separating traffic. \n In this document we will discuss a model for live migrating between two deployed\nCNI implementations. This will have the benefit of reducing downtime of nodes\nand workloads and ensuring that workloads on both configured CNIs can communicate\nduring migration. \n For live migration to work, Cilium will be installed with a separate\nCIDR range and encapsulation port than that of the currently installed CNI. As\nlong as Cilium and the existing CNI use a separate IP range, the Linux\nrouting table takes care of separating traffic. \n Requirements \n Live migration requires the following: \n \n A new, distinct Cluster CIDR for Cilium to use \n Use of the :ref: Cluster Pool IPAM mode<ipam_crd_cluster_pool> \n A distinct overlay, either protocol or port \n An existing network plugin that uses the Linux routing stack, such as Flannel, Calico, or AWS-CNI \n \n Limitations \n Currently, Cilium migration has not been tested with: \n \n BGP-based routing \n Changing IP families (e.g. from IPv4 to IPv6) \n Migrating from Cilium in chained mode \n An existing NetworkPolicy provider \n \n During migration, Cilium's  NetworkPolicy and CiliumNetworkPolicy enforcement\nwill be disabled. Otherwise, traffic from non-Cilium pods may be incorrectly\ndropped. Once the migration process is complete, policy enforcement can\nbe re-enabled. If there is an existing NetworkPolicy provider, you may wish to\ntemporarily delete all NetworkPolicies before proceeding. \n It is strongly recommended to install Cilium using the :ref: cluster-pool <ipam_crd_cluster_pool> \nIPAM allocator. This provides the strongest assurance that there will\nbe no IP collisions. \n .. warning::\nMigration is highly dependent on the exact configuration of existing\nclusters. It is, thus, strongly recommended to perform a trial migration\non a test or lab cluster. \n Overview \n The migration process utilizes the :ref: per-node configuration<per-node-configuration> \nfeature to selectively enable Cilium CNI. This allows for a controlled rollout\nof Cilium without disrupting existing workloads. \n Cilium will be installed, first, in a mode where it establishes an overlay\nbut does not provide CNI networking for any pods. Then, individual nodes will\nbe migrated. \n In summary, the process looks like: \n \n Install cilium in \"secondary\" mode \n Cordon, drain, migrate, and reboot each node \n Remove the existing network provider \n (Optional) Reboot each node again \n \n Migration procedure \n Preparation \n \n \n Optional: Create a :ref: Kind <gs_kind>  cluster and install  Flannel <https://github.com/flannel-io/flannel> _ on it. \n .. parsed-literal:: \n $ cat <<EOF > kind-config.yaml\napiVersion: kind.x-k8s.io/v1alpha4\nkind: Cluster\nnodes:\n- role: control-plane\n- role: worker\n- role: worker\nnetworking:\n  disableDefaultCNI: true\nEOF\n$ kind create cluster --config=kind-config.yaml\n$ kubectl apply -n kube-system --server-side -f \\ |SCM_WEB|\\/examples/misc/migration/install-reference-cni-plugins.yaml\n$ kubectl apply --server-side -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml\n$ kubectl wait --for=condition=Ready nodes --all\n \n \n \n Optional: Monitor connectivity. \n You may wish to install a tool such as  goldpinger <https://github.com/bloomberg/goldpinger> _\nto detect any possible connectivity issues. \n \n \n \n \n Select a  new  CIDR for pods. It must be distinct from all other CIDRs in use. \n For Kind clusters, the default is  10.244.0.0/16 . So, for this example, we will\nuse  10.245.0.0/16 . \n \n \n Select a  distinct  encapsulation port. For example, if the existing cluster\nis using VXLAN, then you should either use GENEVE or configure Cilium to use VXLAN\nwith a different port. \n For this example, we will use VXLAN with a non-default port of 8473. \n \n \n Create a helm  values-migration.yaml  file based on the following example. Be sure to fill\nin the CIDR you selected in step 1. \n .. code-block:: yaml \n operator:\n  unmanagedPodWatcher:\n    restart: false # Migration: Don't restart unmigrated pods\nroutingMode: tunnel # Migration: Optional: default is tunneling, configure as needed\ntunnelProtocol: vxlan # Migration: Optional: default is VXLAN, configure as needed\ntunnelPort: 8473 # Migration: Optional, change only if both networks use the same port by default\ncni:\n  customConf: true # Migration: Don't install a CNI configuration file\n  uninstall: false # Migration: Don't remove CNI configuration on shutdown\nipam:\n  mode: \"cluster-pool\"\n  operator:\n    clusterPoolIPv4PodCIDRList: [\"10.245.0.0/16\"] # Migration: Ensure this is distinct and unused\npolicyEnforcementMode: \"never\" # Migration: Disable policy enforcement\nbpf:\n  hostLegacyRouting: true # Migration: Allow for routing between Cilium and the existing overlay\n \n \n \n Configure any additional Cilium Helm values. \n Cilium supports a number of :ref: Helm configuration options<helm_reference> . You may choose to\nauto-detect typical ones using the :ref: cilium-cli <install_cilium_cli> .\nThis will consume the template and auto-detect any other relevant Helm values.\nReview these values for your particular installation. \n .. parsed-literal:: \n $ cilium install |CHART_VERSION| --values values-migration.yaml --dry-run-helm-values > values-initial.yaml\n$ cat values-initial.yaml\n \n \n \n Install cilium using :ref: helm <k8s_install_helm> . \n .. code-block:: shell-session \n $ helm repo add cilium https://helm.cilium.io/\n$ helm install cilium cilium/cilium --namespace kube-system --values values-initial.yaml \n At this point, you should have a cluster with Cilium installed and an overlay established, but no\npods managed by Cilium itself. You can verify this with the  cilium  command. \n .. code-block:: shell-session \n $ cilium status --wait\n...\nCluster Pods:     0/3 managed by Cilium \n \n \n Create a :ref: per-node config<per-node-configuration>  that will instruct Cilium to \"take over\" CNI networking\non the node. Initially, this will apply to no nodes; you will roll it out gradually via\nthe migration process. \n .. code-block:: shell-session \n cat <<EOF | kubectl apply --server-side -f -\napiVersion: cilium.io/v2\nkind: CiliumNodeConfig\nmetadata:\n  namespace: kube-system\n  name: cilium-default\nspec:\n  nodeSelector:\n    matchLabels:\n      io.cilium.migration/cilium-default: \"true\"\n  defaults:\n    write-cni-conf-when-ready: /host/etc/cni/net.d/05-cilium.conflist\n    custom-cni-conf: \"false\"\n    cni-chaining-mode: \"none\"\n    cni-exclusive: \"true\"\nEOF\n \n \n \n Migration \n At this point, you are ready to begin the migration process. The basic flow is: \n Select a node to be migrated. It is not recommended to start with a control-plane node. \n .. code-block:: shell-session \n $ NODE=\"kind-worker\" # for the Kind example \n \n \n Cordon and, optionally, drain the node in question. \n .. code-block:: shell-session \n $ kubectl cordon $NODE\n$ kubectl drain --ignore-daemonsets $NODE \n Draining is not strictly required, but it is recommended. Otherwise pods will encounter\na brief interruption while the node is rebooted. \n \n \n Label the node. This causes the  CiliumNodeConfig  to apply to this node. \n .. code-block:: shell-session \n $ kubectl label node $NODE --overwrite \"io.cilium.migration/cilium-default=true\" \n \n \n Restart Cilium. This will cause it to write its CNI configuration file. \n .. code-block:: shell-session \n $ kubectl -n kube-system delete pod --field-selector spec.nodeName=$NODE -l k8s-app=cilium\n$ kubectl -n kube-system rollout status ds/cilium -w \n \n \n Reboot the node. \n If using kind, do so with docker: \n .. code-block:: shell-session \n docker restart $NODE \n \n \n Validate that the node has been successfully migrated. \n .. code-block:: shell-session \n $ cilium status --wait\n$ kubectl get -o wide node $NODE\n$ kubectl -n kube-system run --attach --rm --restart=Never verify-network  \n--overrides='{\"spec\": {\"nodeName\": \"'$NODE'\", \"tolerations\": [{\"operator\": \"Exists\"}]}}'  \n--image ghcr.io/nicolaka/netshoot:v0.8 -- /bin/bash -c 'ip -br addr && curl -s -k https://$KUBERNETES_SERVICE_HOST/healthz && echo' \n Ensure the IP address of the pod is in the Cilium CIDR(s) supplied above and that the apiserver\nis reachable. \n \n \n Uncordon the node. \n .. code-block:: shell-session \n $ kubectl uncordon $NODE \n \n \n Once you are satisfied everything has been migrated successfully, select another unmigrated node in the cluster\nand repeat these steps. \n Post-migration \n Perform these steps once the cluster is fully migrated. \n \n \n Ensure Cilium is healthy and that all pods have been migrated: \n .. code-block:: shell-session \n $ cilium status \n \n \n Update the Cilium configuration: \n \n Cilium should be the primary CNI \n NetworkPolicy should be enforced \n The Operator can restart unmanaged pods \n Optional : use :ref: eBPF_Host_Routing . Enabling this will cause a short connectivity\ninterruption on each node as the daemon restarts, but improves networking performance. \n \n You can do this manually, or via the  cilium  tool (this will not apply changes to the cluster): \n .. parsed-literal:: \n $ cilium install |CHART_VERSION| --values values-initial.yaml --dry-run-helm-values  \n--set operator.unmanagedPodWatcher.restart=true --set cni.customConf=false  \n--set policyEnforcementMode=default  \n--set bpf.hostLegacyRouting=false > values-final.yaml # optional, can cause brief interruptions\n$ diff values-initial.yaml values-final.yaml \n Then, apply the changes to the cluster: \n .. code-block:: shell-session \n $ helm upgrade --namespace kube-system cilium cilium/cilium --values values-final.yaml\n$ kubectl -n kube-system rollout restart daemonset cilium\n$ cilium status --wait \n \n \n Delete the per-node configuration: \n .. code-block:: shell-session \n $ kubectl delete -n kube-system ciliumnodeconfig cilium-default \n \n \n Delete the previous network plugin. \n At this point, all pods should be using Cilium for networking. You can easily verify this with  cilium status .\nIt is now safe to delete the previous network plugin from the cluster. \n Most network plugins leave behind some resources, e.g. iptables rules and interfaces. These will be\ncleaned up when the node next reboots. If desired, you may perform a rolling reboot again.",
  "item_type": "unknown",
  "module_path": "/tmp/cilium-repo/Documentation/installation/k8s-install-migration.rst",
  "extracted_at": "2025-09-03T01:13:29.320398Z"
}