{
  "url": "file:///tmp/cilium-repo/Documentation/reference-guides/bpf/architecture.rst",
  "content": ".. only:: not (epub or latex or html) \n WARNING: You are looking at unreleased Cilium documentation.\nPlease use the official rendered version released here:\nhttps://docs.cilium.io\n \n .. _bpf_architect: \n BPF Architecture \n BPF does not define itself by only providing its instruction set, but also by\noffering further infrastructure around it such as maps which act as efficient\nkey / value stores, helper functions to interact with and leverage kernel\nfunctionality, tail calls for calling into other BPF programs, security\nhardening primitives, a pseudo file system for pinning objects (maps,\nprograms), and infrastructure for allowing BPF to be offloaded, for example, to\na network card. \n LLVM provides a BPF back end, so that tools like clang can be used to\ncompile C into a BPF object file, which can then be loaded into the kernel.\nBPF is deeply tied to the Linux kernel and allows for full programmability\nwithout sacrificing native kernel performance. \n Last but not least, also the kernel subsystems making use of BPF are part of\nBPF's infrastructure. The two main subsystems discussed throughout this\ndocument are tc and XDP where BPF programs can be attached to. XDP BPF programs\nare attached at the earliest networking driver stage and trigger a run of the\nBPF program upon packet reception. By definition, this achieves the best\npossible packet processing performance since packets cannot get processed at an\neven earlier point in software. However, since this processing occurs so early\nin the networking stack, the stack has not yet extracted metadata out of the\npacket. On the other hand, tc BPF programs are executed later in the kernel\nstack, so they have access to more metadata and core kernel functionality.\nApart from tc and XDP programs, various other kernel subsystems use BPF,\nsuch as tracing (via kprobes, uprobes, tracepoints, for example). \n The following subsections provide further details on individual aspects of the\nBPF architecture. \n Instruction Set \n BPF is a general purpose RISC instruction set and was originally designed for the\npurpose of writing programs in a subset of C which can be compiled into BPF instructions\nthrough a compiler back end (e.g. LLVM), so that the kernel can later on map them\nthrough an in-kernel JIT compiler into native opcodes for optimal execution performance\ninside the kernel. \n The advantages for pushing these instructions into the kernel include: \n \n \n Making the kernel programmable without having to cross kernel / user space\nboundaries. For example, BPF programs related to networking, as in the case of\nCilium, can implement flexible container policies, load balancing and other means\nwithout having to move packets to user space and back into the kernel. State\nbetween BPF programs and kernel / user space can still be shared through maps\nwhenever needed. \n \n \n Given the flexibility of a programmable data path, programs can be heavily optimized\nfor performance also by compiling out features that are not required for the use cases\nthe program solves. For example, if a container does not require IPv4, then the BPF\nprogram can be built to only deal with IPv6 in order to save resources in the fast-path. \n \n \n In case of networking (e.g. tc and XDP), BPF programs can be updated atomically\nwithout having to restart the kernel, system services or containers, and without\ntraffic interruptions. Furthermore, any program state can also be maintained\nthroughout updates via BPF maps. \n \n \n BPF provides a stable ABI towards user space, and does not require any third party\nkernel modules. BPF is a core part of the Linux kernel that is shipped everywhere,\nand guarantees that existing BPF programs keep running with newer kernel versions.\nThis guarantee is the same guarantee that the kernel provides for system calls with\nregard to user space applications. Moreover, BPF programs are portable across\ndifferent architectures. \n \n \n BPF programs work in concert with the kernel, they make use of existing kernel\ninfrastructure (e.g. drivers, netdevices, tunnels, protocol stack, sockets) and\ntooling (e.g. iproute2) as well as the safety guarantees which the kernel provides.\nUnlike kernel modules, BPF programs are verified through an in-kernel verifier in\norder to ensure that they cannot crash the kernel, always terminate, etc. XDP\nprograms, for example, reuse the existing in-kernel drivers and operate on the\nprovided DMA buffers containing the packet frames without exposing them or an entire\ndriver to user space as in other models. Moreover, XDP programs reuse the existing\nstack instead of bypassing it. BPF can be considered a generic \"glue code\" between\nkernel facilities for crafting programs to solve specific use cases. \n \n \n The execution of a BPF program inside the kernel is always event-driven! Examples: \n \n \n A networking device which has a BPF program attached on its ingress path will\ntrigger the execution of the program once a packet is received. \n \n \n A kernel address which has a kprobe with a BPF program attached will trap once\nthe code at that address gets executed, which will then invoke the kprobe's\ncallback function for instrumentation, subsequently triggering the execution\nof the attached BPF program. \n \n \n BPF consists of eleven 64 bit registers with 32 bit subregisters, a program counter\nand a 512 byte large BPF stack space. Registers are named  r0  -  r10 . The\noperating mode is 64 bit by default, the 32 bit subregisters can only be accessed\nthrough special ALU (arithmetic logic unit) operations. The 32 bit lower subregisters\nzero-extend into 64 bit when they are being written to. \n Register  r10  is the only register which is read-only and contains the frame pointer\naddress in order to access the BPF stack space. The remaining  r0  -  r9 \nregisters are general purpose and of read/write nature. \n A BPF program can call into a predefined helper function, which is defined by\nthe core kernel (never by modules). The BPF calling convention is defined as\nfollows: \n \n r0  contains the return value of a helper function call. \n r1  -  r5  hold arguments from the BPF program to the kernel helper function. \n r6  -  r9  are callee saved registers that will be preserved on helper function call. \n \n The BPF calling convention is generic enough to map directly to  x86_64 ,  arm64 \nand other ABIs, thus all BPF registers map one to one to HW CPU registers, so that a\nJIT only needs to issue a call instruction, but no additional extra moves for placing\nfunction arguments. This calling convention was modeled to cover common call\nsituations without having a performance penalty. Calls with 6 or more arguments\nare currently not supported. The helper functions in the kernel which are dedicated\nto BPF ( BPF_CALL_0()  to  BPF_CALL_5()  functions) are specifically designed\nwith this convention in mind. \n Register  r0  is also the register containing the exit value for the BPF program.\nThe semantics of the exit value are defined by the type of program. Furthermore, when\nhanding execution back to the kernel, the exit value is passed as a 32 bit value. \n Registers  r1  -  r5  are scratch registers, meaning the BPF program needs to\neither spill them to the BPF stack or move them to callee saved registers if these\narguments are to be reused across multiple helper function calls. Spilling means\nthat the variable in the register is moved to the BPF stack. The reverse operation\nof moving the variable from the BPF stack to the register is called filling. The\nreason for spilling/filling is due to the limited number of registers. \n Upon entering execution of a BPF program, register  r1  initially contains the\ncontext for the program. The context is the input argument for the program (similar\nto  argc/argv  pair for a typical C program). BPF is restricted to work on a single\ncontext. The context is defined by the program type, for example, a networking\nprogram can have a kernel representation of the network packet ( skb ) as the\ninput argument. \n The general operation of BPF is 64 bit to follow the natural model of 64 bit\narchitectures in order to perform pointer arithmetics, pass pointers but also pass 64\nbit values into helper functions, and to allow for 64 bit atomic operations. \n The maximum instruction limit per program is restricted to 4096 BPF instructions,\nwhich, by design, means that any program will terminate quickly. For kernel newer\nthan 5.1 this limit was lifted to 1 million BPF instructions. Although the\ninstruction set contains forward as well as backward jumps, the in-kernel BPF\nverifier will forbid loops so that termination is always guaranteed. Since BPF\nprograms run inside the kernel, the verifier's job is to make sure that these are\nsafe to run, not affecting the system's stability. This means that from an instruction\nset point of view, loops can be implemented, but the verifier will restrict that.\nHowever, there is also a concept of tail calls that allows for one BPF program to\njump into another one. This, too, comes with an upper nesting limit of 33 calls,\nand is usually used to decouple parts of the program logic, for example, into stages. \n The instruction format is modeled as two operand instructions, which helps mapping\nBPF instructions to native instructions during JIT phase. The instruction set is\nof fixed size, meaning every instruction has 64 bit encoding. Currently, 87 instructions\nhave been implemented and the encoding also allows to extend the set with further\ninstructions when needed. The instruction encoding of a single 64 bit instruction on a\nbig-endian machine is defined as a bit sequence from most significant bit (MSB) to least\nsignificant bit (LSB) of  op:8 ,  dst_reg:4 ,  src_reg:4 ,  off:16 ,  imm:32 .\n off  and  imm  is of signed type. The encodings are part of the kernel headers and\ndefined in  linux/bpf.h  header, which also includes  linux/bpf_common.h . \n op  defines the actual operation to be performed. Most of the encoding for  op \nhas been reused from cBPF. The operation can be based on register or immediate\noperands. The encoding of  op  itself provides information on which mode to use\n( BPF_X  for denoting register-based operations, and  BPF_K  for immediate-based\noperations respectively). In the latter case, the destination operand is always\na register. Both  dst_reg  and  src_reg  provide additional information about\nthe register operands to be used (e.g.  r0  -  r9 ) for the operation.  off \nis used in some instructions to provide a relative offset, for example, for addressing\nthe stack or other buffers available to BPF (e.g. map values, packet data, etc),\nor jump targets in jump instructions.  imm  contains a constant / immediate value. \n The available  op  instructions can be categorized into various instruction\nclasses. These classes are also encoded inside the  op  field. The  op  field\nis divided into (from MSB to LSB)  code:4 ,  source:1  and  class:3 .  class \nis the more generic instruction class,  code  denotes a specific operational\ncode inside that class, and  source  tells whether the source operand is a register\nor an immediate value. Possible instruction classes include: \n \n \n BPF_LD ,  BPF_LDX : Both classes are for load operations.  BPF_LD  is\nused for loading a double word as a special instruction spanning two instructions\ndue to the  imm:32  split, and for byte / half-word / word loads of packet data.\nThe latter was carried over from cBPF mainly in order to keep cBPF to BPF\ntranslations efficient, since they have optimized JIT code. For native BPF\nthese packet load instructions are less relevant nowadays.  BPF_LDX  class\nholds instructions for byte / half-word / word / double-word loads out of\nmemory. Memory in this context is generic and could be stack memory, map value\ndata, packet data, etc. \n \n \n BPF_ST ,  BPF_STX : Both classes are for store operations. Similar to  BPF_LDX \nthe  BPF_STX  is the store counterpart and is used to store the data from a\nregister into memory, which, again, can be stack memory, map value, packet data,\netc.  BPF_STX  also holds special instructions for performing word and double-word\nbased atomic add operations, which can be used for counters, for example. The\n BPF_ST  class is similar to  BPF_STX  by providing instructions for storing\ndata into memory only that the source operand is an immediate value. \n \n \n BPF_ALU ,  BPF_ALU64 : Both classes contain ALU operations. Generally,\n BPF_ALU  operations are in 32 bit mode and  BPF_ALU64  in 64 bit mode.\nBoth ALU classes have basic operations with source operand which is register-based\nand an immediate-based counterpart. Supported by both are add ( + ), sub ( - ),\nand ( & ), or ( | ), left shift ( << ), right shift ( >> ), xor ( ^ ),\nmul ( * ), div ( / ), mod ( % ), neg ( ~ ) operations. Also mov ( <X> := <Y> )\nwas added as a special ALU operation for both classes in both operand modes.\n BPF_ALU64  also contains a signed right shift.  BPF_ALU  additionally\ncontains endianness conversion instructions for half-word / word / double-word\non a given source register. \n \n \n BPF_JMP : This class is dedicated to jump operations. Jumps can be unconditional\nand conditional. Unconditional jumps simply move the program counter forward, so\nthat the next instruction to be executed relative to the current instruction is\n off + 1 , where  off  is the constant offset encoded in the instruction. Since\n off  is signed, the jump can also be performed backwards as long as it does not\ncreate a loop and is within program bounds. Conditional jumps operate on both,\nregister-based and immediate-based source operands. If the condition in the jump\noperations results in  true , then a relative jump to  off + 1  is performed,\notherwise the next instruction ( 0 + 1 ) is performed. This fall-through\njump logic differs compared to cBPF and allows for better branch prediction as it\nfits the CPU branch predictor logic more naturally. Available conditions are\njeq ( == ), jne ( != ), jgt ( > ), jge ( >= ), jsgt (signed  > ), jsge\n(signed  >= ), jlt ( < ), jle ( <= ), jslt (signed  < ), jsle (signed\n <= ) and jset (jump if  DST & SRC ). Apart from that, there are three\nspecial jump operations within this class: the exit instruction which will leave\nthe BPF program and return the current value in  r0  as a return code, the call\ninstruction, which will issue a function call into one of the available BPF helper\nfunctions, and a hidden tail call instruction, which will jump into a different\nBPF program. \n \n \n The Linux kernel is shipped with a BPF interpreter which executes programs assembled in\nBPF instructions. Even cBPF programs are translated into eBPF programs transparently\nin the kernel, except for architectures that still ship with a cBPF JIT and\nhave not yet migrated to an eBPF JIT. \n Currently  x86_64 ,  arm64 ,  ppc64 ,  s390x ,  mips64 ,  sparc64  and\n arm  architectures come with an in-kernel eBPF JIT compiler. \n All BPF handling such as loading of programs into the kernel or creation of BPF maps\nis managed through a central  bpf()  system call. It is also used for managing map\nentries (lookup / update / delete), and making programs as well as maps persistent\nin the BPF file system through pinning. \n Helper Functions \n Helper functions are a concept which enables BPF programs to consult a core kernel\ndefined set of function calls in order to retrieve / push data from / to the\nkernel. Available helper functions may differ for each BPF program type,\nfor example, BPF programs attached to sockets are only allowed to call into\na subset of helpers compared to BPF programs attached to the tc layer.\nEncapsulation and decapsulation helpers for lightweight tunneling constitute\nan example of functions which are only available to lower tc layers, whereas\nevent output helpers for pushing notifications to user space are available to\ntc and XDP programs. \n Each helper function is implemented with a commonly shared function signature\nsimilar to system calls. The signature is defined as: \n .. code-block:: c \n u64 fn(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5)\n \n The calling convention as described in the previous section applies to all\nBPF helper functions. \n The kernel abstracts helper functions into macros  BPF_CALL_0()  to  BPF_CALL_5() \nwhich are similar to those of system calls. The following example is an extract\nfrom a helper function which updates map elements by calling into the\ncorresponding map implementation callbacks: \n .. code-block:: c \n BPF_CALL_4(bpf_map_update_elem, struct bpf_map *, map, void *, key,\n           void *, value, u64, flags)\n{\n    WARN_ON_ONCE(!rcu_read_lock_held());\n    return map->ops->map_update_elem(map, key, value, flags);\n}\n\nconst struct bpf_func_proto bpf_map_update_elem_proto = {\n    .func           = bpf_map_update_elem,\n    .gpl_only       = false,\n    .ret_type       = RET_INTEGER,\n    .arg1_type      = ARG_CONST_MAP_PTR,\n    .arg2_type      = ARG_PTR_TO_MAP_KEY,\n    .arg3_type      = ARG_PTR_TO_MAP_VALUE,\n    .arg4_type      = ARG_ANYTHING,\n};\n \n There are various advantages of this approach: while cBPF overloaded its\nload instructions in order to fetch data at an impossible packet offset to\ninvoke auxiliary helper functions, each cBPF JIT needed to implement support\nfor such a cBPF extension. In case of eBPF, each newly added helper function\nwill be JIT compiled in a transparent and efficient way, meaning that the JIT\ncompiler only needs to emit a call instruction since the register mapping\nis made in such a way that BPF register assignments already match the\nunderlying architecture's calling convention. This allows for easily extending\nthe core kernel with new helper functionality. All BPF helper functions are\npart of the core kernel and cannot be extended or added through kernel modules. \n The aforementioned function signature also allows the verifier to perform type\nchecks. The above  struct bpf_func_proto  is used to hand all the necessary\ninformation that needs to be known about the helper to the verifier, so that\nthe verifier can make sure that the expected types from the helper match the\ncurrent contents of the BPF program's analyzed registers. \n Argument types can range from passing in any kind of value up to restricted\ncontents such as a pointer / size pair for the BPF stack buffer, which the\nhelper should read from or write to. In the latter case, the verifier can also\nperform additional checks, for example, whether the buffer was previously\ninitialized. \n The list of available BPF helper functions is rather long and constantly growing,\nfor example, at the time of this writing, tc BPF programs can choose from 38\ndifferent BPF helpers. The kernel's  struct bpf_verifier_ops  contains a\n get_func_proto  callback function that provides the mapping of a specific\n enum bpf_func_id  to one of the available helpers for a given BPF program\ntype. \n Maps \n .. image:: /images/bpf_map.png\n:align: center \n Maps are efficient key / value stores that reside in kernel space. They can be\naccessed from a BPF program in order to keep state among multiple BPF program\ninvocations. They can also be accessed through file descriptors from user space\nand can be arbitrarily shared with other BPF programs or user space applications. \n BPF programs which share maps with each other are not required to be of the same\nprogram type, for example, tracing programs can share maps with networking programs.\nA single BPF program can currently access up to 64 different maps directly. \n Map implementations are provided by the core kernel. There are generic maps with\nper-CPU and non-per-CPU flavor that can read / write arbitrary data, but there are\nalso a few non-generic maps that are used along with helper functions. \n Generic maps currently available are  BPF_MAP_TYPE_HASH ,  BPF_MAP_TYPE_ARRAY ,\n BPF_MAP_TYPE_PERCPU_HASH ,  BPF_MAP_TYPE_PERCPU_ARRAY ,  BPF_MAP_TYPE_LRU_HASH ,\n BPF_MAP_TYPE_LRU_PERCPU_HASH  and  BPF_MAP_TYPE_LPM_TRIE . They all use the\nsame common set of BPF helper functions in order to perform lookup, update or\ndelete operations while implementing a different backend with differing semantics\nand performance characteristics. \n Non-generic maps that are currently in the kernel are  BPF_MAP_TYPE_PROG_ARRAY ,\n BPF_MAP_TYPE_PERF_EVENT_ARRAY ,  BPF_MAP_TYPE_CGROUP_ARRAY ,\n BPF_MAP_TYPE_STACK_TRACE ,  BPF_MAP_TYPE_ARRAY_OF_MAPS ,\n BPF_MAP_TYPE_HASH_OF_MAPS . For example,  BPF_MAP_TYPE_PROG_ARRAY  is an\narray map which holds other BPF programs,  BPF_MAP_TYPE_ARRAY_OF_MAPS  and\n BPF_MAP_TYPE_HASH_OF_MAPS  both hold pointers to other maps such that entire\nBPF maps can be atomically replaced at runtime. These types of maps tackle a\nspecific issue which was unsuitable to be implemented solely through a BPF helper\nfunction since additional (non-data) state is required to be held across BPF\nprogram invocations. \n Object Pinning \n .. image:: /images/bpf_fs.png\n:align: center \n BPF maps and programs act as a kernel resource and can only be accessed through\nfile descriptors, backed by anonymous inodes in the kernel. Advantages, but\nalso a number of disadvantages come along with them: \n User space applications can make use of most file descriptor related APIs,\nfile descriptor passing for Unix domain sockets work transparently, etc, but\nat the same time, file descriptors are limited to a processes' lifetime,\nwhich makes options like map sharing rather cumbersome to carry out. \n Thus, it brings a number of complications for certain use cases such as iproute2,\nwhere tc or XDP sets up and loads the program into the kernel and terminates\nitself eventually. With that, also access to maps is unavailable from user\nspace side, where it could otherwise be useful, for example, when maps are\nshared between ingress and egress locations of the data path. Also, third\nparty applications may wish to monitor or update map contents during BPF\nprogram runtime. \n To overcome this limitation, a minimal kernel space BPF file system has been\nimplemented, where BPF map and programs can be pinned to, a process called\nobject pinning. The BPF system call has therefore been extended with two new\ncommands which can pin ( BPF_OBJ_PIN ) or retrieve ( BPF_OBJ_GET ) a\npreviously pinned object. \n For instance, tools such as tc make use of this infrastructure for sharing\nmaps on ingress and egress. The BPF related file system is not a singleton,\nit does support multiple mount instances, hard and soft links, etc. \n Tail Calls \n .. image:: /images/bpf_tailcall.png\n:align: center \n Another concept that can be used with BPF is called tail calls. Tail calls can\nbe seen as a mechanism that allows one BPF program to call another, without\nreturning back to the old program. Such a call has minimal overhead as unlike\nfunction calls, it is implemented as a long jump, reusing the same stack frame. \n Such programs are verified independently of each other, thus for transferring\nstate, either per-CPU maps as scratch buffers or in case of tc programs,  skb \nfields such as the  cb[]  area must be used. \n Only programs of the same type can be tail called, and they also need to match\nin terms of JIT compilation, thus either JIT compiled or only interpreted programs\ncan be invoked, but not mixed together. \n There are two components involved for carrying out tail calls: the first part\nneeds to setup a specialized map called program array ( BPF_MAP_TYPE_PROG_ARRAY )\nthat can be populated by user space with key / values, where values are the\nfile descriptors of the tail called BPF programs, the second part is a\n bpf_tail_call()  helper where the context, a reference to the program array\nand the lookup key is passed to. Then the kernel inlines this helper call\ndirectly into a specialized BPF instruction. Such a program array is currently\nwrite-only from user space side. \n The kernel looks up the related BPF program from the passed file descriptor\nand atomically replaces program pointers at the given map slot. When no map\nentry has been found at the provided key, the kernel will just \"fall through\"\nand continue execution of the old program with the instructions following\nafter the  bpf_tail_call() . Tail calls are a powerful utility, for example,\nparsing network headers could be structured through tail calls. During runtime,\nfunctionality can be added or replaced atomically, and thus altering the BPF\nprogram's execution behavior. \n .. _bpf_to_bpf_calls: \n BPF to BPF Calls \n .. image:: /images/bpf_call.png\n:align: center \n Aside from BPF helper calls and BPF tail calls, a more recent feature that has\nbeen added to the BPF core infrastructure is BPF to BPF calls. Before this\nfeature was introduced into the kernel, a typical BPF C program had to declare\nany reusable code that, for example, resides in headers as  always_inline \nsuch that when LLVM compiles and generates the BPF object file all these\nfunctions were inlined and therefore duplicated many times in the resulting\nobject file, artificially inflating its code size: \n .. code-block:: c \n #include <linux/bpf.h>\n\n#ifndef __section\n# define __section(NAME)                  \\\n   __attribute__((section(NAME), used))\n#endif\n\n#ifndef __inline\n# define __inline                         \\\n   inline __attribute__((always_inline))\n#endif\n\nstatic __inline int foo(void)\n{\n    return XDP_DROP;\n}\n\n__section(\"prog\")\nint xdp_drop(struct xdp_md *ctx)\n{\n    return foo();\n}\n\nchar __license[] __section(\"license\") = \"GPL\";\n \n The main reason why this was necessary was due to lack of function call support\nin the BPF program loader as well as verifier, interpreter and JITs. Starting\nwith Linux kernel 4.16 and LLVM 6.0 this restriction got lifted and BPF programs\nno longer need to use  always_inline  everywhere. Thus, the prior shown BPF\nexample code can then be rewritten more naturally as: \n .. code-block:: c \n #include <linux/bpf.h>\n\n#ifndef __section\n# define __section(NAME)                  \\\n   __attribute__((section(NAME), used))\n#endif\n\nstatic int foo(void)\n{\n    return XDP_DROP;\n}\n\n__section(\"prog\")\nint xdp_drop(struct xdp_md *ctx)\n{\n    return foo();\n}\n\nchar __license[] __section(\"license\") = \"GPL\";\n \n Mainstream BPF JIT compilers like  x86_64  and  arm64  support BPF to BPF\ncalls today with others following in near future. BPF to BPF call is an\nimportant performance optimization since it heavily reduces the generated BPF\ncode size and therefore becomes friendlier to a CPU's instruction cache. \n The calling convention known from BPF helper function applies to BPF to BPF\ncalls just as well, meaning  r1  up to  r5  are for passing arguments to\nthe callee and the result is returned in  r0 .  r1  to  r5  are scratch\nregisters whereas  r6  to  r9  preserved across calls the usual way. The\nmaximum number of nesting calls respectively allowed call frames is  8 .\nA caller can pass pointers (e.g. to the caller's stack frame) down to the\ncallee, but never vice versa. \n BPF JIT compilers emit separate images for each function body and later fix\nup the function call addresses in the image in a final JIT pass. This has\nproven to require minimal changes to the JITs in that they can treat BPF to\nBPF calls as conventional BPF helper calls. \n Up to kernel 5.9, BPF tail calls and BPF subprograms excluded each other. BPF\nprograms that utilized tail calls couldn't take the benefit of reducing program\nimage size and faster load times. Linux kernel 5.10 finally allows users to bring\nthe best of two worlds and adds the ability to combine the BPF subprograms with\ntail calls. \n This improvement comes with some restrictions, though. Mixing these two features\ncan cause a kernel stack overflow. To get an idea of what might happen, see the\npicture below that illustrates the mix of bpf2bpf calls and tail calls: \n .. image:: /images/bpf_tailcall_subprograms.png\n:align: center \n Tail calls, before the actual jump to the target program, will unwind only its\ncurrent stack frame. As we can see in the example above, if a tail call occurs\nfrom within the sub-function, the function's (func1) stack frame will be\npresent on the stack when a program execution is at func2. Once the final\nfunction (func3) function terminates, all the previous stack frames will be\nunwinded and control will get back to the caller of BPF program caller. \n The kernel introduced additional logic for detecting this feature combination.\nThere is a limit on the stack size throughout the whole call chain down to 256\nbytes per subprogram (note that if the verifier detects the bpf2bpf call, then\nthe main function is treated as a sub-function as well). In total, with this\nrestriction, the BPF program's call chain can consume at most 8KB of stack\nspace. This limit comes from the 256 bytes per stack frame multiplied by the\ntail call count limit (33). Without this, the BPF programs will operate on\n512-byte stack size, yielding the 16KB size in total for the maximum count of\ntail calls that would overflow the stack on some architectures. \n One more thing to mention is that this feature combination is currently\nsupported only on the x86-64 architecture. \n JIT \n .. image:: /images/bpf_jit.png\n:align: center \n The 64 bit  x86_64 ,  arm64 ,  ppc64 ,  s390x ,  mips64 ,  sparc64 \nand 32 bit  arm ,  x86_32  architectures are all shipped with an in-kernel\neBPF JIT compiler, also all of them are feature equivalent and can be enabled\nthrough: \n .. code-block:: shell-session \n # echo 1 > /proc/sys/net/core/bpf_jit_enable\n \n The 32 bit  mips ,  ppc  and  sparc  architectures currently have a cBPF\nJIT compiler. The mentioned architectures still having a cBPF JIT as well as all\nremaining architectures supported by the Linux kernel which do not have a BPF JIT\ncompiler at all need to run eBPF programs through the in-kernel interpreter. \n In the kernel's source tree, eBPF JIT support can be easily determined through\nissuing a grep for  HAVE_EBPF_JIT : \n .. code-block:: shell-session \n # git grep HAVE_EBPF_JIT arch/\narch/arm/Kconfig:       select HAVE_EBPF_JIT   if !CPU_ENDIAN_BE32\narch/arm64/Kconfig:     select HAVE_EBPF_JIT\narch/powerpc/Kconfig:   select HAVE_EBPF_JIT   if PPC64\narch/mips/Kconfig:      select HAVE_EBPF_JIT   if (64BIT && !CPU_MICROMIPS)\narch/s390/Kconfig:      select HAVE_EBPF_JIT   if PACK_STACK && HAVE_MARCH_Z196_FEATURES\narch/sparc/Kconfig:     select HAVE_EBPF_JIT   if SPARC64\narch/x86/Kconfig:       select HAVE_EBPF_JIT   if X86_64\n \n JIT compilers speed up execution of the BPF program significantly since they\nreduce the per instruction cost compared to the interpreter. Often instructions\ncan be mapped 1:1 with native instructions of the underlying architecture. This\nalso reduces the resulting executable image size and is therefore more\ninstruction cache friendly to the CPU. In particular in case of CISC instruction\nsets such as  x86 , the JITs are optimized for emitting the shortest possible\nopcodes for a given instruction to shrink the total necessary size for the\nprogram translation. \n Hardening \n BPF locks the entire BPF interpreter image ( struct bpf_prog ) as well\nas the JIT compiled image ( struct bpf_binary_header ) in the kernel as\nread-only during the program's lifetime in order to prevent the code from\npotential corruptions. Any corruption happening at that point, for example,\ndue to some kernel bugs will result in a general protection fault and thus\ncrash the kernel instead of allowing the corruption to happen silently. \n Architectures that support setting the image memory as read-only can be\ndetermined through: \n .. code-block:: shell-session \n $ git grep ARCH_HAS_SET_MEMORY | grep select\narch/arm/Kconfig:    select ARCH_HAS_SET_MEMORY\narch/arm64/Kconfig:  select ARCH_HAS_SET_MEMORY\narch/s390/Kconfig:   select ARCH_HAS_SET_MEMORY\narch/x86/Kconfig:    select ARCH_HAS_SET_MEMORY\n \n The option  CONFIG_ARCH_HAS_SET_MEMORY  is not configurable, thanks to\nwhich this protection is always built in. Other architectures might follow\nin the future. \n In case of the  x86_64  JIT compiler, the JITing of the indirect jump from\nthe use of tail calls is realized through a retpoline in case  CONFIG_RETPOLINE \nhas been set which is the default at the time of writing in most modern Linux\ndistributions. \n In case of  /proc/sys/net/core/bpf_jit_harden  set to  1  additional\nhardening steps for the JIT compilation take effect for unprivileged users.\nThis effectively trades off their performance slightly by decreasing a\n(potential) attack surface in case of untrusted users operating on the\nsystem. The decrease in program execution still results in better performance\ncompared to switching to interpreter entirely. \n Currently, enabling hardening will blind all user provided 32 bit and 64 bit\nconstants from the BPF program when it gets JIT compiled in order to prevent\nJIT spraying attacks which inject native opcodes as immediate values. This is\nproblematic as these immediate values reside in executable kernel memory,\ntherefore, a jump that could be triggered from some kernel bug would jump to\nthe start of the immediate value and then execute these as native instructions. \n JIT constant blinding prevents this due to randomizing the actual instruction,\nwhich means the operation is transformed from an immediate based source operand\nto a register based one through rewriting the instruction by splitting the\nactual load of the value into two steps: 1) load of a blinded immediate\nvalue  rnd ^ imm  into a register, 2) xoring that register with  rnd \nsuch that the original  imm  immediate then resides in the register and\ncan be used for the actual operation. The example was provided for a load\noperation, but really all generic operations are blinded. \n Example of JITing a program with hardening disabled: \n .. code-block:: shell-session \n # echo 0 > /proc/sys/net/core/bpf_jit_harden\n\n  ffffffffa034f5e9 + <x>:\n  [...]\n  39:   mov    $0xa8909090,%eax\n  3e:   mov    $0xa8909090,%eax\n  43:   mov    $0xa8ff3148,%eax\n  48:   mov    $0xa89081b4,%eax\n  4d:   mov    $0xa8900bb0,%eax\n  52:   mov    $0xa810e0c1,%eax\n  57:   mov    $0xa8908eb4,%eax\n  5c:   mov    $0xa89020b0,%eax\n  [...]\n \n The same program gets constant blinded when loaded through BPF\nas an unprivileged user in the case hardening is enabled: \n .. code-block:: shell-session \n # echo 1 > /proc/sys/net/core/bpf_jit_harden\n\n  ffffffffa034f1e5 + <x>:\n  [...]\n  39:   mov    $0xe1192563,%r10d\n  3f:   xor    $0x4989b5f3,%r10d\n  46:   mov    %r10d,%eax\n  49:   mov    $0xb8296d93,%r10d\n  4f:   xor    $0x10b9fd03,%r10d\n  56:   mov    %r10d,%eax\n  59:   mov    $0x8c381146,%r10d\n  5f:   xor    $0x24c7200e,%r10d\n  66:   mov    %r10d,%eax\n  69:   mov    $0xeb2a830e,%r10d\n  6f:   xor    $0x43ba02ba,%r10d\n  76:   mov    %r10d,%eax\n  79:   mov    $0xd9730af,%r10d\n  7f:   xor    $0xa5073b1f,%r10d\n  86:   mov    %r10d,%eax\n  89:   mov    $0x9a45662b,%r10d\n  8f:   xor    $0x325586ea,%r10d\n  96:   mov    %r10d,%eax\n  [...]\n \n Both programs are semantically the same, only that none of the\noriginal immediate values are visible anymore in the disassembly of\nthe second program. \n At the same time, hardening also disables any JIT kallsyms exposure\nfor privileged users, preventing that JIT image addresses are not\nexposed to  /proc/kallsyms  anymore. \n Moreover, the Linux kernel provides the option  CONFIG_BPF_JIT_ALWAYS_ON \nwhich removes the entire BPF interpreter from the kernel and permanently\nenables the JIT compiler. This has been developed as part of a mitigation\nin the context of Spectre v2 such that when used in a VM-based setting,\nthe guest kernel is not going to reuse the host kernel's BPF interpreter\nwhen mounting an attack anymore. For container-based environments, the\n CONFIG_BPF_JIT_ALWAYS_ON  configuration option is optional, but in\ncase JITs are enabled there anyway, the interpreter may as well be compiled\nout to reduce the kernel's complexity. Thus, it is also generally\nrecommended for widely used JITs in case of mainstream architectures\nsuch as  x86_64  and  arm64 . \n Last but not least, the kernel offers an option to disable the use of\nthe  bpf(2)  system call for unprivileged users through the\n /proc/sys/kernel/unprivileged_bpf_disabled  sysctl knob. This is\non purpose a one-time kill switch, meaning once set to  1 , there is\nno option to reset it back to  0  until a new kernel reboot. When\nset only  CAP_SYS_ADMIN  privileged processes out of the initial\nnamespace are allowed to use the  bpf(2)  system call from that\npoint onwards. Upon start, Cilium sets this knob to  1  as well. \n .. code-block:: shell-session \n # echo 1 > /proc/sys/kernel/unprivileged_bpf_disabled\n \n Offloads \n .. image:: /images/bpf_offload.png\n:align: center \n Networking programs in BPF, in particular for tc and XDP do have an\noffload-interface to hardware in the kernel in order to execute BPF\ncode directly on the NIC. \n Currently, the  nfp  driver from Netronome has support for offloading\nBPF through a JIT compiler which translates BPF instructions to an\ninstruction set implemented against the NIC. This includes offloading\nof BPF maps to the NIC as well, thus the offloaded BPF program can\nperform map lookups, updates and deletions. \n BPF sysctls \n The Linux kernel provides few sysctls that are BPF related and covered in this section. \n \n \n /proc/sys/net/core/bpf_jit_enable : Enables or disables the BPF JIT compiler. \n +-------+-------------------------------------------------------------------+\n| Value | Description                                                       |\n+-------+-------------------------------------------------------------------+\n| 0     | Disable the JIT and use only interpreter (kernel's default value) |\n+-------+-------------------------------------------------------------------+\n| 1     | Enable the JIT compiler                                           |\n+-------+-------------------------------------------------------------------+\n| 2     | Enable the JIT and emit debugging traces to the kernel log        |\n+-------+-------------------------------------------------------------------+ \n As described in subsequent sections,  bpf_jit_disasm  tool can be used to\nprocess debugging traces when the JIT compiler is set to debugging mode (option  2 ). \n \n \n /proc/sys/net/core/bpf_jit_harden : Enables or disables BPF JIT hardening.\nNote that enabling hardening trades off performance, but can mitigate JIT spraying\nby blinding out the BPF program's immediate values. For programs processed through\nthe interpreter, blinding of immediate values is not needed / performed. \n +-------+-------------------------------------------------------------------+\n| Value | Description                                                       |\n+-------+-------------------------------------------------------------------+\n| 0     | Disable JIT hardening (kernel's default value)                    |\n+-------+-------------------------------------------------------------------+\n| 1     | Enable JIT hardening for unprivileged users only                  |\n+-------+-------------------------------------------------------------------+\n| 2     | Enable JIT hardening for all users                                |\n+-------+-------------------------------------------------------------------+ \n \n \n /proc/sys/net/core/bpf_jit_kallsyms : Enables or disables export of JITed\nprograms as kernel symbols to  /proc/kallsyms  so that they can be used together\nwith  perf  tooling as well as making these addresses aware to the kernel for\nstack unwinding, for example, used in dumping stack traces. The symbol names\ncontain the BPF program tag ( bpf_prog_<tag> ). If  bpf_jit_harden  is enabled,\nthen this feature is disabled. \n +-------+-------------------------------------------------------------------+\n| Value | Description                                                       |\n+-------+-------------------------------------------------------------------+\n| 0     | Disable JIT kallsyms export (kernel's default value)              |\n+-------+-------------------------------------------------------------------+\n| 1     | Enable JIT kallsyms export for privileged users only              |\n+-------+-------------------------------------------------------------------+ \n \n \n /proc/sys/kernel/unprivileged_bpf_disabled : Enables or disable unprivileged\nuse of the  bpf(2)  system call. The Linux kernel has unprivileged use of\n bpf(2)  enabled by default. \n Once the value is set to 1, unprivileged use will be permanently disabled until\nthe next reboot, neither an application nor an admin can reset the value anymore. \n The value can also be set to 2, which means it can be changed at runtime to 0 or\n1 later while disabling the unprivileged used for now. This value was added\nin Linux 5.13. If  BPF_UNPRIV_DEFAULT_OFF \nis enabled in the kernel config, then this knob will default to 2 instead of 0. \n This knob does not affect any cBPF programs such as seccomp\nor traditional socket filters that do not use the  bpf(2)  system call for\nloading the program into the kernel. \n +-------+---------------------------------------------------------------------+\n| Value | Description                                                         |\n+-------+---------------------------------------------------------------------+\n| 0     | Unprivileged use of bpf syscall enabled (kernel's default value)    |\n+-------+---------------------------------------------------------------------+\n| 1     | Unprivileged use of bpf syscall disabled (until reboot)             |\n+-------+---------------------------------------------------------------------+\n| 2     | Unprivileged use of bpf syscall disabled                            |\n|       | (default if  BPF_UNPRIV_DEFAULT_OFF  is enabled in kernel config) |\n+-------+---------------------------------------------------------------------+",
  "item_type": "unknown",
  "module_path": "/tmp/cilium-repo/Documentation/reference-guides/bpf/architecture.rst",
  "extracted_at": "2025-09-03T01:13:28.777134Z"
}