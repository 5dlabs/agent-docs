{
  "url": "file:///tmp/cilium-repo/Documentation/security/policy-creation.rst",
  "content": ".. only:: not (epub or latex or html)\n\n    WARNING: You are looking at unreleased Cilium documentation.\n    Please use the official rendered version released here:\n    https://docs.cilium.io\n\n.. _policy_verdicts:\n\n*******************************\nCreating Policies from Verdicts\n*******************************\n\nPolicy Audit Mode configures Cilium to allow all traffic while logging all\nconnections that would otherwise be dropped by network policies. Policy Audit\nMode may be configured for the entire daemon using ``--policy-audit-mode=true``\nor for individual Cilium Endpoints. When Policy Audit Mode is enabled, no\nnetwork policy is enforced so this setting is **not recommended for production\ndeployment**. Policy Audit Mode supports auditing network policies implemented\nat networks layers 3 and 4. This guide walks through the process of creating\npolicies using Policy Audit Mode.\n\n.. include:: gsg_requirements.rst\n.. include:: gsg_sw_demo.rst\n\nScale down the deathstar Deployment\n===================================\n\nIn this guide we're going to scale down the deathstar Deployment in order to\nsimplify the next steps:\n\n.. code-block:: shell-session\n\n   $ kubectl scale --replicas=1 deployment deathstar\n   deployment.apps/deathstar scaled\n\nEnable Policy Audit Mode (Entire Daemon)\n========================================\n\nTo observe policy audit messages for all endpoints managed by this Daemonset,\nmodify the Cilium ConfigMap and restart all daemons:\n\n   .. tabs::\n\n      .. group-tab:: Configure via kubectl\n\n         .. code-block:: shell-session\n\n            $ kubectl patch -n $CILIUM_NAMESPACE configmap cilium-config --type merge --patch '{\"data\":{\"policy-audit-mode\":\"true\"}}'\n            configmap/cilium-config patched\n            $ kubectl -n $CILIUM_NAMESPACE rollout restart ds/cilium\n            daemonset.apps/cilium restarted\n            $ kubectl -n $CILIUM_NAMESPACE rollout status ds/cilium\n            Waiting for daemon set \"cilium\" rollout to finish: 0 of 1 updated pods are available...\n            daemon set \"cilium\" successfully rolled out\n\n      .. group-tab:: Helm Upgrade\n\n         If you installed Cilium via ``helm install``, then you can use ``helm\n         upgrade`` to enable Policy Audit Mode:\n\n         .. parsed-literal::\n\n            $ helm upgrade cilium |CHART_RELEASE| \\\\\n                --namespace $CILIUM_NAMESPACE \\\\\n                --reuse-values \\\\\n                --set policyAuditMode=true\n\n\nEnable Policy Audit Mode (Specific Endpoint)\n============================================\n\nCilium can enable Policy Audit Mode for a specific endpoint. This may be helpful when enabling\nPolicy Audit Mode for the entire daemon is too broad. Enabling per endpoint will ensure other\nendpoints managed by the same daemon are not impacted.\n\nThis approach is meant to be temporary.  **Restarting Cilium pod will reset the Policy Audit\nMode to match the daemon's configuration.**\n\nPolicy Audit Mode is enabled for a given endpoint by modifying the endpoint configuration via\nthe ``cilium-dbg`` tool on the endpoint's Kubernetes node. The steps include:\n\n#. Determine the endpoint id on which Policy Audit Mode will be enabled.\n#. Identify the Cilium pod running on the same Kubernetes node corresponding to the endpoint.\n#. Using the Cilium pod above, modify the endpoint configuration by setting ``PolicyAuditMode=Enabled``.\n\nThe following shell commands perform these steps:\n\n.. code-block:: shell-session\n\n   $ PODNAME=$(kubectl get pods -l app.kubernetes.io/name=deathstar -o jsonpath='{.items[*].metadata.name}')\n   $ NODENAME=$(kubectl get pod -o jsonpath=\"{.items[?(@.metadata.name=='$PODNAME')].spec.nodeName}\")\n   $ ENDPOINT=$(kubectl get cep -o jsonpath=\"{.items[?(@.metadata.name=='$PODNAME')].status.id}\")\n   $ CILIUM_POD=$(kubectl -n \"$CILIUM_NAMESPACE\" get pod --all-namespaces --field-selector spec.nodeName=\"$NODENAME\" -lk8s-app=cilium -o jsonpath='{.items[*].metadata.name}')\n   $ kubectl -n \"$CILIUM_NAMESPACE\" exec \"$CILIUM_POD\" -c cilium-agent -- \\\n       cilium-dbg endpoint config \"$ENDPOINT\" PolicyAuditMode=Enabled\n    Endpoint 232 configuration updated successfully\n\nWe can check that Policy Audit Mode is enabled for this endpoint with\n\n.. code-block:: shell-session\n\n   $ kubectl -n \"$CILIUM_NAMESPACE\" exec \"$CILIUM_POD\" -c cilium-agent -- \\\n       cilium-dbg endpoint get \"$ENDPOINT\" -o jsonpath='{[*].spec.options.PolicyAuditMode}'\n   Enabled\n\n.. _observe_policy_verdicts:\n\nObserve policy verdicts\n=======================\n\nIn this example, we are tasked with applying security policy for the deathstar.\nFirst, from the Cilium pod we need to monitor the notifications for policy\nverdicts using the Hubble CLI. We'll be monitoring for inbound traffic towards\nthe deathstar to identify it and determine whether to extend the network policy\nto allow that traffic.\n\nApply a default-deny policy:\n\n.. literalinclude:: ../../examples/minikube/sw_deny_policy.yaml\n     :language: yaml\n\nCiliumNetworkPolicies match on pod labels using an ``endpointSelector`` to identify\nthe sources and destinations to which the policy applies. The above policy denies\ntraffic sent to any pods with label (``org=empire``). Due to the Policy Audit Mode\nenabled above (either for the entire daemon, or for just the ``deathstar`` endpoint),\nthe traffic will not actually be denied but will instead trigger policy verdict\nnotifications.\n\nTo apply this policy, run:\n\n.. parsed-literal::\n\n    $ kubectl create -f \\ |SCM_WEB|\\/examples/minikube/sw_deny_policy.yaml\n    ciliumnetworkpolicy.cilium.io/empire-default-deny created\n\nWith the above policy, we will enable a default-deny posture on ingress to pods\nwith the label ``org=empire`` and enable the policy verdict notifications for\nthose pods. The same principle applies on egress as well.\n\nNow let's send some traffic from the tiefighter to the deathstar:\n\n.. code-block:: shell-session\n\n    $ kubectl exec tiefighter -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing\n    Ship landed\n\nWe can check the policy verdict from the Cilium Pod:\n\n.. code-block:: shell-session\n\n   $ kubectl -n \"$CILIUM_NAMESPACE\" exec \"$CILIUM_POD\" -c cilium-agent -- \\\n       hubble observe flows -t policy-verdict --last 1\n   Feb  7 12:53:39.168: default/tiefighter:54134 (ID:31028) -> default/deathstar-6fb5694d48-5hmds:80 (ID:16530) policy-verdict:none AUDITED (TCP Flags: SYN)\n\nIn the above example, we can see that the Pod ``deathstar-6fb5694d48-5hmds`` has\nreceived traffic from the ``tiefighter`` Pod which doesn't match the policy\n(``policy-verdict:none AUDITED``).\n\n.. _create_network_policy:\n\nCreate the Network Policy\n=========================\n\nWe can get more information about the flow with\n\n.. code-block:: shell-session\n\n   $ kubectl -n \"$CILIUM_NAMESPACE\" exec \"$CILIUM_POD\" -c cilium-agent -- \\\n       hubble observe flows -t policy-verdict -o json --last 1\n\nGiven the above information, we now know the labels of the source and\ndestination Pods, the traffic direction, and the destination port. In this case,\nwe can see clearly that the source (i.e. the tiefighter Pod) is an empire\naircraft (as it has the ``org=empire`` label) so once we've determined that we\nexpect this traffic to arrive at the deathstar, we can form a policy to match\nthe traffic:\n\n.. literalinclude:: ../../examples/minikube/sw_l3_l4_policy.yaml\n     :language: yaml\n\nTo apply this L3/L4 policy, run:\n\n.. parsed-literal::\n\n    $ kubectl create -f \\ |SCM_WEB|\\/examples/minikube/sw_l3_l4_policy.yaml\n    ciliumnetworkpolicy.cilium.io/rule1 created\n\nNow if we run the landing requests again,\n\n.. code-block:: shell-session\n\n    $ kubectl exec tiefighter -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing\n    Ship landed\n\nwe can then observe that the traffic which was previously audited to be dropped\nby the policy is reported as allowed:\n\n.. code-block:: shell-session\n\n   $ kubectl -n \"$CILIUM_NAMESPACE\" exec \"$CILIUM_POD\" -c cilium-agent -- \\\n       hubble observe flows -t policy-verdict --last 1\n   ...\n   Feb  7 13:06:45.130: default/tiefighter:59824 (ID:31028) -> default/deathstar-6fb5694d48-5hmds:80 (ID:16530) policy-verdict:L3-L4 ALLOWED (TCP Flags: SYN)\n\nNow the policy verdict states that the traffic would be allowed:\n``policy-verdict:L3-L4 ALLOWED``. Success!\n\nDisable Policy Audit Mode (Entire Daemon)\n=========================================\n\nThese steps should be repeated for each connection in the cluster to ensure\nthat the network policy allows all of the expected traffic. The final step\nafter deploying the policy is to disable Policy Audit Mode again:\n\n   .. tabs::\n\n      .. group-tab:: Configure via kubectl\n\n         .. code-block:: shell-session\n\n            $ kubectl patch -n $CILIUM_NAMESPACE configmap cilium-config --type merge --patch '{\"data\":{\"policy-audit-mode\":\"false\"}}'\n            configmap/cilium-config patched\n            $ kubectl -n $CILIUM_NAMESPACE rollout restart ds/cilium\n            daemonset.apps/cilium restarted\n            $ kubectl -n $CILIUM_NAMESPACE rollout status ds/cilium\n            Waiting for daemon set \"cilium\" rollout to finish: 0 of 1 updated pods are available...\n            daemon set \"cilium\" successfully rolled out\n\n      .. group-tab:: Helm Upgrade\n\n         .. parsed-literal::\n\n            $ helm upgrade cilium |CHART_RELEASE| \\\\\n                --namespace $CILIUM_NAMESPACE \\\\\n                --reuse-values \\\\\n                --set policyAuditMode=false\n\n\nDisable Policy Audit Mode (Specific Endpoint)\n=============================================\n\nThese steps are nearly identical to enabling Policy Audit Mode.\n\n.. code-block:: shell-session\n\n   $ PODNAME=$(kubectl get pods -l app.kubernetes.io/name=deathstar -o jsonpath='{.items[*].metadata.name}')\n   $ NODENAME=$(kubectl get pod -o jsonpath=\"{.items[?(@.metadata.name=='$PODNAME')].spec.nodeName}\")\n   $ ENDPOINT=$(kubectl get cep -o jsonpath=\"{.items[?(@.metadata.name=='$PODNAME')].status.id}\")\n   $ CILIUM_POD=$(kubectl -n \"$CILIUM_NAMESPACE\" get pod --all-namespaces --field-selector spec.nodeName=\"$NODENAME\" -lk8s-app=cilium -o jsonpath='{.items[*].metadata.name}')\n   $ kubectl -n \"$CILIUM_NAMESPACE\" exec \"$CILIUM_POD\" -c cilium-agent -- \\\n       cilium-dbg endpoint config \"$ENDPOINT\" PolicyAuditMode=Disabled\n    Endpoint 232 configuration updated successfully\n\nAlternatively, **restarting the Cilium pod** will set the endpoint Policy Audit Mode to the daemon set configuration.\n\n\nVerify Policy Audit Mode is Disabled\n====================================\n\n.. code-block:: shell-session\n\n   $ kubectl -n \"$CILIUM_NAMESPACE\" exec \"$CILIUM_POD\" -c cilium-agent -- \\\n       cilium-dbg endpoint get \"$ENDPOINT\" -o jsonpath='{[*].spec.options.PolicyAuditMode}'\n   Disabled\n\nNow if we run the landing requests again, only the *tiefighter* pods with the\nlabel ``org=empire`` should succeed:\n\n.. code-block:: shell-session\n\n    $ kubectl exec tiefighter -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing\n    Ship landed\n\nAnd we can observe that the traffic was allowed by the policy:\n\n.. code-block:: shell-session\n\n    $ kubectl -n \"$CILIUM_NAMESPACE\" exec \"$CILIUM_POD\" -c cilium-agent -- \\\n        hubble observe flows -t policy-verdict --from-pod tiefighter --last 1\n    Feb  7 13:34:26.112: default/tiefighter:37314 (ID:31028) -> default/deathstar-6fb5694d48-5hmds:80 (ID:16530) policy-verdict:L3-L4 ALLOWED (TCP Flags: SYN)\n\n\nThis works as expected. Now the same request from an *xwing* Pod should fail:\n\n.. code-block:: shell-session\n\n    $ kubectl exec xwing -- curl --connect-timeout 3 -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing\n    command terminated with exit code 28\n\nThis curl request should timeout after three seconds, we can observe the policy\nverdict with:\n\n.. code-block:: shell-session\n\n    $ kubectl -n \"$CILIUM_NAMESPACE\" exec \"$CILIUM_POD\" -c cilium-agent -- \\\n        hubble observe flows -t policy-verdict --from-pod xwing --last 1\n    Feb  7 13:43:46.791: default/xwing:54842 (ID:22654) <> default/deathstar-6fb5694d48-5hmds:80 (ID:16530) policy-verdict:none DENIED (TCP Flags: SYN)\n\n\nWe hope you enjoyed the tutorial.  Feel free to play more with the setup,\nfollow the `gs_http` guide, and reach out to us on `Cilium Slack`_ with any\nquestions!\n\nClean-up\n========\n\n.. parsed-literal::\n\n   $ kubectl delete -f \\ |SCM_WEB|\\/examples/minikube/http-sw-app.yaml\n   $ kubectl delete cnp empire-default-deny rule1\n",
  "item_type": "unknown",
  "module_path": "/tmp/cilium-repo/Documentation/security/policy-creation.rst",
  "extracted_at": "2025-09-03T00:53:44.707117Z"
}