{
  "url": "file:///tmp/cilium-repo/Documentation/network/ebpf/intro.rst",
  "content": ".. only:: not (epub or latex or html)\n\n    WARNING: You are looking at unreleased Cilium documentation.\n    Please use the official rendered version released here:\n    https://docs.cilium.io\n\n############\nIntroduction\n############\n\nThe Linux kernel supports a set of BPF hooks in the networking stack\nthat can be used to run BPF programs. The Cilium datapath uses these\nhooks to load BPF programs that when used together create higher level\nnetworking constructs.\n\nThe following is a list of the hooks used by Cilium and a brief\ndescription. For a more thorough documentation on specifics of each\nhook see :ref:`bpf_guide`.\n\n* **XDP:** The XDP BPF hook is at the earliest point possible in the networking driver\n  and triggers a run of the BPF program upon packet reception. This\n  achieves the best possible packet processing performance since the\n  program runs directly on the packet data before any other processing\n  can happen. This hook is ideal for running filtering programs that\n  drop malicious or unexpected traffic, and other common DDOS protection\n  mechanisms.\n\n* **Traffic Control Ingress/Egress:** BPF programs attached to the traffic\n  control (tc) ingress hook are attached to a networking interface, same as\n  XDP, but will run after the networking stack has done initial processing\n  of the packet. The hook is run before the L3 layer of the stack but has\n  access to most of the metadata associated with a packet. This is ideal\n  for doing local node processing, such as applying L3/L4 endpoint policy\n  and redirecting traffic to endpoints. For network-facing devices the\n  tc ingress hook can be coupled with above XDP hook. When this is done it\n  is reasonable to assume that the majority of the traffic at this\n  point is legitimate and destined for the host.\n\n  Containers typically use a virtual device called a veth pair which acts\n  as a virtual wire connecting the container to the host. By attaching to\n  the TC ingress hook of the host side of this veth pair Cilium can monitor\n  and enforce policy on all traffic exiting a container. By attaching a BPF\n  program to the veth pair associated with each container and routing all\n  network traffic to the host side virtual devices with another BPF program\n  attached to the tc ingress hook as well Cilium can monitor and enforce\n  policy on all traffic entering or exiting the node.\n\n* **Socket operations:** The socket operations hook is attached to a specific\n  cgroup and runs on TCP events. Cilium attaches a BPF socket operations\n  program to the root cgroup and uses this to monitor for TCP state transitions,\n  specifically for ESTABLISHED state transitions. When\n  a socket transitions into ESTABLISHED state if the TCP socket has a node\n  local peer (possibly a local proxy) a socket send/recv program is attached.\n\n* **Socket send/recv:** The socket send/recv hook runs on every send operation\n  performed by a TCP socket. At this point the hook can inspect the message\n  and either drop the message, send the message to the TCP layer, or redirect\n  the message to another socket. Cilium uses this to accelerate the datapath redirects\n  as described below.\n\nCombining the above hooks with virtual interfaces (cilium_host, cilium_net),\nan optional overlay interface (cilium_vxlan), Linux kernel crypto support and\na userspace proxy (Envoy) Cilium creates the following networking objects.\n\n* **Prefilter:** The prefilter object runs an XDP program and\n  provides a set of prefilter rules used to filter traffic from the network for best performance. Specifically,\n  a set of CIDR maps supplied by the Cilium agent are used to do a lookup and the packet\n  is either dropped, for example when the destination is not a valid endpoint, or allowed to be processed by the stack. This can be easily\n  extended as needed to build in new prefilter criteria/capabilities.\n\n* **Endpoint Policy:** The endpoint policy object implements the Cilium endpoint enforcement.\n  Using a map to lookup a packet's associated identity and policy, this layer\n  scales well to lots of endpoints. Depending on the policy this layer may drop the\n  packet, forward to a local endpoint, forward to the service object or forward to the\n  L7 Policy object for further L7 rules. This is the primary object in the Cilium\n  datapath responsible for mapping packets to identities and enforcing L3 and L4 policies.\n\n* **Service:** The Service object performs a map lookup on the destination IP\n  and optionally destination port for every packet received by the object.\n  If a matching entry is found, the packet will be forwarded to one of the\n  configured L3/L4 endpoints. The Service block can be used to implement a\n  standalone load balancer on any interface using the TC ingress hook or may\n  be integrated in the endpoint policy object.\n\n* **L3 Encryption:** On ingress the L3 Encryption object marks packets for\n  decryption, passes the packets to the Linux xfrm (transform) layer for\n  decryption, and after the packet is decrypted the object receives the packet\n  then passes it up the stack for further processing by other objects. Depending\n  on the mode, direct routing or overlay, this may be a BPF tail call or the\n  Linux routing stack that passes the packet to the next object. The key required\n  for decryption is encoded in the IPsec header so on ingress we do not need to\n  do a map lookup to find the decryption key.\n\n  On egress a map lookup is first performed using the destination IP to determine\n  if a packet should be encrypted and if so what keys are available on the destination\n  node. The most recent key available on both nodes is chosen and the\n  packet is marked for encryption. The packet is then passed to the Linux\n  xfrm layer where it is encrypted. Upon receiving the now encrypted packet\n  it is passed to the next layer either by sending it to the Linux stack for\n  routing or doing a direct tail call if an overlay is in use.\n\n* **Socket Layer Enforcement:** Socket layer enforcement uses two\n  hooks (the socket operations hook and the socket send/recv hook) to monitor\n  and attach to all TCP sockets associated with Cilium managed endpoints, including\n  any L7 proxies. The socket operations hook\n  will identify candidate sockets for accelerating. These include all local node connections\n  (endpoint to endpoint) and any connection to a Cilium proxy.\n  These identified connections will then have all messages handled by the socket\n  send/recv hook. The fast redirect ensures all policies implemented in Cilium are valid for the associated\n  socket/endpoint mapping and assuming they are sends the message directly to the\n  peer socket.\n\n* **L7 Policy:** The L7 Policy object redirects proxy traffic to a Cilium userspace\n  proxy instance. Cilium uses an Envoy instance as its userspace proxy. Envoy will\n  then either forward the traffic or generate appropriate reject messages based on the configured L7 policy.\n\nThese components are connected to create the flexible and efficient datapath used\nby Cilium. Below we show the following possible flows connecting endpoints on a single\nnode, ingress to an endpoint, and endpoint to egress networking device. In each case\nthere is an additional diagram showing the TCP accelerated path available when socket layer enforcement is enabled.\n",
  "item_type": "unknown",
  "module_path": "/tmp/cilium-repo/Documentation/network/ebpf/intro.rst",
  "extracted_at": "2025-09-03T01:13:29.219975Z"
}