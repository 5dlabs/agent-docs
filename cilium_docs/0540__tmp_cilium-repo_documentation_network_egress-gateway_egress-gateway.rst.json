{
  "url": "file:///tmp/cilium-repo/Documentation/network/egress-gateway/egress-gateway.rst",
  "content": ".. only:: not (epub or latex or html) \n WARNING: You are looking at unreleased Cilium documentation.\nPlease use the official rendered version released here:\nhttps://docs.cilium.io\n \n .. _egress-gateway: \n \n Egress Gateway \n \n The egress gateway feature routes all IPv4 and IPv6 connections originating from\npods and destined to specific cluster-external CIDRs through particular nodes,\nfrom now on called \"gateway nodes\". \n When the egress gateway feature is enabled and egress gateway policies are in\nplace, matching packets that leave the cluster are masqueraded with selected,\npredictable IPs associated with the gateway nodes. As an example, this feature\ncan be used in combination with legacy firewalls to allow traffic to legacy\ninfrastructure only from specific pods within a given namespace. The pods\ntypically have ever-changing IP addresses, and even if masquerading was to be\nused as a way to mitigate this, the IP addresses of nodes can also change\nfrequently over time. \n This document explains how to enable the egress gateway feature and how to\nconfigure egress gateway policies to route and SNAT the egress traffic for a\nspecific workload. \n .. note:: \n This guide assumes that Cilium has been correctly installed in your\nKubernetes cluster. Please see :ref:`k8s_quick_install` for more\ninformation. If unsure, run ``cilium status`` and validate that Cilium is up\nand running.\n \n .. admonition:: Video\n:class: attention \n For more insights on Cilium's Egress Gateway, check out  eCHO episode 76: Cilium Egress Gateway <https://www.youtube.com/watch?v=zEQdgNGa7bg> __. \n Preliminary Considerations \n Cilium must make use of network-facing interfaces and IP addresses present on\nthe designated gateway nodes. These interfaces and IP addresses must be\nprovisioned and configured by the operator based on their networking\nenvironment. The process is highly-dependent on said networking environment. For\nexample, in AWS/EKS, and depending on the requirements, this may mean creating\none or more Elastic Network Interfaces with one or more IP addresses and\nattaching them to instances that serve as gateway nodes so that AWS can\nadequately route traffic flowing from and to the instances. Other cloud\nproviders have similar networking requirements and constructs. \n Additionally, the enablement of the egress gateway feature requires that both\nBPF masquerading and the kube-proxy replacement are enabled. \n Delay for enforcement of egress policies on new pods \n When new pods are started, there is a delay before egress gateway policies are\napplied for those pods. That means traffic from those pods may leave the\ncluster with a source IP address (pod IP or node IP) that doesn't match the\negress gateway IP. That egressing traffic will also not be redirected through\nthe gateway node. \n .. _egress-gateway-incompatible-features: \n Incompatibility with other features \n Because egress gateway isn't compatible with identity allocation mode  kvstore ,\nyou must use Kubernetes as Cilium's identity store ( identityAllocationMode \nset to  crd ). This is the default setting for new installations. \n Egress gateway is not compatible with the Cluster Mesh feature. The gateway selected\nby an egress gateway policy must be in the same cluster as the selected pods. \n Egress gateway is not compatible with the CiliumEndpointSlice feature\n(see :gh-issue: 24833  for details). \n Enable egress gateway \n The egress gateway feature and all the requirements can be enabled as follow: \n .. tabs::\n.. group-tab:: Helm \n     .. parsed-literal::\n\n        $ helm upgrade cilium |CHART_RELEASE| \\\\\n           --namespace kube-system \\\\\n           --reuse-values \\\\\n           --set egressGateway.enabled=true \\\\\n           --set bpf.masquerade=true \\\\\n           --set kubeProxyReplacement=true\n\n.. group-tab:: ConfigMap\n\n    .. code-block:: yaml\n\n        enable-bpf-masquerade: true\n        enable-egress-gateway: true\n        kube-proxy-replacement: true\n \n Rollout both the agent pods and the operator pods to make the changes effective: \n .. code-block:: shell-session \n $ kubectl rollout restart ds cilium -n kube-system\n$ kubectl rollout restart deploy cilium-operator -n kube-system\n \n Writing egress gateway policies \n The API provided by Cilium to drive the egress gateway feature is the\n CiliumEgressGatewayPolicy  resource. \n Metadata \n CiliumEgressGatewayPolicy  is a cluster-scoped custom resource definition, so a\n .metadata.namespace  field should not be specified. \n .. code-block:: yaml \n apiVersion: cilium.io/v2\nkind: CiliumEgressGatewayPolicy\nmetadata:\n  name: example-policy\n \n To target pods belonging to a given namespace only labels/expressions should be\nused instead (as described below). \n Selecting source pods \n The  selectors  field of a  CiliumEgressGatewayPolicy  resource is used to\nselect source pods via a label selector. This can be done using  matchLabels : \n .. code-block:: yaml \n selectors:\n- podSelector:\n    matchLabels:\n      labelKey: labelVal\n \n It can also be done using  matchExpressions : \n .. code-block:: yaml \n selectors:\n- podSelector:\n    matchExpressions:\n    - {key: testKey, operator: In, values: [testVal]}\n    - {key: testKey2, operator: NotIn, values: [testVal2]}\n \n Moreover, multiple  podSelector  can be specified: \n .. code-block:: yaml \n selectors:\n- podSelector:\n  [..]\n- podSelector:\n  [..]\n \n To select pods belonging to a given namespace, the special\n io.kubernetes.pod.namespace  label should be used. \n To only select pods on certain nodes, you can use the  nodeSelector : \n .. code-block:: yaml \n selectors:\n- podSelector:\n    matchLabels:\n      labelKey: labelVal\n  nodeSelector:\n    matchLabels:\n      nodeLabelKey: nodeLabelVal\n \n .. note::\nOnly security identities will be taken into account.\nSee :ref: identity-relevant-labels  for more information.\n nodeSelector  cannot be used alone, it must be used together with  podSelector . \n Selecting the destination \n One or more destination CIDRs can be specified with  destinationCIDRs : \n .. code-block:: yaml \n destinationCIDRs:\n- \"a.b.c.d/32\"\n- \"e.f.g.0/24\"\n- \"a:b::/48\"\n \n .. note:: \n Any IP belonging to these ranges which is also an internal cluster IP (e.g.\npods, nodes, Kubernetes API server) will be excluded from the egress gateway\nSNAT logic.\n \n It's possible to specify exceptions to the  destinationCIDRs  list with\n excludedCIDRs : \n .. code-block:: yaml \n destinationCIDRs:\n- \"a.b.0.0/16\"\n- \"a:b::/48\"\nexcludedCIDRs:\n- \"a.b.c.0/24\"\n- \"a:b:c::/64\"\n \n In this case traffic destined to the  a.b.0.0/16  CIDR, except for the\n a.b.c.0/24  destination, will go through egress gateway and leave the cluster\nwith the designated egress IP. \n Selecting and configuring the gateway node \n The node that should act as gateway node for a given policy can be configured\nwith the  egressGateway  field. The node is matched based on its labels, with\nthe  nodeSelector  field: \n .. code-block:: yaml \n egressGateway:\nnodeSelector:\nmatchLabels:\ntestLabel: testVal \n .. note:: \n In case multiple nodes are a match for the given set of labels, the\nfirst node in lexical ordering based on their name will be selected.\n \n .. note:: \n If there is no match for the given set of labels, Cilium drops the\ntraffic that matches the destination CIDR(s).\n \n The IP address that should be used to SNAT traffic must also be configured.\nThere are 3 different ways this can be achieved: \n \n \n By specifying the interface: \n .. code-block:: yaml \n egressGateway:\nnodeSelector:\nmatchLabels:\ntestLabel: testVal\ninterface: ethX \n In this case the first IPv4 and IPv6 addresses assigned to the  ethX  interface\nwill be used. \n \n \n By explicitly specifying the egress IP: \n .. code-block:: yaml \n egressGateway:\nnodeSelector:\nmatchLabels:\ntestLabel: testVal\negressIP: a.b.c.d \n .. warning:: \n The egress IP must be assigned to a network device on the node. \n \n \n By omitting both  egressIP  and  interface  properties, which will make\nthe agent use the first IPv4 and IPv6 addresses assigned to the interface\nfor the default route. \n .. code-block:: yaml \n egressGateway:\nnodeSelector:\nmatchLabels:\ntestLabel: testVal \n \n \n Regardless of which way the egress IP is configured, the user must ensure that\nCilium is running on the device that has the egress IP assigned to it, by\nsetting the  --devices  agent option accordingly. \n .. warning:: \n The  egressIP  and  interface  properties cannot both be specified in the  egressGateway  spec. Egress Gateway Policies that contain both of these properties will be ignored by Cilium. \n .. note:: \n When Cilium is unable to select the Egress IP for an Egress Gateway policy (for example because the specified  egressIP  is not configured for any\nnetwork interface on the gateway node), then the gateway node will drop traffic that matches the policy with the reason  No Egress IP configured . \n .. note:: \n After Cilium has selected the Egress IP for an Egress Gateway policy (or failed to do so), it does not automatically respond to a change in the\ngateway node's network configuration (for example if an IP address is added or deleted). You can force a fresh selection by re-applying the\nEgress Gateway policy. \n Selecting multiple gateway nodes \n It's possible to select multiple gateway nodes in the same policy. In this case, the gateway nodes\ncan be configured using the  egressGateways  list field. Entries on this list have the exact same\nconfiguration options as the  egressGateway  field: \n .. code-block:: yaml \n egressGateways: \n \n nodeSelector:\nmatchLabels:\ntestLabel: testVal1 \n nodeSelector:\nmatchLabels:\ntestLabel: testVal2 \n \n .. note:: \n The same restrictions as with the ``egressGateway`` field apply to each item of the\n``egressGateways`` list.\n \n .. note:: \n When using multiple gateways the source endpoints matched by the policy will still egress traffic\nthrough a single gateway, not all of them. The endpoints will be assigned to a gateway based on\nits CiliumEndpoint's UID. Hence, an endpoint should use the same gateway during its lifetime\nas long as the gateway nodes matched by the ``nodeSelector`` fields don't change. If a\n``nodeSelector`` field is added, removed, or modified, or if a node matching one of the\n``nodeSelector`` fields is added or removed, the list of gateways will change and the endpoints\nwill be reassigned.\n \n .. warning:: \n As with single-gateway policies, changing the gateway node will break existing egress connections.\nPlease read the following :gh-issue:`39245` which tracks this issue.\n \n Example policy \n Below is an example of a  CiliumEgressGatewayPolicy  resource that conforms to\nthe specification above: \n .. code-block:: yaml \n apiVersion: cilium.io/v2\nkind: CiliumEgressGatewayPolicy\nmetadata:\nname: egress-sample\nspec:\n# Specify which pods should be subject to the current policy.\n# Multiple pod selectors can be specified.\nselectors:\n- podSelector:\nmatchLabels:\norg: empire\nclass: mediabot\n# The following label selects default namespace\nio.kubernetes.pod.namespace: default\nnodeSelector: # optional, if not specified the policy applies to all nodes\nmatchLabels:\nnode.kubernetes.io/name: node1 # only traffic from this node will be SNATed\n# Specify which destination CIDR(s) this policy applies to.\n# Multiple CIDRs can be specified.\ndestinationCIDRs:\n- \"0.0.0.0/0\"\n- \"::/0\" \n # Configure the gateway node.\negressGateway:\n  # Specify which node should act as gateway for this policy.\n  nodeSelector:\n    matchLabels:\n      node.kubernetes.io/name: node2\n\n  # Specify the IP address used to SNAT traffic matched by the policy.\n  # It must exist as an IP associated with a network interface on the instance.\n  egressIP: 10.168.60.100\n\n  # Alternatively it's possible to specify the interface to be used for egress traffic.\n  # In this case the first IPv4 and IPv6 addresses assigned to that interface will be used\n  # as egress IP.\n  # interface: enp0s8\n \n Creating the  CiliumEgressGatewayPolicy  resource above would cause all\ntraffic originating from pods with the  org: empire  and  class: mediabot \nlabels in the  default  namespace on node  node1  and destined to  0.0.0.0/0  or  ::/0 \n(i.e. all traffic leaving the cluster) to be routed through the gateway node with the\n node.kubernetes.io/name: node2  label, which will then SNAT said\ntraffic with the  10.168.60.100  egress IP. \n Selection of the egress network interface \n For gateway nodes with multiple network interfaces, Cilium selects the egress\nnetwork interface based on the node's routing setup\n( ip route get <externalIP> from <egressIP> ). \n Testing the egress gateway feature \n In this section we are going to show the necessary steps to test the feature.\nFirst we deploy a pod that connects to a cluster-external service. Then we apply\na  CiliumEgressGatewayPolicy  and observe that the pod's connection gets\nredirected through the Gateway node.\nWe assume a 2-node cluster with IPs  192.168.60.11  (node1) and\n 192.168.60.12  (node2). The client pod gets deployed to node1, and the CEGP\nselects node2 as Gateway node. \n Create an external service (optional) \n If you don't have an external service to experiment with, you can use Nginx, as\nthe server access logs will show from which IP address the request is coming. \n Create an nginx service on a Linux node that is external to the existing Kubernetes\ncluster, and use it as the destination of the egress traffic: \n .. code-block:: shell-session \n $ # Install and start nginx\n$ sudo apt install nginx\n$ sudo systemctl start nginx\n \n In this example, the IP associated with the host running the Nginx instance will\nbe  192.168.60.13 . \n Deploy client pods \n Deploy a client pod that will be used to connect to the Nginx instance: \n .. parsed-literal:: \n $ kubectl create -f \\ |SCM_WEB|\\/examples/kubernetes-dns/dns-sw-app.yaml\n$ kubectl get pods\nNAME                             READY   STATUS    RESTARTS   AGE\npod/mediabot                     1/1     Running   0          14s\n\n$ kubectl exec mediabot -- curl http://192.168.60.13:80\n \n Verify from the Nginx access log (or other external services) that the request\nis coming from one of the nodes in the Kubernetes cluster. In this example the\naccess logs should contain something like: \n .. code-block:: shell-session \n $ tail /var/log/nginx/access.log\n[...]\n192.168.60.11 - - [04/Apr/2021:22:06:57 +0000] \"GET / HTTP/1.1\" 200 612 \"-\" \"curl/7.52.1\"\n \n since the client pod is running on the node  192.168.60.11  it is expected\nthat, without any Cilium egress gateway policy in place, traffic will leave the\ncluster with the IP of the node. \n Apply egress gateway policy \n Download the  egress-sample  Egress Gateway Policy yaml: \n .. parsed-literal:: \n $ wget \\ |SCM_WEB|\\/examples/kubernetes-egress-gateway/egress-gateway-policy.yaml\n \n Modify the  destinationCIDRs  to include the IP of the host where your\ndesignated external service is running on. \n Specifying an IP address in the  egressIP  field is optional.\nTo make things easier in this example, it is possible to comment out that line.\nThis way, the agent will use the first IPv4 and IPv6 addresses assigned to the interface\nfor the default route. \n To let the policy select the node designated to be the Egress Gateway, apply the\nlabel  egress-node: true  to it: \n .. code-block:: shell-session \n $ kubectl label nodes <egress-gateway-node> egress-node=true\n \n Note that the Egress Gateway node should be a different node from the one where\nthe  mediabot  pod is running on. \n Apply the  egress-sample  egress gateway Policy, which will cause all traffic\nfrom the mediabot pod to leave the cluster with the IP of the Egress Gateway node: \n .. code-block:: shell-session \n $ kubectl apply -f egress-gateway-policy.yaml\n \n Verify the setup \n We can now verify with the client pod that the policy is working correctly: \n .. code-block:: shell-session \n $ kubectl exec mediabot -- curl http://192.168.60.13:80\n<HTML><HEAD><meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\">\n[...]\n \n The access log from Nginx should show that the request is coming from the\nselected Egress IP rather than the one of the node where the pod is running: \n .. code-block:: shell-session \n $ tail /var/log/nginx/access.log\n[...]\n192.168.60.100 - - [04/Apr/2021:22:06:57 +0000] \"GET / HTTP/1.1\" 200 612 \"-\" \"curl/7.52.1\"\n \n Troubleshooting \n To troubleshoot a policy that is not behaving as expected, you can view the\negress configuration in a cilium agent (the configuration is propagated to all agents,\nso it shouldn't matter which one you pick). \n .. code-block:: shell-session \n $ kubectl -n kube-system exec ds/cilium -- cilium-dbg bpf egress list\nDefaulted container \"cilium-agent\" out of: cilium-agent, config (init), mount-cgroup (init), apply-sysctl-overwrites (init), mount-bpf-fs (init), wait-for-node-init (init), clean-cilium-state (init)\nSource IP    Destination CIDR    Egress IP   Gateway IP\n192.168.2.23 192.168.60.13/32    0.0.0.0     192.168.60.12\n \n The Source IP address matches the IP address of each pod that matches the\npolicy's  podSelector . The Gateway IP address matches the (internal) IP address\nof the egress node that matches the policy's  nodeSelector . The Egress IP is\n0.0.0.0 on all agents except for the one running on the egress gateway node,\nwhere you should see the Egress IP address being used for this traffic (which\nwill be the  egressIP  from the policy, if specified). \n If the egress list shown does not contain entries as expected to match your\npolicy, check that the pod(s) and egress node are labeled correctly to match\nthe policy selectors. \n Troubleshooting SNAT Connection Limits \n For more advanced troubleshooting topics please see advanced egress gateway troubleshooting topic for :ref: SNAT connection limits<snat_connection_limits> .",
  "item_type": "unknown",
  "module_path": "/tmp/cilium-repo/Documentation/network/egress-gateway/egress-gateway.rst",
  "extracted_at": "2025-09-03T01:13:29.251847Z"
}