{
  "url": "file:///tmp/cilium-repo/Documentation/operations/performance/scalability/report.rst",
  "content": ".. only:: not (epub or latex or html)\n\n    WARNING: You are looking at unreleased Cilium documentation.\n    Please use the official rendered version released here:\n    https://docs.cilium.io\n\n.. _scalability_guide:\n\n******************\nScalability report\n******************\n\nThis report is intended for users planning to run Cilium on clusters with more\nthan 200 nodes in CRD mode (without a kvstore available). In our development\ncycle we have deployed Cilium on large clusters and these were the options that\nwere suitable for our testing:\n\n=====\nSetup\n=====\n\n.. code-block:: shell-session\n\n helm template cilium \\\\\n     --namespace kube-system \\\\\n     --set endpointHealthChecking.enabled=false \\\\\n     --set healthChecking=false \\\\\n     --set ipam.mode=kubernetes \\\\\n     --set k8sServiceHost=<KUBE-APISERVER-LB-IP-ADDRESS> \\\\\n     --set k8sServicePort=<KUBE-APISERVER-LB-PORT-NUMBER> \\\\\n     --set prometheus.enabled=true \\\\\n     --set operator.prometheus.enabled=true \\\\\n   > cilium.yaml\n\n\n* ``--set endpointHealthChecking.enabled=false`` and\n  ``--set healthChecking=false`` disable endpoint health\n  checking entirely. However it is recommended that those features be enabled\n  initially on a smaller cluster (3-10 nodes) where it can be used to detect\n  potential packet loss due to firewall rules or hypervisor settings.\n\n* ``--set ipam.mode=kubernetes`` is set to ``\"kubernetes\"`` since our\n  cloud provider has pod CIDR allocation enabled in ``kube-controller-manager``.\n\n* ``--set k8sServiceHost`` and ``--set k8sServicePort`` were set\n  with the IP address of the loadbalancer that was in front of ``kube-apiserver``.\n  This allows Cilium to not depend on kube-proxy to connect to ``kube-apiserver``.\n\n* ``--set prometheus.enabled=true`` and\n  ``--set operator.prometheus.enabled=true`` were just set because we\n  had a Prometheus server probing for metrics in the entire cluster.\n\nOur testing cluster consisted of 3 controller nodes and 1000 worker nodes.\nWe have followed the recommended settings from the\n`official Kubernetes documentation <https://kubernetes.io/docs/setup/best-practices/cluster-large/>`_\nand have provisioned our machines with the following settings:\n\n* **Cloud provider**: Google Cloud\n\n* **Controllers**: 3x n1-standard-32 (32vCPU, 120GB memory and 50GB SSD, kernel 5.4.0-1009-gcp)\n\n* **Workers**: 1 pool of 1000x custom-2-4096 (2vCPU, 4GB memory and 10GB HDD, kernel 5.4.0-1009-gcp)\n\n* **Metrics**: 1x n1-standard-32 (32vCPU, 120GB memory and 10GB HDD + 500GB HDD)\n  this is a dedicated node for prometheus and grafana pods.\n\n.. note::\n\n    All 3 controller nodes were behind a GCE load balancer.\n\n    Each controller contained ``etcd``, ``kube-apiserver``,\n    ``kube-controller-manager`` and ``kube-scheduler`` instances.\n\n    The CPU, memory and disk size set for the workers might be different for\n    your use case. You might have pods that require more memory or CPU available\n    so you should design your workers based on your requirements.\n\n    During our testing we had to set the ``etcd`` option\n    ``quota-backend-bytes=17179869184`` because ``etcd`` failed once it reached\n    around ``2GiB`` of allocated space.\n\n    We provisioned our worker nodes without ``kube-proxy`` since Cilium is\n    capable of performing all functionalities provided by ``kube-proxy``. We\n    created a load balancer in front of ``kube-apiserver`` to allow Cilium to\n    access ``kube-apiserver`` without ``kube-proxy``, and configured Cilium with\n    the options ``--set k8sServiceHost=<KUBE-APISERVER-LB-IP-ADDRESS>``\n    and ``--set k8sServicePort=<KUBE-APISERVER-LB-PORT-NUMBER>``.\n\n    Our ``DaemonSet`` ``updateStrategy`` had the ``maxUnavailable`` set to 250\n    pods instead of 2, but this value highly depends on your requirements when\n    you are performing a rolling update of Cilium.\n\n=====\nSteps\n=====\n\nFor each step we took, we provide more details below, with our findings and\nexpected behaviors.\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n1. Install Kubernetes v1.18.3 with EndpointSlice feature enabled\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nTo test the most up-to-date functionalities from Kubernetes and Cilium, we have\nperformed our testing with Kubernetes v1.18.3 and the EndpointSlice feature\nenabled to improve scalability.\n\nSince Kubernetes requires an ``etcd`` cluster, we have deployed v3.4.9.\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n2. Deploy Prometheus, Grafana and Cilium\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nWe have used Prometheus v2.18.1 and Grafana v7.0.1 to retrieve and analyze\n``etcd``, ``kube-apiserver``, ``cilium`` and ``cilium-operator`` metrics.\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\n3. Provision 2 worker nodes\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThis helped us to understand if our testing cluster was correctly provisioned\nand all metrics were being gathered.\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n4. Deploy 5 namespaces with 25 deployments on each namespace\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n* Each deployment had 1 replica (125 pods in total).\n\n* To measure **only** the resources consumed by Cilium, all deployments used\n  the same base image ``registry.k8s.io/pause:3.2``. This image does not have any\n  CPU or memory overhead.\n\n* We provision a small number of pods in a small cluster to understand the CPU\n  usage of Cilium:\n\n.. figure:: images/image_4_01.png\n\nThe mark shows when the creation of 125 pods started.\nAs expected, we can see a slight increase of the CPU usage on both\nCilium agents running and in the Cilium operator. The agents peaked at 6.8% CPU\nusage on a 2vCPU machine.\n\n.. figure:: images/image_4_02.png\n\nFor the memory usage, we have not seen a significant memory growth in the\nCilium agent. On the eBPF memory side, we do see it increasing due to the\ninitialization of some eBPF maps for the new pods.\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n5. Provision 998 additional nodes (total 1000 nodes)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.. figure:: images/image_5_01.png\n\nThe first mark represents the action of creating nodes, the second mark\nwhen 1000 Cilium pods were in ready state. The CPU usage increase is expected\nsince each Cilium agent receives events from Kubernetes whenever a new node is\nprovisioned in the cluster. Once all nodes were deployed the CPU usage was\n0.15% on average on a 2vCPU node.\n\n.. figure:: images/image_5_02.png\n\nAs we have increased the number of nodes in the cluster to 1000, it is expected\nto see a small growth of the memory usage in all metrics. However, it is\nrelevant to point out that **an increase in the number of nodes does not cause\nany significant increase in Ciliumâ€™s memory consumption in both control and\ndataplane.**\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n6. Deploy 25 more deployments on each namespace\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThis will now bring us a total of\n``5 namespaces * (25 old deployments + 25 new deployments)=250`` deployments in\nthe entire cluster.\nWe did not install 250 deployments from the start since we only had 2 nodes and\nthat would create 125 pods on each worker node. According to the Kubernetes\ndocumentation the maximum recommended number of pods per node is 100.\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n7. Scale each deployment to 200 replicas (50000 pods in total)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nHaving 5 namespaces with 50 deployments means that we have 250 different unique\nsecurity identities. Having a low cardinality in the labels selected by Cilium\nhelps scale the cluster. By default, Cilium has a limit of 16k security\nidentities, but it can be increased with ``bpf-policy-map-max`` in the Cilium\n``ConfigMap``.\n\n.. figure:: images/image_7_01.png\n\nThe first mark represents the action of scaling up the deployments, the second\nmark when 50000 pods were in ready state.\n\n* It is expected to see the CPU usage of Cilium increase since, on each node,\n  Cilium agents receive events from Kubernetes when a new pod is scheduled\n  and started.\n\n* The average CPU consumption of all Cilium agents was 3.38% on a 2vCPU machine.\n  At one point, roughly around minute 15:23, one of those Cilium agents picked\n  27.94% CPU usage.\n\n* Cilium Operator had a stable 5% CPU consumption while the pods were being\n  created.\n\n.. figure:: images/image_7_02.png\n\nSimilar to the behavior seen while increasing the number of worker nodes,\nadding new pods also increases Cilium memory consumption.\n\n* As we increased the number of pods from 250 to 50000, we saw a maximum memory\n  usage of 573MiB for one of the Cilium agents while the average was 438 MiB.\n* For the eBPF memory usage we saw a max usage of 462.7MiB\n* This means that each **Cilium agent's memory increased by 10.5KiB per new pod\n  in the cluster.**\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n8. Deploy 250 policies for 1 namespace\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nHere we have created 125 L4 network policies and 125 L7 policies. Each policy\nselected all pods on this namespace and was allowed to send traffic to another\npod on this namespace. Each of the 250 policies allows access to a disjoint set\nof ports. In the end we will have 250 different policies selecting 10000 pods.\n\n.. code-block:: yaml\n\n    apiVersion: \"cilium.io/v2\"\n    kind: CiliumNetworkPolicy\n    metadata:\n      name: \"l4-rule-#\"\n      namespace: \"namespace-1\"\n    spec:\n      endpointSelector:\n        matchLabels:\n          my-label: testing\n      fromEndpoints:\n        matchLabels:\n          my-label: testing\n      egress:\n        - toPorts:\n          - ports:\n            - port: \"[0-125]+80\" // from 80 to 12580\n              protocol: TCP\n    ---\n    apiVersion: \"cilium.io/v2\"\n    kind: CiliumNetworkPolicy\n    metadata:\n      name: \"l7-rule-#\"\n      namespace: \"namespace-1\"\n    spec:\n      endpointSelector:\n        matchLabels:\n          my-label: testing\n      fromEndpoints:\n        matchLabels:\n          my-label: testing\n      ingress:\n      - toPorts:\n        - ports:\n          - port: '[126-250]+80' // from 12680 to 25080\n            protocol: TCP\n          rules:\n            http:\n            - method: GET\n              path: \"/path1$\"\n            - method: PUT\n              path: \"/path2$\"\n              headers:\n              - 'X-My-Header: true'\n\n.. figure:: images/image_8_01.png\n\nIn this case we saw one of the Cilium agents jumping to 100% CPU usage for 15\nseconds while the average peak was 40% during a period of 90 seconds.\n\n.. figure:: images/image_8_02.png\n\nAs expected, **increasing the number of policies does not have a significant\nimpact on the memory usage of Cilium since the eBPF policy maps have a constant\nsize** once a pod is initialized.\n\n.. figure:: images/image_8_03.png\n.. figure:: images/image_8_04.png\n\n\nThe first mark represents the point in time when we ran ``kubectl create`` to\ncreate the ``CiliumNetworkPolicies``. Since we created the 250 policies\nsequentially, we cannot properly compute the convergence time. To do that,\nwe could use a single CNP with multiple policy rules defined under the\n``specs`` field (instead of the ``spec`` field).\n\nNevertheless, we can see the time it took the last Cilium agent to increment its\nPolicy Revision, which is incremented individually on each Cilium agent every\ntime a CiliumNetworkPolicy (CNP) is received, between second ``15:45:44``\nand ``15:45:46`` and see when was the last time an Endpoint was regenerated by\nchecking the 99th percentile of the \"Endpoint regeneration time\". In this\nmanner, that it took less than 5s. We can also verify **the maximum time was\nless than 600ms for an endpoint to have the policy enforced.**\n\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n9. Deploy 250 policies for CiliumClusterwideNetworkPolicies (CCNP)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe difference between these policies and the previous ones installed is that\nthese select all pods in all namespaces. To recap, this means that we will now\nhave **250 different network policies selecting 10000 pods and 250 different\nnetwork policies selecting 50000 pods on a cluster with 1000 nodes.** Similarly\nto the previous step we will deploy 125 L4 policies and another 125 L7 policies.\n\n.. figure:: images/image_9_01.png\n.. figure:: images/image_9_02.png\n\nSimilar to the creation of the previous 250 CNPs, there was also an increase in\nCPU usage during the creation of the CCNPs. The CPU usage was similar even\nthough the policies were effectively selecting more pods.\n\n.. figure:: images/image_9_03.png\n\nAs all pods running in a node are selected by **all 250 CCNPs created**, we see\nan increase of the **Endpoint regeneration time** which **peaked a little above\n3s.**\n\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n10. \"Accidentally\" delete 10000 pods\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nIn this step we have \"accidentally\" deleted 10000 random pods. Kubernetes will\nthen recreate 10000 new pods so it will help us understand what the convergence\ntime is for all the deployed network polices.\n\n.. figure:: images/image_10_01.png\n.. figure:: images/image_10_02.png\n\n\n* The first mark represents the point in time when pods were \"deleted\" and the\n  second mark represents the point in time when Kubernetes finished recreating\n  10k pods.\n\n* Besides the CPU usage slightly increasing while pods are being scheduled in\n  the cluster, we did see some interesting data points in the eBPF memory usage.\n  As each endpoint can have one or more dedicated eBPF maps, the eBPF memory usage\n  is directly proportional to the number of pods running in a node. **If the\n  number of pods per node decreases so does the eBPF memory usage.**\n\n.. figure:: images/image_10_03.png\n\nWe inferred the time it took for all the endpoints to get regenerated by looking\nat the number of Cilium endpoints with the policy enforced over time.\nLuckily enough we had another metric that was showing how many Cilium endpoints\nhad policy being enforced:\n\n.. figure:: images/image_10_04.png\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n11. Control plane metrics over the test run\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe focus of this test was to study the Cilium agent resource consumption at\nscale. However, we also monitored some metrics of the control plane nodes such as\netcd metrics and CPU usage of the k8s-controllers and we present them in the\nnext figures.\n\n.. figure:: images/image_11_01.png\n\nMemory consumption of the 3 etcd instances during the entire scalability\ntesting.\n\n.. figure:: images/image_11_02.png\n\nCPU usage for the 3 controller nodes, average latency per request type in\nthe etcd cluster as well as the number of operations per second made to etcd.\n\n.. figure:: images/image_11_03.png\n\nAll etcd metrics, from left to right, from top to bottom: database size,\ndisk sync duration, client traffic in, client traffic out, peer traffic in,\npeer traffic out.\n\n=============\nFinal Remarks\n=============\n\nThese experiments helped us develop a better understanding of Cilium running\nin a large cluster entirely in CRD mode and without depending on etcd. There is\nstill some work to be done to optimize the memory footprint of eBPF maps even\nfurther, as well as reducing the memory footprint of the Cilium agent. We will\naddress those in the next Cilium version.\n\nWe can also determine that it is scalable to run Cilium in CRD mode on a cluster\nwith more than 200 nodes. However, it is worth pointing out that we need to run\nmore tests to verify Cilium's behavior when it loses the connectivity with\n``kube-apiserver``, as can happen during a control plane upgrade for example.\nThis will also be our focus in the next Cilium version.\n",
  "item_type": "unknown",
  "module_path": "/tmp/cilium-repo/Documentation/operations/performance/scalability/report.rst",
  "extracted_at": "2025-09-03T01:13:29.359955Z"
}