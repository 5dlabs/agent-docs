{
  "url": "file:///tmp/cilium-repo/Documentation/network/concepts/routing.rst",
  "content": ".. only:: not (epub or latex or html)\n\n    WARNING: You are looking at unreleased Cilium documentation.\n    Please use the official rendered version released here:\n    https://docs.cilium.io\n\n.. _routing:\n\n#######\nRouting\n#######\n\n.. _arch_overlay:\n.. _encapsulation:\n\nEncapsulation\n=============\n\nWhen no configuration is provided, Cilium automatically runs in this mode as it\nis the mode with the fewest requirements on the underlying networking\ninfrastructure.\n\nIn this mode, all cluster nodes form a mesh of tunnels using the UDP-based\nencapsulation protocols :term:`VXLAN` or :term:`Geneve`. All traffic between Cilium nodes\nis encapsulated.\n\nRequirements on the network\n---------------------------\n\n* Encapsulation relies on normal node to node connectivity. This means that if\n  Cilium nodes can already reach each other, all routing requirements are\n  already met.\n\n* The underlying network and firewalls must allow encapsulated packets:\n\n  ================== =====================\n  Encapsulation Mode Port Range / Protocol\n  ================== =====================\n  VXLAN (Default)    8472/UDP\n  Geneve             6081/UDP\n  ================== =====================\n\nAdvantages of the model\n-----------------------\n\nSimplicity\n  The network which connects the cluster nodes does not need to be made aware\n  of the PodCIDRs. Cluster nodes can spawn multiple routing or link-layer\n  domains. The topology of the underlying network is irrelevant as long as\n  cluster nodes can reach each other using IP/UDP.\n\nAddressing space\n  Due to not depending on any underlying networking limitations, the available\n  addressing space is potentially much larger and allows to run any number of\n  pods per node if the PodCIDR size is configured accordingly.\n\nAuto-configuration\n  When running together with an orchestration system such as Kubernetes, the\n  list of all nodes in the cluster including their associated allocation prefix\n  node is made available to each agent automatically. New nodes joining the\n  cluster will automatically be incorporated into the mesh.\n\nIdentity context\n  Encapsulation protocols allow for the carrying of metadata along with the\n  network packet. Cilium makes use of this ability to transfer metadata such as\n  the source security identity. The identity transfer is an optimization\n  designed to avoid one identity lookup on the remote node.\n\n\nDisadvantages of the model\n--------------------------\n\nMTU Overhead\n  Due to adding encapsulation headers, the effective MTU available for payload\n  is lower than with native-routing (50 bytes per network packet for VXLAN).\n  This results in a lower maximum throughput rate for a particular network\n  connection. This can be largely mitigated by enabling jumbo frames (50 bytes\n  of overhead for each 1500 bytes vs 50 bytes of overhead for each 9000 bytes).\n\nConfiguration\n-------------\n\nThe following options can be used to configure encapsulation:\n\n* ``tunnel-protocol``: Set the encapsulation protocol to ``vxlan`` or\n  ``geneve``, defaults to ``vxlan``.\n* ``underlay-protocol``: Set the IP family for the underlay. Defaults to\n  ``ipv4``. The underlying network must support that protocol.\n* ``tunnel-port``: Set the port for the encapsulation protocol. Defaults\n  to ``8472`` for ``vxlan`` and ``6081`` for ``geneve``.\n\n.. _arch_direct_routing:\n.. _native_routing:\n\nNative-Routing\n==============\n\nThe native routing datapath is enabled with ``routing-mode: native`` and enables\nthe native packet forwarding mode. The native packet forwarding mode leverages\nthe routing capabilities of the network Cilium runs on instead of performing\nencapsulation.\n\n.. image:: native_routing.png\n    :align: center\n\nIn native routing mode, Cilium will delegate all packets which are not\naddressed to another local endpoint to the routing subsystem of the Linux\nkernel. This means that the packet will be routed as if a local process would\nhave emitted the packet. As a result, the network connecting the cluster nodes\nmust be capable of routing PodCIDRs.\n\nCilium automatically enables IP forwarding in the Linux kernel when native\nrouting is configured.\n\nRequirements on the network\n---------------------------\n\n* In order to run the native routing mode, the network connecting the hosts on\n  which Cilium is running on must be capable of forwarding IP traffic using\n  addresses given to pods or other workloads.\n\n* The Linux kernel on the node must be aware on how to forward packets of pods\n  or other workloads of all nodes running Cilium. This can be achieved in two\n  ways:\n\n  1. The node itself does not know how to route all pod IPs but a router exists\n     on the network that knows how to reach all other pods. In this scenario,\n     the Linux node is configured to contain a default route to point to such a\n     router. This model is used for cloud provider network integration. See\n     :ref:`gke_datapath`, :ref:`aws_eni_datapath`, and :ref:`ipam_azure` for\n     more details.\n\n  2. Each individual node is made aware of all pod IPs of all other nodes and\n     routes are inserted into the Linux kernel routing table to represent this.\n     If all nodes share a single L2 network, then this can be taken care of by\n     enabling the option ``auto-direct-node-routes: true``. Otherwise, an\n     additional system component such as a BGP daemon must be run to distribute\n     the routes.  See the guide :ref:`kube-router` on how to achieve this using\n     the kube-router project.\n\nConfiguration\n-------------\n\nThe following configuration options must be set to run the datapath in native\nrouting mode:\n\n* ``routing-mode: native``: Enable native routing mode.\n* ``ipv4-native-routing-cidr: x.x.x.x/y``: Set the CIDR in which native routing\n  can be performed.\n\nThe following configuration options are optional when running the datapath in \nnative routing mode:\n\n* ``direct-routing-skip-unreachable``: If a BGP daemon is running and there \n  is multiple native subnets to the cluster network, \n  ``direct-routing-skip-unreachable: true`` can be added alongside \n  ``auto-direct-node-routes`` to give each node L2 connectivity in each zone \n  without traffic always needing to be routed by the BGP routers.\n\n.. _aws_eni_datapath:\n\nAWS ENI\n=======\n\nThe AWS ENI datapath is enabled when Cilium is run with the option\n``--ipam=eni``. It is a special purpose datapath that is useful when running\nCilium in an AWS environment.\n\nAdvantages of the model\n-----------------------\n\n* Pods are assigned ENI IPs which are directly routable in the AWS VPC. This\n  simplifies communication of pod traffic within VPCs and avoids the need for\n  SNAT.\n\n* Pod IPs are assigned a security group. The security groups for pods are\n  configured per node which allows to create node pools and give different\n  security group assignments to different pods. See section :ref:`ipam_eni` for\n  more details.\n\nDisadvantages of this model\n---------------------------\n\n* The number of ENI IPs is limited per instance. The limit depends on the EC2\n  instance type. This can become a problem when attempting to run a larger\n  number of pods on very small instance types.\n\n* Allocation of ENIs and ENI IPs requires interaction with the EC2 API which is\n  subject to rate limiting. This is primarily mitigated via the operator\n  design, see section :ref:`ipam_eni` for more details.\n\nArchitecture\n------------\n\nIngress\n~~~~~~~\n\n1. Traffic is received on one of the ENIs attached to the instance which is\n   represented on the node as interface ``ethN``.\n\n2. An IP routing rule ensures that traffic to all local pod IPs is done using\n   the main routing table::\n\n       20:\tfrom all to 192.168.105.44 lookup main\n\n3. The main routing table contains an exact match route to steer traffic into a\n   veth pair which is hooked into the pod::\n\n       192.168.105.44 dev lxc5a4def8d96c5\n\n4. All traffic passing ``lxc5a4def8d96c5`` on the way into the pod is subject\n   to Cilium's eBPF program to enforce network policies, provide service reverse\n   load-balancing, and visibility.\n\nEgress\n~~~~~~\n\n1. The pod's network namespace contains a default route which points to the\n   node's router IP via the veth pair which is named ``eth0`` inside of the pod\n   and ``lxcXXXXXX`` in the host namespace. The router IP is allocated from the\n   ENI space, allowing for sending of ICMP errors from the router IP for Path\n   MTU purposes.\n\n2. After passing through the veth pair and before reaching the Linux routing\n   layer, all traffic is subject to Cilium's eBPF program to enforce network\n   policies, implement load-balancing and provide networking features.\n\n3. An IP routing rule ensures that traffic from individual endpoints are using\n   a routing table specific to the ENI from which the endpoint IP was\n   allocated::\n\n       30:\tfrom 192.168.105.44 to 192.168.0.0/16 lookup 92\n\n4. The ENI specific routing table contains a default route which redirects\n   to the router of the VPC via the ENI interface::\n\n       default via 192.168.0.1 dev eth2\n       192.168.0.1 dev eth2\n\n\nConfiguration\n-------------\n\nThe AWS ENI datapath is enabled by setting the following option:\n\n.. code-block: yaml\n\n        ipam: eni\n        enable-endpoint-routes: \"true\"\n        auto-create-cilium-node-resource: \"true\"\n        egress-masquerade-interfaces: eth+\n\n* ``ipam: eni`` Enables the ENI specific IPAM backend and indicates to the\n  datapath that ENI IPs will be used.\n\n* ``enable-endpoint-routes: \"true\"`` enables direct routing to the ENI\n  veth pairs without requiring to route via the ``cilium_host`` interface.\n\n* ``auto-create-cilium-node-resource: \"true\"`` enables the automatic creation of\n  the ``CiliumNode`` custom resource with all required ENI parameters. It is\n  possible to disable this and provide the custom resource manually.\n\n* ``egress-masquerade-interfaces: eth+`` is the interface selector of all\n  interfaces which are subject to masquerading. Masquerading can be disabled\n  entirely with ``enable-ipv4-masquerade: \"false\"``.\n\nSee the section :ref:`ipam_eni` for details on how to configure ENI IPAM\nspecific parameters.\n\n.. _gke_datapath:\n\nGoogle Cloud\n============\n\nWhen running Cilium on Google Cloud via either Google Kubernetes Engine (GKE)\nor self-managed, it is possible to utilize the `Google Cloud's networking layer\n<https://cloud.google.com/products/networking>`_ with Cilium running in a\n:ref:`native_routing` configuration. This provides native networking\nperformance while benefiting from many additional Cilium features such as\npolicy enforcement, load-balancing with DSR, efficient\nNodePort/ExternalIP/HostPort implementation, extensive visibility features, and\nso on.\n\n.. image:: gke_datapath.png\n    :align: center\n\nAddressing\n   Cilium will assign IPs to pods out of the PodCIDR assigned to the specific\n   Kubernetes node. By using `Alias IP ranges\n   <https://cloud.google.com/vpc/docs/alias-ip>`_, these IPs are natively\n   routable on Google Cloud's network without additional encapsulation or route\n   distribution.\n\nMasquerading\n   All traffic not staying with the ``ipv4-native-routing-cidr`` (defaults to\n   the Cluster CIDR) will be masqueraded to the node's IP address to become\n   publicly routable.\n\nLoad-balancing\n   ClusterIP load-balancing will be performed using eBPF for all version of GKE.\n\nPolicy enforcement & visibility\n   All NetworkPolicy enforcement and visibility is provided using eBPF.\n\nConfiguration\n-------------\n\nThe following configuration options must be set to run the datapath on GKE:\n\n* ``gke.enabled: true``: Enables the Google Kubernetes Engine (GKE) datapath.\n  Setting this to ``true`` will enable the following options:\n\n  * ``ipam: kubernetes``: Enable :ref:`k8s_hostscope` IPAM\n  * ``routing-mode: native``: Enable native routing mode\n  * ``enable-endpoint-routes: true``: Enable per-endpoint routing on the node\n    (automatically disables the local node route).\n* ``ipv4-native-routing-cidr: x.x.x.x/y``: Set the CIDR in which native routing\n  is supported.\n\nSee the getting started guide :ref:`k8s_install_quick` to install Cilium on\nGoogle Kubernetes Engine (GKE).\n",
  "item_type": "unknown",
  "module_path": "/tmp/cilium-repo/Documentation/network/concepts/routing.rst",
  "extracted_at": "2025-09-03T01:13:29.209895Z"
}