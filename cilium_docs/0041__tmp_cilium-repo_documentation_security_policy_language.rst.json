{
  "url": "file:///tmp/cilium-repo/Documentation/security/policy/language.rst",
  "content": ".. only:: not (epub or latex or html)\n\n    WARNING: You are looking at unreleased Cilium documentation.\n    Please use the official rendered version released here:\n    https://docs.cilium.io\n\n.. _policy_examples:\n\nLayer 3 Examples\n================\n\nThe layer 3 policy establishes the base connectivity rules regarding which endpoints\ncan talk to each other. Layer 3 policies can be specified using the following methods:\n\n* `Endpoints based`: This is used to describe the relationship if both\n  endpoints are managed by Cilium and are thus assigned labels. The\n  advantage of this method is that IP addresses are not encoded into the\n  policies and the policy is completely decoupled from the addressing.\n\n* `Services based`: This is an intermediate form between Labels and CIDR and\n  makes use of the services concept in the orchestration system. A good example\n  of this is the Kubernetes concept of Service endpoints which are\n  automatically maintained to contain all backend IP addresses of a service.\n  This allows to avoid hardcoding IP addresses into the policy even if the\n  destination endpoint is not controlled by Cilium.\n\n* `Entities based`: Entities are used to describe remote peers which can be\n  categorized without knowing their IP addresses. This includes connectivity\n  to the local host serving the endpoints or all connectivity to outside of\n  the cluster.\n\n* `Node based`: This is an extension of ``remote-node`` entity. Optionally nodes\n  can have unique identity that can be used to allow/block access only from specific ones.\n\n* `CIDR based`: This is used to describe the relationship to or from external\n  services if the remote peer is not an endpoint. This requires to hardcode either\n  IP addresses or subnets into the policies. This construct should be used as a\n  last resort as it requires stable IP or subnet assignments.\n\n* `DNS based`: Selects remote, non-cluster, peers using DNS names converted to\n  IPs via DNS lookups. It shares all limitations of the `CIDR based` rules\n  above. DNS information is acquired by routing DNS traffic via a proxy.\n  DNS TTLs are respected.\n\n.. _Endpoints based:\n\nEndpoints based\n---------------\n\nEndpoints-based L3 policy is used to establish rules between endpoints inside\nthe cluster managed by Cilium. Endpoints-based L3 policies are defined by using\nan `EndpointSelector` inside a rule to select what kind of traffic can be\nreceived (on ingress), or sent (on egress). An empty `EndpointSelector` allows\nall traffic. The examples below demonstrate this in further detail.\n\n.. note:: **Kubernetes:** See section :ref:`k8s_namespaces` for details on how\n\t  the `EndpointSelector` applies in a Kubernetes environment with\n\t  regard to namespaces.\n\nIngress\n~~~~~~~\n\nAn endpoint is allowed to receive traffic from another endpoint if at least one\ningress rule exists which selects the destination endpoint with the\n`EndpointSelector` in the ``endpointSelector`` field. To restrict traffic upon\ningress to the selected endpoint, the rule selects the source endpoint with the\n`EndpointSelector` in the ``fromEndpoints`` field.\n\nSimple Ingress Allow\n~~~~~~~~~~~~~~~~~~~~\n\nThe following example illustrates how to use a simple ingress rule to allow\ncommunication from endpoints with the label ``role=frontend`` to endpoints with\nthe label ``role=backend``.\n\n.. literalinclude:: ../../../examples/policies/l3/simple/l3.yaml\n  :language: yaml\n\n\nIngress Allow All Endpoints\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nAn empty `EndpointSelector` will select all endpoints, thus writing a rule that will allow\nall ingress traffic to an endpoint may be done as follows:\n\n.. literalinclude:: ../../../examples/policies/l3/ingress-allow-all/ingress-allow-all.yaml\n  :language: yaml\n\nNote that while the above examples allow all ingress traffic to an endpoint, this does not\nmean that all endpoints are allowed to send traffic to this endpoint per their policies.\nIn other words, policy must be configured on both sides (sender and receiver).\n\nEgress\n~~~~~~\n\nAn endpoint is allowed to send traffic to another endpoint if at least one\negress rule exists which selects the destination endpoint with the\n`EndpointSelector` in the ``endpointSelector`` field. To restrict traffic upon\negress to the selected endpoint, the rule selects the destination endpoint with\nthe `EndpointSelector` in the ``toEndpoints`` field.\n\nSimple Egress Allow\n~~~~~~~~~~~~~~~~~~~~\n\nThe following example illustrates how to use a simple egress rule to allow\ncommunication to endpoints with the label ``role=backend`` from endpoints with\nthe label ``role=frontend``.\n\n.. literalinclude:: ../../../examples/policies/l3/simple/l3_egress.yaml\n  :language: yaml\n\nEgress Allow All Endpoints\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nAn empty `EndpointSelector` will select all egress endpoints from an endpoint\nbased on the `CiliumNetworkPolicy` namespace (``default`` by default). The\nfollowing rule allows all egress traffic from endpoints with the label\n``role=frontend`` to all other endpoints in the same namespace:\n\n.. literalinclude:: ../../../examples/policies/l3/egress-allow-all/egress-allow-all.yaml\n  :language: yaml\n\nNote that while the above examples allow all egress traffic from an endpoint, the receivers\nof the egress traffic may have ingress rules that deny the traffic. In other words,\npolicy must be configured on both sides (sender and receiver).\n\nSimple Egress Deny\n~~~~~~~~~~~~~~~~~~\n\nThe following example illustrates how to deny communication to endpoints with\nthe label ``role=backend`` from endpoints with the label ``role=frontend``.\nIf an ``egressDeny`` rule matches, egress traffic is denied even if the policy\ncontains ``egress`` rules that would otherwise allow it.\n\n.. literalinclude:: ../../../examples/policies/l3/egress-deny/egress-deny.yaml\n   :language: yaml\n\nIngress/Egress Default Deny\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nAn endpoint can be put into the default deny mode at ingress or egress if a\nrule selects the endpoint and contains the respective rule section ingress or\negress.\n\n.. note:: Any rule selecting the endpoint will have this effect, this example\n          illustrates how to put an endpoint into default deny mode without\n          whitelisting other peers at the same time.\n\n.. literalinclude:: ../../../examples/policies/l3/egress-default-deny/egress-default-deny.yaml\n  :language: yaml\n\nAdditional Label Requirements\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. warning::\n\n   The ``fromRequires`` and ``toRequires`` fields are deprecated as of Cilium\n   1.17.x. They will be dropped from support in Cilium 1.18.\n\nIt is often required to apply the principle of *separation of concern* when defining\npolicies. For this reason, an additional construct exists which allows to establish\nbase requirements for any connectivity to happen.\n\nFor this purpose, the ``fromRequires`` field can be used to establish label\nrequirements which serve as a foundation for any ``fromEndpoints``\nrelationship.  ``fromRequires`` is a list of additional constraints which must\nbe met in order for the selected endpoints to be reachable. These additional\nconstraints do not grant access privileges by themselves, so to allow traffic\nthere must also be rules which match ``fromEndpoints``. The same applies for\negress policies, with ``toRequires`` and ``toEndpoints``.\n\nThe purpose of this rule is to allow establishing base requirements such as, any\nendpoint in ``env=prod`` can only be accessed if the source endpoint also carries\nthe label ``env=prod``.\n\n.. warning::\n\n   ``toRequires`` and ``fromRequires`` apply to all rules that share the same\n   endpoint selector and are not limited by other egress or ingress rules.\n   As a result ``toRequires`` and ``fromRequires`` limits all ingress and egress traffic\n   that applies to its endpoint selector. An important implication of the fact\n   that ``toRequires`` and ``fromRequires`` limit all ingress and egress traffic\n   that applies to an endpoint selector is that the other egress and ingress rules\n   (such as ``fromEndpoints``, ``fromPorts``, ``toEntities``, ``toServices``, and the rest)\n   do not limit the scope of the ``toRequires`` of ``fromRequires`` fields. Pairing other\n   ingress and egress rules with a ``toRequires`` or ``fromRequires`` will result in valid\n   policy, but the requirements set in ``toRequires`` and ``fromRequires`` stay in effect\n   no matter what would otherwise be allowed by the other rules.\n\n\nThis example shows how to require every endpoint with the label ``env=prod`` to\nbe only accessible if the source endpoint also has the label ``env=prod``.\n\n.. literalinclude:: ../../../examples/policies/l3/requires/requires.yaml\n  :language: yaml\n\nThis ``fromRequires`` rule doesn't allow anything on its own and needs to be\ncombined with other rules to allow traffic. For example, when combined with the\nexample policy below, the endpoint with label ``env=prod`` will become\naccessible from endpoints that have both labels ``env=prod`` and\n``role=frontend``.\n\n.. literalinclude:: ../../../examples/policies/l3/requires/endpoints.yaml\n  :language: yaml\n\n.. _Services based:\n\nServices based\n--------------\n\nTraffic from endpoints to services running in your cluster can be allowed via\n``toServices`` statements in Egress rules. Policies can reference\n`Kubernetes Services <https://kubernetes.io/docs/concepts/services-networking/service>`_\nby name or label selector.\n\nThis feature uses the discovered services' `label selector <https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors>`_\nas an :ref:`endpoint selector <endpoints based>` within the policy.\n\n.. note::\n\n   `Services without selectors <https://kubernetes.io/docs/concepts/services-networking/service/#services-without-selectors>`_\n   are handled differently. The IPs in the service's EndpointSlices are, converted to\n   :ref:`CIDR <cidr based>` selectors. CIDR selectors cannot select pods,\n   and that limitation applies here as well.\n\n   The special Kubernetes Service ``default/kubernetes`` does not use a label\n   selector. It is **not recommended** to grant access to the Kubernetes API server\n   with a ``toServices``-based policy. Use instead the\n   `kube-apiserver entity <kube_apiserver_entity>`.\n\n\nThis example shows how to allow all endpoints with the label ``id=app2``\nto talk to all endpoints of Kubernetes Service ``myservice`` in kubernetes\nnamespace ``default`` as well as all services with label ``env=staging`` in\nnamespace ``another-namespace``.\n\n.. literalinclude:: ../../../examples/policies/l3/service/service.yaml\n  :language: yaml\n\n.. _Entities based:\n\nEntities based\n--------------\n\n``fromEntities`` is used to describe the entities that can access the selected\nendpoints. ``toEntities`` is used to describe the entities that can be accessed\nby the selected endpoints.\n\nThe following entities are defined:\n\nhost\n    The host entity includes the local host. This also includes all\n    containers running in host networking mode on the local host.\nremote-node\n    Any node in any of the connected clusters other than the local host. This\n    also includes all containers running in host-networking mode on remote\n    nodes.\nkube-apiserver\n    The kube-apiserver entity represents the kube-apiserver in a Kubernetes\n    cluster. This entity represents both deployments of the kube-apiserver:\n    within the cluster and outside of the cluster.\ningress\n    The ingress entity represents the Cilium Envoy instance that handles ingress\n    L7 traffic. Be aware that this also applies for pod-to-pod traffic within\n    the same cluster when using ingress endpoints (also known as *hairpinning*).\ncluster\n    Cluster is the logical group of all network endpoints inside of the local\n    cluster. This includes all Cilium-managed endpoints of the local cluster,\n    unmanaged endpoints in the local cluster, as well as the host,\n    remote-node, and init identities. This also includes all remote nodes\n    in a clustermesh scenario.\ninit\n    The init entity contains all endpoints in bootstrap phase for which the\n    security identity has not been resolved yet. This is typically only\n    observed in non-Kubernetes environments. See section\n    :ref:`endpoint_lifecycle` for details.\nhealth\n    The health entity represents the health endpoints, used to check cluster\n    connectivity health. Each node managed by Cilium hosts a health endpoint.\n    See `cluster_connectivity_health` for details on health checks.\nunmanaged\n    The unmanaged entity represents endpoints not managed by Cilium. Unmanaged\n    endpoints are considered part of the cluster and are included in the\n    cluster entity.\nworld\n    The world entity corresponds to all endpoints outside of the cluster.\n    Allowing to world is identical to allowing to CIDR 0.0.0.0/0. An alternative\n    to allowing from and to world is to define fine grained DNS or CIDR based\n    policies.\nall\n    The all entity represents the combination of all known clusters as well\n    world and whitelists all communication.\n\n.. note:: The ``kube-apiserver`` entity may not work for *ingress traffic* in some Kubernetes\n   distributions, such as Azure AKS and GCP GKE. This is due to the fact that ingress\n   control-plane traffic is being tunneled through worker nodes, which does not preserve\n   the original source IP. You may be able to use a broader ``fromEntities: cluster`` rule\n   instead. Restricting *egress traffic* via ``toEntities: kube-apiserver`` however is expected\n   to work on these Kubernetes distributions.\n\n.. _kube_apiserver_entity:\n\nAccess to/from kube-apiserver\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nAllow all endpoints with the label ``env=dev`` to access the kube-apiserver.\n\n.. literalinclude:: ../../../examples/policies/l3/entities/apiserver.yaml\n  :language: yaml\n\nAccess to/from local host\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\nAllow all endpoints with the label ``env=dev`` to access the host that is\nserving the particular endpoint.\n\n.. note:: Kubernetes will automatically allow all communication from the\n\t  local host of all local endpoints. You can run the agent with the\n\t  option ``--allow-localhost=policy`` to disable this behavior which\n\t  will give you control over this via policy.\n\n.. literalinclude:: ../../../examples/policies/l3/entities/host.yaml\n  :language: yaml\n\n.. _policy-remote-node:\n\nAccess to/from all nodes in the cluster (or clustermesh)\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nAllow all endpoints with the label ``env=dev`` to receive traffic from any host\nin the cluster that Cilium is running on.\n\n.. literalinclude:: ../../../examples/policies/l3/entities/nodes.yaml\n  :language: yaml\n\nAccess to/from outside cluster\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThis example shows how to enable access from outside of the cluster to all\nendpoints that have the label ``role=public``.\n\n.. literalinclude:: ../../../examples/policies/l3/entities/world.yaml\n  :language: yaml\n\n.. _policy_node_based:\n.. _Node based:\n\nNode based\n----------\n\n.. note:: Example below with ``fromNodes/toNodes`` fields will only take effect when\n     ``enable-node-selector-labels`` flag is set to true (or equivalent Helm value\n     ``nodeSelectorLabels: true``).\n\nWhen ``--enable-node-selector-labels=true`` is specified, every cilium-agent\nallocates a different local :ref:`security identity <arch_id_security>` for all\nother nodes. But instead of using :ref:`local scoped identity <local_scoped_identity>`\nit uses :ref:`remote-node scoped identity<remote_node_scoped_identity>` identity range.\n\nBy default all labels that ``Node`` object has attached are taken into account,\nwhich might result in allocation of **unique** identity for each remote-node.\nFor these cases it is also possible to filter only\n:ref:`security relevant labels <security relevant labels>` with ``--node-labels`` flag.\n\nThis example shows how to allow all endpoints with the label ``env=prod`` to receive\ntraffic **only** from control plane (labeled\n``node-role.kubernetes.io/control-plane=\"\"``) nodes in the cluster (or clustermesh).\n\nNote that by default policies automatically select nodes from all the clusters in\na Cluster Mesh environment unless it is explicitly specified. To restrict node\nselection to the local cluster by default you can enable the option\n``--policy-default-local-cluster`` via the ConfigMap option ``policy-default-local-cluster``\nor the Helm value ``clustermesh.policyDefaultLocalCluster``.\n\n.. literalinclude:: ../../../examples/policies/l3/entities/customnodes.yaml\n  :language: yaml\n\n.. _policy_cidr:\n.. _CIDR based:\n\nIP/CIDR based\n-------------\n\nCIDR policies are used to define policies to and from endpoints which are not\nmanaged by Cilium and thus do not have labels associated with them. These are\ntypically external services, VMs or metal machines running in particular\nsubnets. CIDR policy can also be used to limit access to external services, for\nexample to limit external access to a particular IP range. CIDR policies can\nbe applied at ingress or egress.\n\nCIDR rules apply if Cilium cannot map the source or destination to an identity\nderived from endpoint labels, ie the `reserved_labels`. For example, CIDR rules\nwill apply to traffic where one side of the connection is:\n\n* A network endpoint outside the cluster\n* The host network namespace where the pod is running.\n* Within the cluster prefix but the IP's networking is not provided by Cilium.\n* (:ref:`optional <cidr_select_nodes>`) Node IPs within the cluster\n\nConversely, CIDR rules do not apply to traffic where both sides of the\nconnection are either managed by Cilium or use an IP belonging to a node in the\ncluster (including host networking pods). This traffic may be allowed using\nlabels, services or entities -based policies as described above.\n\nIngress\n~~~~~~~\n\nfromCIDR\n  List of source prefixes/CIDRs that are allowed to talk to all endpoints\n  selected by the ``endpointSelector``.\n\nfromCIDRSet\n  List of source prefixes/CIDRs that are allowed to talk to all endpoints\n  selected by the ``endpointSelector``, along with an optional list of\n  prefixes/CIDRs per source prefix/CIDR that are subnets of the source\n  prefix/CIDR from which communication is not allowed.\n\n  ``fromCIDRSet`` may also reference prefixes/CIDRs indirectly via a :ref:`CiliumCIDRGroup`.\n\nEgress\n~~~~~~\n\ntoCIDR\n  List of destination prefixes/CIDRs that endpoints selected by\n  ``endpointSelector`` are allowed to talk to. Note that endpoints which are\n  selected by a ``fromEndpoints`` are automatically allowed to reply back to\n  the respective destination endpoints.\n\ntoCIDRSet\n  List of destination prefixes/CIDRs that endpoints selected by\n  ``endpointSelector`` are allowed to talk to, along with an optional list of\n  prefixes/CIDRs per source prefix/CIDR that are subnets of the destination\n  prefix/CIDR to which communication is not allowed.\n\n  ``toCIDRSet`` may also reference prefixes/CIDRs indirectly via a :ref:`CiliumCIDRGroup`.\n\nAllow to external CIDR block\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThis example shows how to allow all endpoints with the label ``app=myService``\nto talk to the external IP ``20.1.1.1``, as well as the CIDR prefix ``10.0.0.0/8``,\nbut not CIDR prefix ``10.96.0.0/12``\n\n.. literalinclude:: ../../../examples/policies/l3/cidr/cidr.yaml\n  :language: yaml\n\n.. _cidr_select_nodes:\n\nSelecting nodes with CIDR / ipBlock\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. include:: ../../beta.rst\n\nBy default, CIDR-based selectors do not match in-cluster entities (pods or nodes).\nOptionally, you can direct the policy engine to select nodes by CIDR / ipBlock.\nThis requires you to configure Cilium with ``--policy-cidr-match-mode=nodes`` or\nthe equivalent Helm value ``policyCIDRMatchMode: nodes``. It is safe to toggle this\noption on a running cluster, and toggling the option affects neither upgrades nor downgrades.\n\nWhen ``--policy-cidr-match-mode=nodes`` is specified, every agent allocates a\ndistinct local :ref:`security identity <arch_id_security>` for all other nodes.\nThis slightly increases memory usage -- approximately 1MB for every 1000 nodes\nin the cluster.\n\nThis is particularly relevant to self-hosted clusters -- that is, clusters where\nthe apiserver is hosted on in-cluster nodes. Because CIDR-based selectors ignore\nnodes by default, you must ordinarily use the ``kube-apiserver`` :ref:`entity <Entities based>`\nas part of a CiliumNetworkPolicy. Setting ``--policy-cidr-match-mode=nodes`` permits\nselecting the apiserver via an ``ipBlock`` peer in a KubernetesNetworkPolicy.\n\n.. _DNS based:\n\nDNS based\n---------\n\nDNS policies are used to define Layer 3 policies to endpoints that are not\nmanaged by Cilium, but have DNS queryable domain names. The IP addresses\nprovided in DNS responses are allowed by Cilium in a similar manner to IPs in\n`CIDR based`_ policies. They are an alternative when the remote IPs may change\nor are not know prior, or when DNS is more convenient. To enforce policy on\nDNS requests themselves, see `Layer 7 Examples`_.\n\n.. note::\n\n\tIn order to associate domain names with IP addresses, Cilium intercepts\n\tDNS responses per-Endpoint using a `DNS Proxy`_. This requires Cilium\n\tto be configured with ``--enable-l7-proxy=true`` and an L7 policy allowing\n\tDNS requests. For more details, see :ref:`DNS Obtaining Data`.\n\nAn L3 `CIDR based`_ rule is generated for every ``toFQDNs``\nrule and applies to the same endpoints. The IP information is selected for\ninsertion by ``matchName`` or ``matchPattern`` rules, and is collected from all\nDNS responses seen by Cilium on the node. Multiple selectors may be included in\na single egress rule.\n\n.. note:: The DNS Proxy is provided in each Cilium agent.\n   As a result, DNS requests targeted by policies depend on the availability\n   of the Cilium agent pod.\n   This includes DNS policies (:ref:`proxy_visibility`).\n\n``toFQDNs`` egress rules cannot contain any other L3 rules, such as\n``toEndpoints`` (under `Endpoints Based`_) and ``toCIDRs`` (under `CIDR Based`_).\nThey may contain L4/L7 rules, such as ``toPorts`` (see `Layer 4 Examples`_)\nwith, optionally, ``HTTP`` and ``Kafka`` sections (see `Layer 7 Examples`_).\n\n.. note:: DNS based rules are intended for external connections and behave\n          similarly to `CIDR based`_ rules. See `Services based`_ and\n          `Endpoints based`_ for cluster-internal traffic.\n\nIPs to be allowed are selected via:\n\n``toFQDNs.matchName``\n  Inserts IPs of domains that match ``matchName`` exactly. Multiple distinct\n  names may be included in separate ``matchName`` entries and IPs for domains\n  that match any ``matchName`` will be inserted.\n\n``toFQDNs.matchPattern``\n  Inserts IPs of domains that match the pattern in ``matchPattern``, accounting\n  for wildcards. Patterns are composed of literal characters that are allowed\n  in domain names: a-z, 0-9, ``.`` and ``-``.\n\n  ``*`` is allowed as a wildcard with a number of convenience behaviors:\n\n  * ``*`` within a domain allows 0 or more valid DNS characters, except for the\n    ``.`` separator. ``*.cilium.io`` will match ``sub.cilium.io`` but not\n    ``cilium.io`` or ``sub.sub.cilium.io``. ``part*ial.com`` will match ``partial.com`` and\n    ``part-extra-ial.com``.\n  * ``*`` alone matches all names, and inserts all cached DNS IPs into this\n    rule.\n\nThe example below allows all DNS traffic on port 53 to the DNS service and\nintercepts it via the `DNS Proxy`_. If using a non-standard DNS port for\na DNS application behind a Kubernetes Service, the port must match the backend\nport. When the application makes a request for my-remote-service.com, Cilium\nlearns the IP address and will allow traffic due to the match on the name under\nthe ``toFQDNs.matchName`` rule.\n\nExample\n~~~~~~~\n\n.. literalinclude:: ../../../examples/policies/l3/fqdn/fqdn.yaml\n  :language: yaml\n\nManaging Short-Lived Connections & Maximum IPs per FQDN/endpoint\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nMany short-lived connections can grow the number of IPs mapping to an FQDN\nquickly. In order to limit the number of IP addresses that map a particular\nFQDN, each FQDN has a per-endpoint max capacity of IPs that will be retained\n(default: 50). Once this limit is exceeded, the oldest IP entries are\nautomatically expired from the cache. This capacity can be changed using the\n``--tofqdns-endpoint-max-ip-per-hostname`` option.\n\nAs with long-lived connections above, live connections are not expired until\nthey terminate. It is safe to mix long- and short-lived connections from the\nsame Pod. IPs above the limit described above will only be removed if unused by\na connection.\n\n\n\n.. _l4_policy:\n\nLayer 4 Examples\n================\n\nLimit ingress/egress ports\n--------------------------\n\nLayer 4 policy can be specified in addition to layer 3 policies or independently.\nIt restricts the ability of an endpoint to emit and/or receive packets on a\nparticular port using a particular protocol. If no layer 4 policy is specified\nfor an endpoint, the endpoint is allowed to send and receive on all layer 4\nports and protocols including ICMP. If any layer 4 policy is specified, then\nICMP will be blocked unless it's related to a connection that is otherwise\nallowed by the policy. Layer 4 policies apply to ports after service port\nmapping has been applied.\n\nLayer 4 policy can be specified at both ingress and egress using the\n``toPorts`` field. The ``toPorts`` field takes a ``PortProtocol`` structure\nwhich is defined as follows:\n\n.. code-block:: go\n\n        // PortProtocol specifies an L4 port with an optional transport protocol\n        type PortProtocol struct {\n                // Port can be an L4 port number, or a name in the form of \"http\"\n                // or \"http-8080\". EndPort is ignored if Port is a named port.\n                Port string `json:\"port\"`\n\n                // EndPort can only be an L4 port number. It is ignored when\n                // Port is a named port.\n                //\n                // +optional\n                EndPort int32 `json:\"endPort,omitempty\"`\n\n                // Protocol is the L4 protocol. If omitted or empty, any protocol\n                // matches. Accepted values: \"TCP\", \"UDP\", \"\"/\"ANY\"\n                //\n                // Matching on ICMP is not supported.\n                //\n                // +optional\n                Protocol string `json:\"protocol,omitempty\"`\n        }\n\nExample (L4)\n~~~~~~~~~~~~\n\nThe following rule limits all endpoints with the label ``app=myService`` to\nonly be able to emit packets using TCP on port 80, to any layer 3 destination:\n\n.. literalinclude:: ../../../examples/policies/l4/l4.yaml\n  :language: yaml\n\nExample Port Ranges\n~~~~~~~~~~~~~~~~~~~\n\nThe following rule limits all endpoints with the label ``app=myService`` to\nonly be able to emit packets using TCP on ports 80-444, to any layer 3 destination:\n\n.. literalinclude:: ../../../examples/policies/l4/l4_port_range.yaml\n  :language: yaml\n\n.. note:: Layer 7 rules support port ranges, except for DNS rules.\n\nLabels-dependent Layer 4 rule\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThis example enables all endpoints with the label ``role=frontend`` to\ncommunicate with all endpoints with the label ``role=backend``, but they must\ncommunicate using TCP on port 80. Endpoints with other labels will not be\nable to communicate with the endpoints with the label ``role=backend``, and\nendpoints with the label ``role=frontend`` will not be able to communicate with\n``role=backend`` on ports other than 80.\n\n.. literalinclude:: ../../../examples/policies/l4/l3_l4_combined.yaml\n  :language: yaml\n\nCIDR-dependent Layer 4 Rule\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThis example enables all endpoints with the label ``role=crawler`` to\ncommunicate with all remote destinations inside the CIDR ``192.0.2.0/24``, but\nthey must communicate using TCP on port 80. The policy does not allow Endpoints\nwithout the label ``role=crawler`` to communicate with destinations in the CIDR\n``192.0.2.0/24``. Furthermore, endpoints with the label ``role=crawler`` will\nnot be able to communicate with destinations in the CIDR ``192.0.2.0/24`` on\nports other than port 80.\n\n.. literalinclude:: ../../../examples/policies/l4/cidr_l4_combined.yaml\n  :language: yaml\n\nLimit ICMP/ICMPv6 types\n-----------------------\n\nICMP policy can be specified in addition to layer 3 policies or independently.\nIt restricts the ability of an endpoint to emit and/or receive packets on a\nparticular ICMP/ICMPv6 type (both type (integer) and corresponding CamelCase message (string) are supported).\nIf any ICMP policy is specified, layer 4 and ICMP communication will be blocked\nunless it's related to a connection that is otherwise allowed by the policy.\n\nICMP policy can be specified at both ingress and egress using the\n``icmps`` field. The ``icmps`` field takes a ``ICMPField`` structure\nwhich is defined as follows:\n\n.. code-block:: go\n\n        // ICMPField is a ICMP field.\n        //\n        // +deepequal-gen=true\n        // +deepequal-gen:private-method=true\n        type ICMPField struct {\n            // Family is a IP address version.\n            // Currently, we support `IPv4` and `IPv6`.\n            // `IPv4` is set as default.\n            //\n            // +kubebuilder:default=IPv4\n            // +kubebuilder:validation:Optional\n            // +kubebuilder:validation:Enum=IPv4;IPv6\n            Family string `json:\"family,omitempty\"`\n\n\t        // Type is a ICMP-type.\n\t        // It should be an 8bit code (0-255), or it's CamelCase name (for example, \"EchoReply\").\n\t        // Allowed ICMP types are:\n\t        //     Ipv4: EchoReply | DestinationUnreachable | Redirect | Echo | EchoRequest |\n\t        //\t\t     RouterAdvertisement | RouterSelection | TimeExceeded | ParameterProblem |\n\t        //\t\t\t Timestamp | TimestampReply | Photuris | ExtendedEcho Request | ExtendedEcho Reply\n\t        //     Ipv6: DestinationUnreachable | PacketTooBig | TimeExceeded | ParameterProblem |\n\t        //\t\t\t EchoRequest | EchoReply | MulticastListenerQuery| MulticastListenerReport |\n\t        // \t\t\t MulticastListenerDone | RouterSolicitation | RouterAdvertisement | NeighborSolicitation |\n\t        // \t\t\t NeighborAdvertisement | RedirectMessage | RouterRenumbering | ICMPNodeInformationQuery |\n\t        // \t\t\t ICMPNodeInformationResponse | InverseNeighborDiscoverySolicitation | InverseNeighborDiscoveryAdvertisement |\n\t        // \t\t\t HomeAgentAddressDiscoveryRequest | HomeAgentAddressDiscoveryReply | MobilePrefixSolicitation |\n\t        // \t\t\t MobilePrefixAdvertisement | DuplicateAddressRequestCodeSuffix | DuplicateAddressConfirmationCodeSuffix |\n\t        // \t\t\t ExtendedEchoRequest | ExtendedEchoReply\n\t        //\n\t        // +deepequal-gen=false\n\t        // +kubebuilder:validation:XIntOrString\n\t        // +kubebuilder:validation:Pattern=\"^([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5]|EchoReply|DestinationUnreachable|Redirect|Echo|RouterAdvertisement|RouterSelection|TimeExceeded|ParameterProblem|Timestamp|TimestampReply|Photuris|ExtendedEchoRequest|ExtendedEcho Reply|PacketTooBig|ParameterProblem|EchoRequest|MulticastListenerQuery|MulticastListenerReport|MulticastListenerDone|RouterSolicitation|RouterAdvertisement|NeighborSolicitation|NeighborAdvertisement|RedirectMessage|RouterRenumbering|ICMPNodeInformationQuery|ICMPNodeInformationResponse|InverseNeighborDiscoverySolicitation|InverseNeighborDiscoveryAdvertisement|HomeAgentAddressDiscoveryRequest|HomeAgentAddressDiscoveryReply|MobilePrefixSolicitation|MobilePrefixAdvertisement|DuplicateAddressRequestCodeSuffix|DuplicateAddressConfirmationCodeSuffix)$\"\n            Type *intstr.IntOrString `json:\"type\"`\n        }\n\nExample (ICMP/ICMPv6)\n~~~~~~~~~~~~~~~~~~~~~\n\nThe following rule limits all endpoints with the label ``app=myService`` to\nonly be able to emit packets using ICMP with type 8 and ICMPv6 with message EchoRequest,\nto any layer 3 destination:\n\n.. literalinclude:: ../../../examples/policies/l4/icmp.yaml\n  :language: yaml\n\nLimit TLS Server Name Indication (SNI)\n--------------------------------------\n\nWhen multiple websites are hosted on the same server with a shared IP address,\nServer Name Indication (SNI), an extension of the TLS protocol, ensures that\nthe client receives the correct SSL certificate for the website they are\ntrying to access. SNI allows the hostname or domain name of the website to be\nspecified during the TLS handshake, rather than after the handshake when the\nHTTP connection is established.\n\nCilium Network Policy can limit an endpoint's ability to establish a TLS\nhandshake to a specified list of SNIs. The SNI policy is always configured\nat the egress level and is usually set up alongside port policies.\n\nExample (TLS SNI)\n~~~~~~~~~~~~~~~~~\n\n.. note:: TLS SNI policy enforcement requires L7 proxy enabled.\n\nThe following rule limits all endpoints with the label ``app=myService`` to\nonly be able to establish TLS connections with ``one.one.one.one`` SNI. Any\nother attempt to another SNI (for example, with ``cilium.io``) will be rejected.\n\n.. literalinclude:: ../../../examples/policies/l4/l4_sni.yaml\n  :language: yaml\n\nBelow is the same SSL error while trying to connect to ``cilium.io`` from curl.\n\n.. code-block:: shell-session\n\n    $ kubectl exec <my-service-pod> -- curl -v https://cilium.io\n    * Host cilium.io:443 was resolved.\n    * IPv6: (none)\n    * IPv4: 104.198.14.52\n    *   Trying 104.198.14.52:443...\n    * Connected to cilium.io (104.198.14.52) port 443\n    * ALPN: curl offers h2,http/1.1\n    * TLSv1.3 (OUT), TLS handshake, Client hello (1):\n    *  CAfile: /etc/ssl/certs/ca-certificates.crt\n    *  CApath: /etc/ssl/certs\n    * Recv failure: Connection reset by peer\n    * OpenSSL SSL_connect: Connection reset by peer in connection to cilium.io:443\n    * Closing connection\n    curl: (35) Recv failure: Connection reset by peer\n    command terminated with exit code 35\n\n\n.. _l7_policy:\n\nLayer 7 Examples\n================\n\nLayer 7 policy rules are embedded into `l4_policy` rules and can be specified\nfor ingress and egress. ``L7Rules`` structure is a base type containing an\nenumeration of protocol specific fields.\n\n.. code-block:: go\n\n        // L7Rules is a union of port level rule types. Mixing of different port\n        // level rule types is disallowed, so exactly one of the following must be set.\n        // If none are specified, then no additional port level rules are applied.\n        type L7Rules struct {\n                // HTTP specific rules.\n                //\n                // +optional\n                HTTP []PortRuleHTTP `json:\"http,omitempty\"`\n\n                // Kafka-specific rules.\n                //\n                // +optional\n                Kafka []PortRuleKafka `json:\"kafka,omitempty\"`\n\n                // DNS-specific rules.\n                //\n                // +optional\n                DNS []PortRuleDNS `json:\"dns,omitempty\"`\n        }\n\nThe structure is implemented as a union, i.e. only one member field can be used\nper port. If multiple ``toPorts`` rules with identical ``PortProtocol`` select\nan overlapping list of endpoints, then the layer 7 rules are combined together\nif they are of the same type. If the type differs, the policy is rejected.\n\nEach member consists of a list of application protocol rules. A layer 7\nrequest is permitted if at least one of the rules matches. If no rules are\nspecified, then all traffic is permitted.\n\nIf a layer 4 rule is specified in the policy, and a similar layer 4 rule\nwith layer 7 rules is also specified, then the layer 7 portions of the\nlatter rule will have no effect.\n\n.. note:: Unlike layer 3 and layer 4 policies, violation of layer 7 rules does\n          not result in packet drops. Instead, if possible, an application\n          protocol specific access denied message is crafted and returned, e.g.\n          an *HTTP 403 access denied* is sent back for HTTP requests which\n          violate the policy, or a *DNS REFUSED* response for DNS requests.\n\n.. note:: Layer 7 rules support port ranges, except for DNS rules.\n\n.. note:: In `HostPolicies`, i.e. policies that use :ref:`NodeSelector`,\n          only DNS layer 7 rules are currently supported.\n          Other types of layer 7 rules are not supported in `HostPolicies`.\n\n.. note:: Layer 7 policies will proxy traffic through a node-local :ref:`envoy`\n          instance, which will either be deployed as a DaemonSet or embedded in the agent pod.\n          When Envoy is embedded in the agent pod, Layer 7 traffic targeted by policies\n          will therefore depend on the availability of the Cilium agent pod.\n\n.. note:: L7 policies for SNATed IPv6 traffic (e.g., pod-to-world) require a kernel with the `fix <https://patchwork.kernel.org/project/netdevbpf/patch/20250318161516.3791383-1-maxim@isovalent.com/>`__ applied.\n          The stable kernel versions with the fix are 6.14.1, 6.12.22, 6.6.86, 6.1.133, 5.15.180, 5.10.236. See :gh-issue:`37932` for the reference.\n\nHTTP\n----\n\nThe following fields can be matched on:\n\nPath\n  Path is an extended POSIX regex matched against the path of a request.\n  Currently it can contain characters disallowed from the conventional \"path\"\n  part of a URL as defined by RFC 3986. Paths must begin with a ``/``. If\n  omitted or empty, all paths are all allowed.\n\nMethod\n  Method is an extended POSIX regex matched against the method of a request,\n  e.g. ``GET``, ``POST``, ``PUT``, ``PATCH``, ``DELETE``, ...  If omitted or\n  empty, all methods are allowed.\n\nHost\n  Host is an extended POSIX regex matched against the host header of a request,\n  e.g. ``foo.com``. If omitted or empty, the value of the host header is\n  ignored.\n\nHeaders\n  Headers is a list of HTTP headers which must be present in the request. If\n  omitted or empty, requests are allowed regardless of headers present.\n\n  It's also possible to do some more advanced header matching against header\n  values. ``HeaderMatches`` is a list of HTTP headers which must be present and\n  match against the given values. Mismatch field can be used to specify what\n  to do when there is no match.\n\nAllow GET /public\n~~~~~~~~~~~~~~~~~\n\nThe following example allows ``GET`` requests to the URL ``/public`` from the\nendpoints with the labels ``env=prod`` to endpoints with the labels\n``app=service``, but requests to any other URL, or using another method, will\nbe rejected. Requests on ports other than port 80 will be dropped.\n\n.. literalinclude:: ../../../examples/policies/l7/http/simple/l7.yaml\n  :language: yaml\n\nAll GET /path1 and PUT /path2 when header set\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe following example limits all endpoints which carry the labels\n``app=myService`` to only be able to receive packets on port 80 using TCP.\nWhile communicating on this port, the only API endpoints allowed will be ``GET\n/path1``, and ``PUT /path2`` with the HTTP header ``X-My-Header`` set to\n``true``:\n\n.. literalinclude:: ../../../examples/policies/l7/http/http.yaml\n  :language: yaml\n\n.. _kafka_policy:\n\nKafka (beta)\n------------\n\n.. include:: ../../deprecated.rst\n\nPortRuleKafka is a list of Kafka protocol constraints. All fields are optional,\nif all fields are empty or missing, the rule will match all Kafka messages.\nThere are two ways to specify the Kafka rules. We can choose to specify a\nhigh-level \"produce\" or \"consume\" role to a topic or choose to specify more\nlow-level Kafka protocol specific apiKeys. Writing rules based on Kafka roles\nis easier and covers most common use cases, however if more granularity is\nneeded then users can alternatively write rules using specific apiKeys.\n\nThe following fields can be matched on:\n\nRole\n  Role is a case-insensitive string which describes a group of API keys\n  necessary to perform certain higher-level Kafka operations such as \"produce\"\n  or \"consume\". A Role automatically expands into all APIKeys required\n  to perform the specified higher-level operation.\n  The following roles are supported:\n\n    - \"produce\": Allow producing to the topics specified in the rule.\n    - \"consume\": Allow consuming from the topics specified in the rule.\n\n  This field is incompatible with the APIKey field, i.e APIKey and Role\n  cannot both be specified in the same rule.\n  If omitted or empty, and if APIKey is not specified, then all keys are\n  allowed.\n\nAPIKey\n  APIKey is a case-insensitive string matched against the key of a request,\n  for example \"produce\", \"fetch\", \"createtopic\", \"deletetopic\". For a more\n  extensive list, see the `Kafka protocol reference <https://kafka.apache.org/protocol#protocol_api_keys>`_.\n  This field is incompatible with the Role field.\n\nAPIVersion\n  APIVersion is the version matched against the api version of the Kafka\n  message. If set, it must be a string representing a positive integer. If\n  omitted or empty, all versions are allowed.\n\nClientID\n  ClientID is the client identifier as provided in the request.\n\n  From Kafka protocol documentation: This is a user supplied identifier for the\n  client application. The user can use any identifier they like and it will be\n  used when logging errors, monitoring aggregates, etc. For example, one might\n  want to monitor not just the requests per second overall, but the number\n  coming from each client application (each of which could reside on multiple\n  servers). This id acts as a logical grouping across all requests from a\n  particular client.\n\n  If omitted or empty, all client identifiers are allowed.\n\nTopic\n  Topic is the topic name contained in the message. If a Kafka request contains\n  multiple topics, then all topics in the message must be allowed by the policy\n  or the message will be rejected.\n\n  This constraint is ignored if the matched request message type does not\n  contain any topic. The maximum length of the Topic is 249 characters,\n  which must be either ``a-z``, ``A-Z``, ``0-9``, ``-``, ``.`` or ``_``.\n\n  If omitted or empty, all topics are allowed.\n\nAllow producing to topic empire-announce using Role\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. literalinclude:: ../../../examples/policies/l7/kafka/kafka-role.yaml\n  :language: yaml\n\nAllow producing to topic empire-announce using apiKeys\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. literalinclude:: ../../../examples/policies/l7/kafka/kafka.yaml\n  :language: yaml\n\n.. _dns_discovery:\n\nDNS Policy and IP Discovery\n---------------------------\n\nPolicy may be applied to DNS traffic, allowing or disallowing specific DNS\nquery names or patterns of names (other DNS fields, such as query type, are not\nconsidered). This policy is effected via a DNS proxy, which is also used to\ncollect IPs used to populate L3 `DNS based`_ ``toFQDNs`` rules.\n\n.. note::  While Layer 7 DNS policy can be applied without any other Layer 3\n           rules, the presence of a Layer 7 rule (with its Layer 3 and 4\n           components) will block other traffic.\n\nDNS policy may be applied via:\n\n``matchName``\n  Allows queries for domains that match ``matchName`` exactly. Multiple\n  distinct names may be included in separate ``matchName`` entries and queries\n  for domains that match any ``matchName`` will be allowed.\n\n``matchPattern``\n  Allows queries for domains that match the pattern in ``matchPattern``,\n  accounting for wildcards. Patterns are composed of literal characters that\n  that are allowed in domain names: a-z, 0-9, ``.`` and ``-``.\n\n  ``*`` is allowed as a wildcard with a number of convenience behaviors:\n\n  * ``*`` within a domain allows 0 or more valid DNS characters, except for the\n    ``.`` separator. ``*.cilium.io`` will match ``sub.cilium.io`` but not\n    ``cilium.io``. ``part*ial.com`` will match ``partial.com`` and\n    ``part-extra-ial.com``.\n  * ``*`` alone matches all names, and inserts all IPs in DNS responses into\n    the cilium-agent DNS cache.\n\nIn this example, L7 DNS policy allows queries for ``cilium.io``, any subdomains\nof ``cilium.io``, and any subdomains of ``api.cilium.io``. No other DNS queries\nwill be allowed.\n\nThe separate L3 ``toFQDNs`` egress rule allows connections to any IPs returned\nin DNS queries for ``cilium.io``, ``sub.cilium.io``, ``service1.api.cilium.io``\nand any matches of ``special*service.api.cilium.io``, such as\n``special-region1-service.api.cilium.io`` but not\n``region1-service.api.cilium.io``. DNS queries to ``anothersub.cilium.io`` are\nallowed but connections to the returned IPs are not, as there is no L3\n``toFQDNs`` rule selecting them. L4 and L7 policy may also be applied (see\n`DNS based`_), restricting connections to TCP port 80 in this case.\n\n.. literalinclude:: ../../../examples/policies/l7/dns/dns.yaml\n  :language: yaml\n\n.. note:: When applying DNS policy in kubernetes, queries for\n          service.namespace.svc.cluster.local. must be explicitly allowed\n          with ``matchPattern: *.*.svc.cluster.local.``.\n\n          Similarly, queries that rely on the DNS search list to complete the\n          FQDN must be allowed in their entirety. e.g. A query for\n          ``servicename`` that succeeds with\n          ``servicename.namespace.svc.cluster.local.`` must have the latter\n          allowed with ``matchName`` or ``matchPattern``. See `Alpine/musl deployments and DNS Refused`_.\n\n.. note:: DNS policies do not support port ranges.\n\n.. _DNS Obtaining Data:\n\nObtaining DNS Data for use by ``toFQDNs``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nIPs are obtained via intercepting DNS requests with a proxy. These IPs can be\nselected with ``toFQDN`` rules. DNS responses are cached within Cilium agent\nrespecting TTL.\n\n.. _DNS Proxy:\n\nDNS Proxy\n\"\"\"\"\"\"\"\"\"\n  A DNS Proxy intercepts egress DNS traffic and records IPs seen in the\n  responses. This interception is, itself, a separate policy rule governing the\n  DNS requests, and must be specified separately. For details on how to enforce\n  policy on DNS requests and configuring the DNS proxy, see `Layer 7\n  Examples`_.\n\n  Only IPs in intercepted DNS responses to an application will be allowed in\n  the Cilium policy rules. For a given domain name, IPs from responses to all\n  pods managed by a Cilium instance are allowed by policy (respecting TTLs).\n  This ensures that allowed IPs are consistent with those returned to\n  applications. The DNS Proxy is the only method to allow IPs from responses\n  allowed by wildcard L7 DNS ``matchPattern`` rules for use in ``toFQDNs``\n  rules.\n\n  The following example obtains DNS data by interception without blocking any\n  DNS requests. It allows L3 connections to ``cilium.io``, ``sub.cilium.io``\n  and any subdomains of ``sub.cilium.io``.\n\n.. literalinclude:: ../../../examples/policies/l7/dns/dns-visibility.yaml\n  :language: yaml\n\n.. note:: DNS policies do not support port ranges.\n\n\nAlpine/musl deployments and DNS Refused\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nSome common container images treat the DNS ``Refused`` response when the `DNS\nProxy`_ rejects a query as a more general failure. This stops traversal of the\nsearch list defined in ``/etc/resolv.conf``. It is common for pods to search by\nappending ``.svc.cluster.local.`` to DNS queries. When this occurs, a lookup\nfor ``cilium.io`` may first be attempted as\n``cilium.io.namespace.svc.cluster.local.`` and rejected by the proxy. Instead\nof continuing and eventually attempting ``cilium.io.`` alone, the Pod treats\nthe DNS lookup is treated as failed.\n\nThis can be mitigated with the ``--tofqdns-dns-reject-response-code`` option.\nThe default is ``refused`` but ``nameError`` can be selected, causing the proxy\nto return a NXDomain response to refused queries.\n\nA more pod-specific solution is to configure ``ndots`` appropriately for each\nPod, via ``dnsConfig``, so that the search list is not used for DNS lookups\nthat do not need it. See the `Kubernetes documentation <https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-s-dns-config>`_\nfor instructions.\n\n\n.. _deny_policies:\n\nDeny Policies\n=============\n\nDeny policies, available and enabled by default since Cilium 1.9, allows to\nexplicitly restrict certain traffic to and from a Pod.\n\nDeny policies take precedence over allow policies, regardless of whether they\nare a Cilium Network Policy, a Clusterwide Cilium Network Policy or even a\nKubernetes Network Policy.\n\nSimilarly to \"allow\" policies, Pods will enter default-deny mode as soon a\nsingle policy selects it.\n\nIf multiple allow and deny policies are applied to the same pod, the following\ntable represents the expected enforcement for that Pod:\n\n+--------------------------------------------------------------------------------------------+\n| **Set of Ingress Policies Deployed to Server Pod**                                         |\n+---------------------+-----------------------+---------+---------+--------+--------+--------+\n|                     | Layer 7 (HTTP)        | ✓       | ✓       | ✓      | ✓      |        |\n|                     +-----------------------+---------+---------+--------+--------+--------+\n|                     | Layer 4 (80/TCP)      | ✓       | ✓       | ✓      | ✓      |        |\n| **Allow Policies**  +-----------------------+---------+---------+--------+--------+--------+\n|                     | Layer 4 (81/TCP)      | ✓       | ✓       | ✓      | ✓      |        |\n|                     +-----------------------+---------+---------+--------+--------+--------+\n|                     | Layer 3 (Pod: Client) | ✓       | ✓       | ✓      | ✓      |        |\n+---------------------+-----------------------+---------+---------+--------+--------+--------+\n|                     | Layer 4 (80/TCP)      |         | ✓       |        | ✓      | ✓      |\n| **Deny Policies**   +-----------------------+---------+---------+--------+--------+--------+\n|                     | Layer 3 (Pod: Client) |         |         | ✓      | ✓      |        |\n+---------------------+-----------------------+---------+---------+--------+--------+--------+\n| **Result for Traffic Connections (Allowed / Denied)**                                      |\n+---------------------+-----------------------+---------+---------+--------+--------+--------+\n|                     | curl server:81        | Allowed | Allowed | Denied | Denied | Denied |\n|                     +-----------------------+---------+---------+--------+--------+--------+\n| **Client → Server** | curl server:80        | Allowed | Denied  | Denied | Denied | Denied |\n|                     +-----------------------+---------+---------+--------+--------+--------+\n|                     | ping server           | Allowed | Allowed | Denied | Denied | Denied |\n+---------------------+-----------------------+---------+---------+--------+--------+--------+\n\nIf we pick the second column in the above table, the bottom section shows the\nforwarding behaviour for a policy that selects curl or ping traffic between the\nclient and server:\n\n* Curl to port 81 is allowed because there is an allow policy on port 81, and\n  no deny policy on that port;\n* Curl to port 80 is denied because there is a deny policy on that port;\n* Ping to the server is allowed because there is a Layer 3 allow policy and no deny.\n\nThe following policy will deny ingress from \"world\" on all namespaces on all\nPods managed by Cilium. Existing inter-cluster policies will still be allowed\nas this policy is allowing traffic from everywhere except from \"world\".\n\n.. literalinclude:: ../../../examples/policies/l3/entities/from_world_deny.yaml\n  :language: yaml\n\nDeny policies do not support: policy enforcement at L7, i.e., specifically\ndenying an URL and ``toFQDNs``, i.e., specifically denying traffic to a specific\ndomain name.\n\n.. _disk_policies:\n\nDisk based Cilium Network Policies\n==================================\nThis functionality enables users to place network policy YAML files directly into\nthe node's filesystem, bypassing the need for definition via k8s CRD.\nBy setting the config field ``static-cnp-path``, users specify the directory from\nwhich policies will be loaded. The Cilium agent then processes all policy YAML files\npresent in this directory, transforming them into rules that are incorporated into\nthe policy engine. Additionally, the Cilium agent monitors this directory for any\nnew policy YAML files as well as any updates or deletions, making corresponding\nupdates to the policy engine's rules. It is important to note that this feature\nonly supports CiliumNetworkPolicy and CiliumClusterwideNetworkPolicy.\n\nThe directory that the Cilium agent needs to monitor should be mounted from the host\nusing volume mounts. For users deploying via Helm, this can be enabled via ``extraArgs``\nand ``extraHostPathMounts`` as follows:\n\n.. code-block:: yaml\n\n   extraArgs:\n   - --static-cnp-path=/policies\n   extraHostPathMounts:\n   - name: static-policies\n      mountPath: /policies\n      hostPath: /policies\n      hostPathType: Directory\n\nTo determine whether a policy was established via Kubernetes CRD or directly from a directory,\nexecute the command ``cilium policy get`` and examine the source attribute within the policy.\nIn output, you could notice policies that have been sourced from a directory will have the\n``source`` field set as ``directory``. Additionally, ``cilium endpoint get <endpoint_id>`` also have\nfields to show the source of policy associated with that endpoint.\n\nPrevious limitations and known issues\n-------------------------------------\n\nFor Cilium versions prior to 1.14 deny-policies for peers outside the cluster\nsometimes did not work because of :gh-issue:`15198`.  Make sure that you are\nusing version 1.14 or later if you are relying on deny policies to manage\nexternal traffic to your cluster.\n\n.. _HostPolicies:\n\nHost Policies\n=============\n\nHost policies take the form of a :ref:`CiliumClusterwideNetworkPolicy` with a\n:ref:`NodeSelector` instead of an :ref:`EndpointSelector`. Host policies can\nhave layer 3 and layer 4 rules on both ingress and egress. They cannot have\nlayer 7 rules.\n\nHost policies apply to all the nodes selected by their :ref:`NodeSelector`. In\neach selected node, they apply only to the host namespace, including\nhost-networking pods. They don't apply to communications between\nnon-host-networking pods and locations outside of the cluster.\n\nInstallation of Host Policies requires the addition of the following ``helm``\nflags when installing Cilium:\n\n* ``--set devices='{interface}'`` where ``interface`` refers to the network\n  device Cilium is configured on, for example ``eth0``. If you omit this\n  option, Cilium auto-detects what interface the host firewall applies to.\n* ``--set hostFirewall.enabled=true``\n\nAs an example, the following policy allows ingress traffic for any node with\nthe label ``type=ingress-worker`` on TCP ports 22, 6443 (kube-apiserver), 2379\n(etcd), and 4240 (health checks), as well as UDP port 8472 (VXLAN).\n\n.. literalinclude:: ../../../examples/policies/host/lock-down-ingress.yaml\n  :language: yaml\n\nTo reuse this policy, replace the ``port:`` values with ports used in your\nenvironment.\n\nIn order to allow protocols such as VRRP and IGMP that don't have any transport-layer\nports, set ``--enable-extended-ip-protocols`` flag to true. By default, such traffic is\ndropped with ``DROP_CT_UNKNOWN_PROTO`` error.\n\nAs an example, the following policy allows egress traffic on any node with\nthe label ``type=egress-worker`` on TCP ports 22, 6443/443 (kube-apiserver), 2379\n(etcd), and 4240 (health checks), UDP port 8472 (VXLAN), and traffic with VRRP protocol.\n\n.. literalinclude:: ../../../examples/policies/host/allow-extended-protocols.yaml\n  :language: yaml\n\n.. _troubleshooting_host_policies:\n\nTroubleshooting Host Policies\n-----------------------------\n\nIf you have troubles with Host Policies, try the following steps:\n\n- Ensure the ``helm`` options listed in :ref:`the Host Policies description\n  <HostPolicies>` were applied during installation.\n\n- To verify that your policy has been accepted and applied by the Cilium agent,\n  run ``kubectl get CiliumClusterwideNetworkPolicy -o yaml`` and make sure the\n  policy is listed.\n\n- If policies don't seem to be applied to your nodes, verify the\n  ``nodeSelector`` is labeled correctly in your environment. In the example\n  configuration, you can run ``kubectl get nodes -o\n  custom-columns=NAME:.metadata.name,LABELS:.metadata.labels | grep\n  type:ingress-worker`` to verify labels match the policy.\n\nTo troubleshoot policies for a given node, try the following steps. For all\nsteps, run ``cilium-dbg`` in the relevant namespace, on the Cilium agent pod\nfor the node, for example with:\n\n.. code-block:: shell-session\n\n   $ kubectl exec -n $CILIUM_NAMESPACE $CILIUM_POD_NAME -- cilium-dbg ...\n\nRetrieve the endpoint ID for the host endpoint on the node with ``cilium-dbg\nendpoint get -l reserved:host -o jsonpath='{[0].id}'``. Use this ID to replace\n``$HOST_EP_ID`` in the next steps:\n\n- If policies are applied, but not enforced for the node, check the status of\n  the policy audit mode with ``cilium-dbg endpoint config $HOST_EP_ID | grep\n  PolicyAuditMode``. If necessary, :ref:`disable the audit mode\n  <disable_policy_audit_mode>`.\n\n- Run ``cilium-dbg endpoint list``, and look for the host endpoint, with\n  ``$HOST_EP_ID`` and the ``reserved:host`` label. Ensure that policy is\n  enabled in the selected direction.\n\n- Run ``cilium-dbg status list`` and check the devices listed in the ``Host\n  firewall`` field. Verify that traffic actually reaches the listed devices.\n\n- Use ``cilium-dbg monitor`` with ``--related-to $HOST_EP_ID`` to examine\n  traffic for the host endpoint.\n\n.. _host_policies_known_issues:\n\nHost Policies known issues\n--------------------------\n\n- The first time Cilium enforces Host Policies in the cluster, it may drop\n  reply traffic for legitimate connections that should be allowed by the\n  policies in place. Connections should stabilize again after a few seconds.\n  One workaround is to enable, disable, then re-enable Host Policies\n  enforcement. For details, see :gh-issue:`25448`.\n\n- In the context of ClusterMesh, the following combination of options is not\n  supported:\n\n  - Cilium operating in CRD mode (as opposed to KVstore mode),\n  - Host Policies enabled,\n  - tunneling enabled,\n  - kube-proxy-replacement enabled, and\n  - WireGuard enabled.\n\n  This combination results in a failure to connect to the\n  clustermesh-apiserver. For details, refer to :gh-issue:`31209`.\n\n- Host Policies do not work on host WireGuard interfaces. For details, see\n  :gh-issue:`17636`.\n\n- When Host Policies are enabled, hosts drop traffic from layer-2 protocols\n  that they consider as unknown, even if no Host Policies are loaded. For\n  example, this affects LLC traffic (see :gh-issue:`17877`) or VRRP traffic\n  (see :gh-issue:`18347`).\n\n- When kube-proxy-replacement is disabled, or configured not to implement\n  services for the native device (such as NodePort), hosts will enforce Host\n  Policies on service addresses rather than the service endpoints. For details,\n  refer to :gh-issue:`12545`.\n",
  "item_type": "unknown",
  "module_path": "/tmp/cilium-repo/Documentation/security/policy/language.rst",
  "extracted_at": "2025-09-03T01:13:28.751840Z"
}