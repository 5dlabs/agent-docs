{
  "url": "file:///tmp/cilium-repo/Documentation/network/clustermesh/aks-clustermesh-prep.rst",
  "content": ".. _gs_clustermesh_aks_prep: \n \n AKS-to-AKS Clustermesh Preparation \n \n This is a step-by-step guide on how to install and prepare\nAKS (Azure Kubernetes Service) clusters in BYOCNI mode to meet the requirements\nfor the clustermesh feature. \n This guide describes how to install two AKS clusters in BYOCNI (Bring Your Own CNI)\nmode and connect them together via clustermesh. This guide is not\napplicable for cross-cloud clustermesh since this guide doesn't expose the node\nIPs outside of the Azure cloud. \n .. note:: \n     BYOCNI requires the ``aks-preview`` CLI extension with version >=\n    0.5.55, which itself requires an ``az`` CLI version >= 2.32.0.\n \n Install cluster one\n################### \n \n \n Create a resource group for the cluster (or set the environment variables\nto an existing resource group). \n .. code-block:: bash \n export NAME=\"$(whoami)-$RANDOM\"\nexport AZURE_RESOURCE_GROUP=\"${NAME}-group\"\n\n#  westus2 can be changed to any available location (`az account list-locations`)\naz group create --name \"${AZURE_RESOURCE_GROUP}\" -l westus2\n \n \n \n Create a VNet (virtual network).\nCreating a custom VNet is required to ensure that the Node, Pod, and\nService CIDRs are unique and they don't overlap with other clusters. \n .. note::\nThe example below uses range  192.168.10.0/24  range, but you could use any range except for  169.254.0.0/16 ,  172.30.0.0/16 ,\n 172.31.0.0/16 , or  192.0.2.0/24  which are\n reserved by Azure <https://docs.microsoft.com/en-us/azure/aks/configure-azure-cni#prerequisites> __. \n .. code-block:: bash \n az network vnet create \\\n    --resource-group \"${AZURE_RESOURCE_GROUP}\" \\\n    --name \"${NAME}-cluster-net\" \\\n    --address-prefixes 192.168.10.0/24 \\\n    --subnet-name \"${NAME}-node-subnet\" \\\n    --subnet-prefix 192.168.10.0/24\n\n# Store the ID of the created subnet\nexport NODE_SUBNET_ID=$(az network vnet subnet show \\\n    --resource-group \"${AZURE_RESOURCE_GROUP}\" \\\n    --vnet-name \"${NAME}-cluster-net\" \\\n    --name \"${NAME}-node-subnet\" \\\n    --query id \\\n    -o tsv)\n \n \n \n You now have a virtual network and a subnet with the same CIDR. Create an AKS cluster without a CNI and request to use a custom VNet and subnet. \n During creation request to use  \"10.10.0.0/16\"  as the pod CIDR and\n \"10.11.0.0/16\"  as the services CIDR. These can be changed to any range\nexcept for Azure reserved ranges and ranges used by other clusters you intend to\nadd to the clustermesh. \n .. code-block:: bash \n az aks create \\\n    --resource-group \"${AZURE_RESOURCE_GROUP}\" \\\n    --name \"${NAME}\" \\\n    --network-plugin none \\\n    --pod-cidr \"10.10.0.0/16\" \\\n    --service-cidr \"10.11.0.0/16\" \\\n    --dns-service-ip \"10.11.0.10\" \\\n    --vnet-subnet-id \"${NODE_SUBNET_ID}\"\n\n# Get kubectl credentials, the command will merge the new credentials\n# with the existing ~/.kube/config\naz aks get-credentials \\\n    --resource-group \"${AZURE_RESOURCE_GROUP}\" \\\n    --name \"${NAME}\"\n \n \n \n Install Cilium, it is important to give\nthe cluster a unique cluster ID and to tell Cilium to use our custom pod CIDR. \n .. parsed-literal:: \n cilium install |CHART_VERSION| \\\n    --set azure.resourceGroup=\"${AZURE_RESOURCE_GROUP}\" \\\n    --set cluster.id=1 \\\n    --set ipam.operator.clusterPoolIPv4PodCIDRList='{10.10.0.0/16}'\n \n \n \n Check the status of Cilium. \n .. code-block:: bash \n cilium status   \n \n \n \n Before configuring cluster two, store the name of the current cluster. \n .. code-block:: bash \n export CLUSTER1=${NAME}\n \n \n \n Install cluster two\n################### \n Installing the second cluster uses the same commands but with slightly different\narguments. \n \n \n Create a new resource group. \n .. code-block:: bash \n export NAME=\"$(whoami)-$RANDOM\"\nexport AZURE_RESOURCE_GROUP=\"${NAME}-group\"\n\n# eastus2 can be changed to any available location (`az account list-locations`)\naz group create --name \"${AZURE_RESOURCE_GROUP}\" -l eastus2\n \n \n \n Create a VNet in this resource group. Make sure to use a non-overlapping prefix. \n .. note::\nThe example below uses range  192.168.20.0/24 , but you could use any range except for  169.254.0.0/16 ,  172.30.0.0/16 ,\n 172.31.0.0/16 , or  192.0.2.0/24  which are\n reserved by Azure <https://docs.microsoft.com/en-us/azure/aks/configure-azure-cni#prerequisites> __. \n .. code-block:: bash \n az network vnet create \\\n    --resource-group \"${AZURE_RESOURCE_GROUP}\" \\\n    --name \"${NAME}-cluster-net\" \\\n    --address-prefixes 192.168.20.0/24 \\\n    --subnet-name \"${NAME}-node-subnet\" \\\n    --subnet-prefix 192.168.20.0/24\n\n# Store the ID of the created subnet\nexport NODE_SUBNET_ID=$(az network vnet subnet show \\\n    --resource-group \"${AZURE_RESOURCE_GROUP}\" \\\n    --vnet-name \"${NAME}-cluster-net\" \\\n    --name \"${NAME}-node-subnet\" \\\n    --query id \\\n    -o tsv)\n \n \n \n Create an AKS cluster without CNI and request to use your custom VNet and\nsubnet. \n During creation use  \"10.20.0.0/16\"  as the pod CIDR and\n \"10.21.0.0/16\"  as the services CIDR. These can be changed to any range\nexcept for Azure reserved ranges and ranges used by other clusters you intend to\nadd to the clustermesh. \n .. code-block:: bash \n az aks create \\\n    --resource-group \"${AZURE_RESOURCE_GROUP}\" \\\n    --name \"${NAME}\" \\\n    --network-plugin none \\\n    --pod-cidr \"10.20.0.0/16\" \\\n    --service-cidr \"10.21.0.0/16\" \\\n    --dns-service-ip \"10.21.0.10\" \\\n    --vnet-subnet-id \"${NODE_SUBNET_ID}\"\n\n# Get kubectl credentials and add them to ~/.kube/config\naz aks get-credentials \\\n    --resource-group \"${AZURE_RESOURCE_GROUP}\" \\\n    --name \"${NAME}\"\n \n \n \n Install Cilium, it is important to give\nthe cluster a unique cluster ID and to tell Cilium to use your custom pod CIDR. \n .. parsed-literal:: \n cilium install |CHART_VERSION| \\\n    --set azure.resourceGroup=\"${AZURE_RESOURCE_GROUP}\" \\\n    --set cluster.id=2 \\\n    --set ipam.operator.clusterPoolIPv4PodCIDRList='{10.20.0.0/16}'\n \n \n \n Check the status of Cilium. \n .. code-block:: bash \n cilium status\n \n \n \n Before configuring peering and clustermesh, store the current cluster\nname. \n .. code-block:: bash \n export CLUSTER2=${NAME}\n \n \n \n Peering virtual networks\n######################## \n Virtual networks can't connect to each other by default. You can enable cross\nVNet communication by creating bi-directional \"peering\". \n Create a peering from cluster one to cluster two using the\nfollowing commands. \n .. code-block:: bash \n export VNET_ID=$(az network vnet show \\\n    --resource-group \"${CLUSTER2}-group\" \\\n    --name \"${CLUSTER2}-cluster-net\" \\\n    --query id -o tsv)\n\naz network vnet peering create \\\n    -g \"${CLUSTER1}-group\" \\\n    --name \"peering-${CLUSTER1}-to-${CLUSTER2}\" \\\n    --vnet-name \"${CLUSTER1}-cluster-net\" \\\n    --remote-vnet \"${VNET_ID}\" \\\n    --allow-vnet-access\n \n This allows outbound traffic from cluster one to cluster two. To allow\nbi-directional traffic, add a peering to the other direction as well. \n .. code-block:: bash \n export VNET_ID=$(az network vnet show \\\n    --resource-group \"${CLUSTER1}-group\" \\\n    --name \"${CLUSTER1}-cluster-net\" \\\n    --query id -o tsv)\n\naz network vnet peering create \\\n    -g \"${CLUSTER2}-group\" \\\n    --name \"peering-${CLUSTER2}-to-${CLUSTER1}\" \\\n    --vnet-name \"${CLUSTER2}-cluster-net\" \\\n    --remote-vnet \"${VNET_ID}\" \\\n    --allow-vnet-access\n \n Node-to-node traffic between clusters is now possible. All requirements for\nclustermesh are met. Enabling clustermesh is explained in :ref: gs_clustermesh .",
  "item_type": "unknown",
  "module_path": "/tmp/cilium-repo/Documentation/network/clustermesh/aks-clustermesh-prep.rst",
  "extracted_at": "2025-09-03T01:13:29.202154Z"
}