{
  "url": "file:///tmp/cilium-repo/Documentation/security/tls-visibility.rst",
  "content": ".. only:: not (epub or latex or html) \n WARNING: You are looking at unreleased Cilium documentation.\nPlease use the official rendered version released here:\nhttps://docs.cilium.io\n \n .. _gs_tls_inspection: \n \n Inspecting TLS Encrypted Connections with Cilium \n \n This document serves as an introduction for how network security teams can use Cilium to transparently inspect\nTLS-encrypted connections.  This TLS-aware inspection allows Cilium API-aware visibility and policy to function\neven for connections where client to server\ncommunication is protected by TLS, such as when a client accesses the API service via HTTPS.  This capability is similar to\nwhat is possible to traditional hardware firewalls, but is implemented entirely in software on the Kubernetes worker node,\nand is policy driven, allowing inspection to target only selected network connectivity. \n This type of visibility is\nextremely valuable to be able to monitor how external API services are being used,\nfor example, understanding which S3 buckets are being accessed by an given application. \n .. include:: gsg_requirements.rst \n A Brief Overview of the TLS Certificate Model \n TLS is a protocol that \"wraps\" other protocols like HTTP and ensures that communication between client and\nserver has confidentiality (no one can read the data except the intended recipient), integrity (recipient\ncan confirm that the data has not been modified in transit), and authentication (sender can confirm that\nit is talking with the intended destination, not an impostor).  We will provide a highly simplified overview\nof TLS in this document, but for full details, please see\n <https://en.wikipedia.org/wiki/Transport_Layer_Security> _ . \n From an authentication perspective, the TLS model relies on a \"Certificate Authority\" (CA) which is an\nentity that is trusted to create proof that a given network service (e.g., www.cilium.io)\nis who they say they are.   The goal is to prevents a malicious party in the network between the client\nand the server from intercepting the traffic and pretending to be the destination server. \n In the case of \"friendly interception\" for network security monitoring, Cilium uses a model similar to\ntraditional firewalls with TLS inspection capabilities:  the network security team creates their own \"internal\ncertificate authority\"\nthat can be used to create alternative certificates for external destinations.  This model requires each\nclient workload to also trust this new certificate, otherwise the client's TLS library will reject\nthe connection as invalid.  In this model, the network firewall uses the certificate signed by the internal\nCA to act like the destination service and terminate the TLS connection.  This allows the firewall to\ninspect and even modify the application layer data, and then initiate another TLS connect to the actual\ndestination service. \n The CA model within TLS is based on cryptographic keys and certificates.  Realizing the above model\nrequires four primary steps: \n \n \n Create an internal certificate authority by generating a CA private key and CA certificate. \n \n \n For any destination where TLS inspection is desired (e.g., httpbin.org in the example below),\ngenerate a private key and certificate signing request with a common name that matches the destination DNS\nname. \n \n \n Use the CA private key to create a signed certificate. \n \n \n Ensure that all clients where TLS inspection is have the CA certificate installed so that they will\ntrust all certificates signed by that CA. \n \n \n Given that Cilium will be terminating the initial TLS connection from the\nclient and creating a new TLS connection to the destination, Cilium must be told the set of CAs that it\nshould trust when validating the new TLS connection to the destination service. \n \n \n .. note:: \n In a non-demo environment it is EXTREMELY important that you keep the above private keys safe, as anyone\nwith access to this private key will be able to inspect TLS-encrypted traffic (certificates on the other\nhand are public information, and are not at all sensitive).  In the guide below, the\nCA private key does not need to be provided to Cilium at all (it is used only to create certificates, which\ncan be done offline) and private keys for individual destination services are stored as Kubernetes secrets.\nThese secrets should be stored in a namespace where they can be accessed by Cilium, but not general purpose\nworkloads.\n \n How TLS Inspection works \n All TLS inspection relies on terminating the originating connection with a certificate\nthat will be accepted, then originating a new TLS connection using a client certificate\nif necessary. \n Because of this, the Network Policy requires configuring a  terminatingTLS  and optionally\nan  originatingTLS  stanza. \n When the Network Policy contains these details, then Cilium will redirect TLS connections to Envoy,\nand allow connections that complete a TLS handshake and pass the configured Network Policy. \n One of the most important parts of the configuration for this is how the certificates get to Envoy. \n In the current version, Cilium has two options, NPDS (the original) and SDS (the new and better version). \n Network Policy Discovery Service (NPDS) \n In this version, certificates and keys are sent inline as Base64 encoded text in dedicated fields\nin the Cilium-owned Network Policy Discovery Service. \n This had the advantage that it was straightforward to build, but does come with a big disadvantage: \n Each Network Policy rule that does TLS Interception keeps its own copy of each secret inline in the\nNPDS config in Envoy. So, if (as is likely for a larger installation), you have the same secret reused\nmultiple times (for example if you generate one certificate that will terminate for many SANs, but you\nhave multiple rules using that certificate, or you include a valid root certificate bundle in the\n originatingTLS  config), then multiple copies of the certificate will be stored in Envoy's memory.\nThis memory use can really add up in a large installation. \n It also means that we don't benefit from work that has been done to protect secrets when they are sent\nusing Secret Discovery Service (. \n Secret Discovery Service (SDS) \n Both of the above reasons are why Envoy supports SDS for Network Policy secrets as of Cilium 1.17. \n In this configuration, Cilium reads relevant Secrets from a configured secrets namespace, and exposes\nthose secrets to Envoy using the core Envoy SDS API. Those secrets are then referenced in the NPDS config\nthat's sent to Envoy to configure the Network Policy filter there by name, rather than being included\ndirectly as Base64 encoded text. \n This means that Envoy looks up the SDS secrets for NPDS in the same way as it does the secrets for\nIngress or Gateway API config. \n This method also allows Envoy to deduplicate the storage of the secrets, since they are essentially\nbeing passed by reference instead of being passed by value. \n Because of these advantages over the older NPDS method, SDS is the default for new Cilium installations\nas at Cilium 1.17. \n Configuring TLS Interception \n There are three ways to use Cilium in 1.17 and later: \n \n Using SDS, Secrets referenced in Network Policy can be located anywhere in the cluster, and are\ncopied into a configured namespace ( cilium-secrets  by default) by the Cilium Operator, synchronized\nfrom there into SDS, then referenced in NPDS using that name. This is the default for new clusters,\nand the recommended method of operation. \n Secrets can be located anywhere in the cluster, and the Cilium Agent can be granted read access to\nall Secrets in the cluster. In this case, Secrets are read directly from their original location by\nthe Cilium Agent and sent inline in NPDS. This deployment method is included for backwards compatibility\npurposes and is  not recommended , as it  significantly expands  the security scope of the agent. \n Secrets can be added directly to the  cilium-secrets  namespace, then referenced in that namespace\nfrom Network Policy. This is also included for backwards compatibility based on user feedback about\nhow this feature was actually being used. It is the default for  upgraded  clusters that have not\nconfigured any settings and are using the  upgradeCompatibility  setting in Helm, set to  1.16 \nor below. \n \n There are three settings in Helm that affect TLS Interception: \n \n tls.secretsNamespace.name  - default  cilium-secrets . Configures the secrets namespace that will\nbe used for Policy secrets. Note that this is set to the same value as a similar setting for Ingress,\nGateway API, and BGP configuration by default, but  may  be set to a different value. \n tls.readSecretsOnlyFromSecretsNamespace  - default  true . This setting tells the Helm chart\nand Cilium whether the Cilium Agent should only read secrets from the configured Secrets namespace,\nor if the Cilium Agent should attempt to read Secrets directly from their location in the cluster.\nPrevious versions of Cilium used the item  tls.secretsBackend , which could be set to  local \n(meaning only read from the Secrets namespace) or  k8s  (meaning read from any namespace), but that\nfield is now  deprecated , as its naming had become detached from its function. Previous installations\nthat set  tls.secretsBackend  to  k8s  should migrate to setting  tls.readSecretsOnlyFromSecretsNamespace \nto  false  instead, although the setting will continue to work for Cilium 1.17.  tls.secretsBackend \nwill be removed in a future Cilium version. \n tls.secretSync.enabled  - default  true  for new clusters. Configures secret synchronization and\nSDS use for Network Policy secrets. SDS use requires this to be set to  true , and must be disabled\nwhen this field is set to  false , so having an additional field for SDS config added no value. \n \n Configuring the three available modes for TLS Interception \n SDS Mode (recommended, default for new clusters):\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ \n Set the following settings in your Helm Values: \n .. code-block:: YAML \n tls:\n  readSecretsOnlyFromSecretsNamespace: true\n  secretsNamespace: \n    name: cilium-secrets # This setting is optional, as it is the default\n  secretSync:\n    enabled: true\n \n Read all Secrets in the Cluster mode (not recommended)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ \n Set the following settings in your Helm Values: \n .. code-block:: YAML \n tls:\n  readSecretsOnlyFromSecretsNamespace: false\n  secretSync:\n    enabled: false\n \n Read Secrets only from secrets namespace, no SDS (default for upgraded clusters)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ \n Set the following settings in your Helm Values: \n .. code-block:: YAML \n tls:\n  readSecretsOnlyFromSecretsNamespace: true\n  secretsNamespace: \n    name: cilium-secrets # This setting is optional, as it is the default\n  secretSync:\n    enabled: false\n \n .. Note:: \n If you are using this mode, then you will need to replace all references to\n``kube-system`` in the validation instructions on this page with ``cilium-secrets``\n(or whatever value you have set that namespace to).\n \n Once you've chosen an option and configured your Cilium installation accordingly, proceed with\nverifying your install using the rest of these instructions. \n Deploy the Demo Application \n To demonstrate TLS-interception we will use the same  mediabot  application that we used for the DNS-aware policy example.\nThis application will access the Star Wars API service using HTTPS, which would normally mean that network-layer mechanisms\nlike Cilium would not be able to see the HTTP-layer details of the communication, since all application data is encrypted\nusing TLS before that data is sent on the network. \n In this guide we will learn about: \n \n Creating an internal Certificate Authority (CA) and associated certificates signed by that CA to enable TLS interception. \n Using Cilium network policy to select the traffic to intercept using DNS-based policy rules. \n Inspecting the details of the HTTP request using cilium monitor (accessing this visibility data via Hubble, and applying\nCilium network policies to filter/modify the HTTP request is also possible, but is beyond the scope of this simple Getting Started Guide) \n \n First off, we will create a single pod  mediabot  application: \n .. parsed-literal:: \n $ kubectl create -f \\ |SCM_WEB|/examples/kubernetes-dns/dns-sw-app.yaml\n$ kubectl wait pod/mediabot --for=condition=Ready\n$ kubectl get pods\nNAME                             READY   STATUS    RESTARTS   AGE\npod/mediabot                     1/1     Running   0          14s \n Generating and Installing TLS Keys and Certificates \n Now that we understand TLS and have configured Cilium to use TLS interception, we will walk through the\nconcrete steps to generate the appropriate keys and certificates using the  openssl  utility. \n The following image describes the different files containing cryptographic data that are generated\nor copied, and what components in the system need access to those files: \n .. image:: images/cilium-tls-interception.png \n You can use openssl on your local system if it is already installed, but if not a simple\nshortcut is to use  kubectl exec  to execute  /bin/bash  within any of the cilium pods, and\nthen run the resulting  openssl  commands.  Use  kubectl cp  to copy the resulting files out\nof the cilium pod when it is time to use them to create Kubernetes secrets of copy them to the\n mediabot  pod. \n Create an Internal Certificate Authority (CA) \n Generate CA private key named 'myCA.key': \n .. code-block:: shell-session \n $ openssl genrsa -des3 -out myCA.key 2048\n \n Enter any password, just remember it for some of the later steps. \n Generate CA certificate from the private key: \n .. code-block:: shell-session \n $ openssl req -x509 -new -nodes -key myCA.key -sha256 -days 1825 -out myCA.crt\n \n The values you enter for each prompt do not need to be any specific value, and do not need to be\naccurate. \n Create Private Key and Certificate Signing Request for a Given DNS Name \n Generate an internal private key and certificate signing with a common name that matches the DNS name\nof the destination service to be intercepted for inspection (in this example, use  httpbin.org ). \n First create the private key: \n .. code-block:: shell-session \n $ openssl genrsa -out internal-httpbin.key 2048\n \n Next, create a certificate signing request, specifying the DNS name of the destination service\nfor the common name field when prompted.  All other prompts can be filled with any value. \n .. code-block:: shell-session \n $ openssl req -new -key internal-httpbin.key -out internal-httpbin.csr\n \n The only field that must be a specific value is ensuring that  Common Name  is the exact DNS\ndestination  httpbin.org  that will be provided to the client. \n This example workflow will work for any DNS\nname as long as the toFQDNs rule in the policy YAML (below) is also updated to match the DNS name in the certificate. \n Use CA to Generate a Signed Certificate for the DNS Name \n Use the internal CA private key to create a signed certificate for httpbin.org named  internal-httpbin.crt . \n .. code-block:: shell-session \n $ openssl x509 -req -days 360 -in internal-httpbin.csr -CA myCA.crt -CAkey myCA.key -CAcreateserial -out internal-httpbin.crt -sha256\n \n Next we create a Kubernetes secret that includes both the private key and signed certificates for the destination service: \n .. code-block:: shell-session \n $ kubectl create secret tls httpbin-tls-data -n kube-system --cert=internal-httpbin.crt --key=internal-httpbin.key\n \n Add the Internal CA as a Trusted CA Inside the Client Pod \n Once the CA certificate is inside the client pod, we still must make sure that the CA file is picked up by the TLS library used by your\napplication.  Most Linux applications automatically use a set of trusted CA certificates that are bundled along with the Linux distro.\nIn this guide, we are using an Ubuntu container as the client, and so will update it with Ubuntu specific instructions.  Other Linux distros\nwill have different mechanisms.  Also, individual applications may leverage their own certificate stores rather than use the OS certificate\nstore.  Java applications and the aws-cli are two common examples.  Please refer to the application or application runtime documentation\nfor more details. \n For Ubuntu, we first copy the additional CA certificate to the client pod filesystem \n .. code-block:: shell-session \n $ kubectl cp myCA.crt default/mediabot:/usr/local/share/ca-certificates/myCA.crt\n \n Then run the Ubuntu-specific utility that adds this certificate to the global set of trusted certificate authorities in /etc/ssl/certs/ca-certificates.crt . \n .. code-block:: shell-session \n $ kubectl exec mediabot -- update-ca-certificates\n \n This command will issue a WARNING, but this can be ignored. \n Provide Cilium with List of Trusted CAs \n Next, we will provide Cilium with the set of CAs that it should trust when originating the secondary TLS connections.\nThis list should correspond to the standard set of global CAs that your organization trusts.  A logical option for this is the standard CAs\nthat are trusted by your operating system, since this is the set of CAs that were being used prior to introducing TLS inspection. \n To keep things simple, in this example we will simply copy this list out of the Ubuntu filesystem of the mediabot pod, though it is\nimportant to understand that this list of trusted CAs is not specific to a particular TLS client or server, and so this step need only\nbe performed once regardless of how many TLS clients or servers are involved in TLS inspection. \n .. code-block:: shell-session \n $ kubectl cp default/mediabot:/etc/ssl/certs/ca-certificates.crt ca-certificates.crt\n \n We then will create a Kubernetes secret using this certificate bundle so that Cilium can read the\ncertificate bundle and use it to validate outgoing TLS connections. \n .. code-block:: shell-session \n $ kubectl create secret generic tls-orig-data -n kube-system --from-file=ca.crt=./ca-certificates.crt\n \n Apply DNS and TLS-aware Egress Policy \n Up to this point, we have created keys and certificates to enable TLS inspection, but we\nhave not told Cilium which traffic we want to intercept and inspect.   This is done using\nthe same Cilium Network Policy constructs that are used for other Cilium Network Policies. \n The following Cilium network policy indicates that Cilium should perform HTTP-aware inspect\nof communication between the  mediabot  pod to  httpbin.org . \n .. literalinclude:: ../../examples/kubernetes-tls-inspection/l7-visibility-tls.yaml\n:language: yaml \n Let's take a closer look at the policy: \n \n The  endpointSelector  means that this policy will only apply to pods with labels  class: mediabot, org:empire  to have the egress access. \n The first egress section uses  toFQDNs: matchName  specification to allow TCP port 443 egress to  httpbin.org . \n The  http  section below the toFQDNs rule\nindicates that such connections should be parsed as HTTP, with a policy of  {}  which will allow all requests. \n The  terminatingTLS  and  originatingTLS  sections indicate that TLS interception should be used to terminate the initial TLS connection\nfrom mediabot and initiate a new out-bound TLS connection to  httpbin.org . \n The second egress section allows  mediabot  pods to access  kube-dns  service. Note that\n rules: dns  instructs Cilium to inspect and allow DNS lookups matching specified patterns.\nIn this case, inspect and allow all DNS queries. \n \n Note that with this policy the  mediabot  doesn't have access to any internal cluster service other than  kube-dns \nand will have no access to any other external destinations either. Refer to :ref: Network Policy \nto learn more about policies for controlling access to internal cluster services. \n Let's apply the policy: \n .. parsed-literal:: \n $ kubectl create -f \\ |SCM_WEB|/examples/kubernetes-tls-inspection/l7-visibility-tls.yaml \n Demonstrating TLS Inspection \n Recall that the policy we pushed will allow all HTTPS requests from  mediabot  to  httpbin.org , but will parse all data at\nthe HTTP-layer, meaning that cilium monitor will report each HTTP request and response. \n To see this, open a new window and run the following command to identity the name of the\ncilium pod (e.g, cilium-97s78) that is running on the same Kubernetes worker node as the  mediabot  pod. \n Then start running cilium-dbg monitor in \"L7 mode\" to monitor for HTTP requests being reported by Cilium: \n .. code-block:: shell-session \n $ kubectl exec -it -n kube-system cilium-d5x8v -- cilium-dbg monitor -t l7\n \n Next in the original window, from the  mediabot  pod we can access  httpbin.org  via HTTPS: \n .. code-block:: shell-session \n $ kubectl exec -it mediabot -- curl -sL 'https://httpbin.org/anything'\n...\n...\n\n$ kubectl exec -it mediabot -- curl -sL 'https://httpbin.org/headers'\n...\n...\n \n Looking back at the cilium-dbg monitor window, you will see each individual HTTP request and response.  For example:: \n -> Request http from 2585 ([k8s:class=mediabot k8s:org=empire k8s:io.kubernetes.pod.namespace=default k8s:io.cilium.k8s.policy.serviceaccount=default k8s:io.cilium.k8s.policy.cluster=default]) to 0 ([reserved:world]), identity 24948->2, verdict Forwarded GET https://httpbin.org/anything => 0\n-> Response http to 2585 ([k8s:io.kubernetes.pod.namespace=default k8s:io.cilium.k8s.policy.serviceaccount=default k8s:io.cilium.k8s.policy.cluster=default k8s:class=mediabot k8s:org=empire]) from 0 ([reserved:world]), identity 24948->2, verdict Forwarded GET https://httpbin.org/anything => 200\n \n Refer to :ref: l4_policy  and :ref: l7_policy  to learn more about Cilium L4 and L7 network policies. \n Clean-up \n .. parsed-literal:: \n $ kubectl delete -f \\ |SCM_WEB|/examples/kubernetes-dns/dns-sw-app.yaml\n$ kubectl delete cnp l7-visibility-tls\n$ kubectl delete secret -n kube-system tls-orig-data\n$ kubectl delete secret -n kube-system httpbin-tls-data",
  "item_type": "unknown",
  "module_path": "/tmp/cilium-repo/Documentation/security/tls-visibility.rst",
  "extracted_at": "2025-09-03T01:13:28.745626Z"
}