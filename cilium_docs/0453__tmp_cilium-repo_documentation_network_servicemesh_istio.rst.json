{
  "url": "file:///tmp/cilium-repo/Documentation/network/servicemesh/istio.rst",
  "content": ".. only:: not (epub or latex or html)\n\n    WARNING: You are looking at unreleased Cilium documentation.\n    Please use the official rendered version released here:\n    https://docs.cilium.io\n\n.. _gsg_istio:\n\n***********************\nIntegration with Istio\n***********************\n\nThis page helps you get started using Istio with a Cilium-enabled Kubernetes cluster.\nThis document covers the following common aspects of Cilium's integration with Istio:\n\n* Cilium configuration\n* Istio configuration\n* Demo application\n\n.. note::\n\n   You can run Cilium with Istio in two ways:\n\n   1. **With kube-proxy present (recommended):**\n\n      - Set ``kubeProxyReplacement: false`` (the default).\n      - Cilium does not fully replace kube-proxy; kube-proxy continues to handle ClusterIP routing.\n      - This is the recommended setup for using Istio with minimal disruption, particularly in sidecar or ambient mode.\n\n   2. **With kube-proxy removed (full replacement):**\n\n      - Set ``kubeProxyReplacement: true``, ``socketLB.hostNamespaceOnly: true``, and ``cni.exclusive: false``.\n      - These settings prevent Cilium’s socket-based load balancing from interfering with Istio’s proxying.\n      - kube-proxy can be removed in this mode, but these configurations are required to ensure compatibility.\n\n   In summary, you can run Istio with Cilium and kube-proxy by setting ``kubeProxyReplacement: false`` (the default and recommended for most Istio installs);\n   or you can run without kube-proxy by setting ``kubeProxyReplacement: true``, but you must carefully configure Cilium to avoid conflicts with Istio.\n\n\nCilium Configuration\n====================\n\nThe main goal of Cilium configuration is to ensure that traffic redirected to\nIstio's `sidecar proxies (sidecar mode) <https://istio.io/latest/docs/ops/deployment/architecture/>`_  or `node proxy (ambient mode) <https://istio.io/latest/docs/ops/ambient/architecture/>`_\nis not disrupted. Disruptions can happen when you enable Cilium's ``kubeProxyReplacement`` feature  (see :ref:`kubeproxy-free` docs),\nwhich enables socket based load balancing inside a Pod.\n\n\nTo ensure that Cilium does not interfere with Istio, it is important to set the\n``bpf-lb-sock-hostns-only`` parameter in the Cilium ConfigMap to ``true``. This can be achieved by using the\n``--set`` flag with the ``socketLB.hostNamespaceOnly`` Helm value set to ``true``.\nYou can confirm the result with the following command:\n\n.. code-block:: shell-session\n\n    $ kubectl get configmaps -n kube-system cilium-config -oyaml | grep bpf-lb-sock-hostns\n    bpf-lb-sock-hostns-only: \"true\"\n\n\nIstio uses a CNI plugin to implement functionality for both sidecar and ambient modes.\nTo ensure that Cilium does not interfere with other CNI plugins on the node, it is important to set the ``cni-exclusive``\nparameter in the Cilium ConfigMap to ``false``. This can be achieved by using the ``--set`` flag with the ``cni.exclusive``\nHelm value set to ``false``.\nYou can confirm the result with the following command:\n\n.. code-block:: shell-session\n\n    $ kubectl get configmaps -n kube-system cilium-config -oyaml | grep cni-exclusive\n    cni-exclusive: \"false\"\n\n.. _gsg_istio_cnp:\n\nIstio configuration\n===============================\n\nWhen you deploy Cilium and Istio together, be aware of:\n\n* Either Cilium or Istio L7 HTTP policy controls can be used, but it is not recommended to use **both** Cilium and Istio L7 HTTP policy\n  controls at the same time, to avoid split-brain problems.\n\n  In order to use Cilium L7 HTTP policy controls (for example, :ref:`l7_policy`) with Istio (sidecar or ambient modes), you must:\n\n  - Sidecar: Disable Istio mTLS for the workloads you wish to manage with Cilium L7 policy by configuring\n    ``mtls.mode=DISABLE`` under Istio's `PeerAuthentication <https://istio.io/latest/docs/reference/config/security/peer_authentication/#PeerAuthentication>`_.\n\n  - Ambient: Remove the workloads you wish to manage with Cilium L7 policy from Istio ambient by removing either the\n    ``istio.io/dataplane-mode`` label from the namespace,\n    or annotating the pods you wish to manage with Cilium L7 with ``ambient.istio.io/redirection: disabled``.\n\n  as otherwise the traffic between Istio-managed workloads will be encrypted by Istio with mTLS, and not accessible to Cilium for the purposes of L7 policy enforcement.\n\n  If using Istio L7 HTTP policy controls, policy will be managed in Istio and disabling mTLS between workloads is not required.\n\n* If using Istio mTLS in ambient mode with Istio L7 HTTP policy controls, traffic between ambient workloads will be\n  `encrypted and tunneled in and out of the pods by Istio over port 15008 <https://istio.io/latest/docs/ops/ambient/usage/traffic-redirection/>`_.\n  In this scenario, Cilium NetworkPolicy will still apply to the encrypted and tunneled L4 traffic entering and leaving the Istio-managed pods,\n  but Cilium will have no visibility into the actual source and destination of that tunneled and encrypted L4 traffic, or any L7 information.\n  This means that Istio should be used to enforce policy for traffic between Istio-managed, mTLS-secured workloads at L4 or above.\n  Traffic ingressing to Istio-managed workloads from non-Istio-managed workloads will continue to be fully subjected to Cilium-enforced Kubernetes NetworkPolicy,\n  as it would not be tunneled or encrypted.\n\n* When using Istio in sidecar mode with `automatic sidecar injection <https://istio.io/latest/docs/setup/additional-setup/sidecar-injection/#automatic-sidecar-injection>`_,\n  together with Cilium overlay mode (VXLAN or GENEVE), ``istiod`` pods must be running with ``hostNetwork: true`` in order to be reachable by the API server.\n\nDemo Application (Using Cilium with Istio ambient mode)\n=======================================================\n\nThe following guide demonstrates the interaction between Istio's ambient ``mTLS`` mode and\nCilium network policies when using Cilium L7 HTTP policy controls instead of Istio L7 HTTP policy controls, including the caveat described in the :ref:`gsg_istio_cnp` section.\n\nPrerequisites\n^^^^^^^^^^^^^\n\n* Istio is already installed on the local Kubernetes cluster.\n* Cilium is already installed with the ``socketLB.hostNamespaceOnly`` and ``cni.exclusive=false`` Helm values.\n* Istio's ``istioctl`` is installed on the local host.\n\nStart by deploying a set of web servers and client applications across three different namespaces:\n\n.. parsed-literal::\n\n    kubectl create ns red\n    kubectl label namespace red istio.io/dataplane-mode=ambient\n    kubectl -n red apply -f <(curl -s \\ |SCM_WEB|\\/examples/kubernetes-istio/httpbin.yaml)\n    kubectl -n red apply -f <(curl -s \\ |SCM_WEB|\\/examples/kubernetes-istio/netshoot.yaml)\n    kubectl create ns blue\n    kubectl label namespace blue istio.io/dataplane-mode=ambient\n    kubectl -n blue apply -f <(curl -s \\ |SCM_WEB|\\/examples/kubernetes-istio/httpbin.yaml)\n    kubectl -n blue apply -f <(curl -s \\ |SCM_WEB|\\/examples/kubernetes-istio/netshoot.yaml)\n    kubectl create ns green\n    kubectl -n green apply -f \\ |SCM_WEB|\\/examples/kubernetes-istio/netshoot.yaml\n\nBy default, Istio works in ``PERMISSIVE`` mode, allowing both Istio-ambient-managed and Istio-unmanaged pods\nto send and receive unsecured traffic between each other. You can test the connectivity between client and server applications deployed in the preceding example by entering the following commands:\n\n.. code-block:: shell-session\n\n    kubectl exec -n red deploy/netshoot -- curl http://httpbin.red/ip -s -o /dev/null -m 1 -w \"client 'red' to server 'red': %{http_code}\\n\"\n    kubectl exec -n blue deploy/netshoot -- curl http://httpbin.red/ip -s -o /dev/null -m 1 -w \"client 'blue' to server 'red': %{http_code}\\n\"\n    kubectl exec -n green deploy/netshoot -- curl http://httpbin.red/ip -s -o /dev/null -m 1 -w \"client 'green' to server 'red': %{http_code}\\n\"\n    kubectl exec -n red deploy/netshoot -- curl http://httpbin.blue/ip -s -o /dev/null -m 1 -w \"client 'red' to server 'blue': %{http_code}\\n\"\n    kubectl exec -n blue deploy/netshoot -- curl http://httpbin.blue/ip -s -o /dev/null -m 1 -w \"client 'blue' to server 'blue': %{http_code}\\n\"\n    kubectl exec -n green deploy/netshoot -- curl http://httpbin.blue/ip -s -o /dev/null -m 1 -w \"client 'green' to server 'blue': %{http_code}\\n\"\n\nAll commands should complete successfully:\n\n.. code-block:: shell-session\n\n    client 'red' to server 'red': 200\n    client 'blue' to server 'red': 200\n    client 'green' to server 'red': 200\n    client 'red' to server 'blue': 200\n    client 'blue' to server 'blue': 200\n    client 'green' to server 'blue': 200\n\nYou can apply Cilium-enforced L4 NetworkPolicy to restrict communication between namespaces.\nThe following command applies an L4 network policy that restricts communication\nin the ``blue`` namespace to clients located only in ``blue`` and ``red`` namespaces.\n\n.. parsed-literal::\n    kubectl -n blue apply -f \\ |SCM_WEB|\\/examples/kubernetes-istio/l4-policy.yaml\n\nRe-run the same connectivity checks to confirm the expected result:\n\n.. code-block:: shell-session\n\n    client 'red' to server 'red': 200\n    client 'blue' to server 'red': 200\n    client 'green' to server 'red': 200\n    client 'red' to server 'blue': 200\n    client 'blue' to server 'blue': 200\n    client 'green' to server 'blue': 000\n    command terminated with exit code 28\n\nYou can then decide to enhance the same network policy to perform additional HTTP-based checks.\nThe following command applies a Cilium L7 network policy allowing communication only with the ``/ip`` URL path:\n\n.. parsed-literal::\n    kubectl -n blue apply -f \\ |SCM_WEB|\\/examples/kubernetes-istio/l7-policy.yaml\n\nAt this point, all communication with the ``blue`` namespace is broken since the Cilium proxy (HTTP) interferes with Istio's mTLS-based HTTPS connections:\n\n.. code-block:: shell-session\n\n    client 'red' to server 'red': 200\n    client 'blue' to server 'red': 200\n    client 'green' to server 'red': 200\n    client 'red' to server 'blue': 000\n    command terminated with exit code 28\n    client 'blue' to server 'blue': 000\n    command terminated with exit code 28\n    client 'green' to server 'blue': 000\n    command terminated with exit code 28\n\nTo solve the problem and allow Cilium to manage L7 policy, you must remove the workloads or namespaces\nyou want Cilium to manage L7 policy for from the Istio ambient mesh:\n\n.. parsed-literal::\n\n    kubectl label namespace red istio.io/dataplane-mode-\n    kubectl label namespace blue istio.io/dataplane-mode-\n\nRe-run a connectivity check to confirm that communication with the ``blue`` namespaces has been restored.\nYou can verify that Cilium is enforcing the L7 network policy by accessing a different URL path, for example ``/deny``:\n\n.. code-block:: shell-session\n\n    $ kubectl exec -n red deploy/netshoot -- curl http://httpbin.blue/deny -s -o /dev/null -m 1 -w \"client 'red' to server 'blue': %{http_code}\\n\"\n    client 'red' to server 'blue': 403\n\nDemo Application (Istio sidecar mode)\n=====================================\n\nThe following guide demonstrates the interaction between Istio's sidecar-based ``mTLS`` mode and\nCilium network policies when using Cilium L7 HTTP policy controls instead of Istio L7 HTTP policy controls, including the caveat described in the :ref:`gsg_istio_cnp` section around disabling ``mTLS``\n\nPrerequisites\n^^^^^^^^^^^^^\n\n* Istio is already installed on the local Kubernetes cluster.\n* Cilium is already installed with the ``socketLB.hostNamespaceOnly`` and ``cni.exclusive=false`` Helm values.\n* Istio's ``istioctl`` is installed on the local host.\n\nStart by deploying a set of web servers and client applications across three different namespaces:\n\n.. parsed-literal::\n\n    kubectl create ns red\n    kubectl -n red apply -f <(curl -s \\ |SCM_WEB|\\/examples/kubernetes-istio/httpbin.yaml | istioctl kube-inject -f -)\n    kubectl -n red apply -f <(curl -s \\ |SCM_WEB|\\/examples/kubernetes-istio/netshoot.yaml | istioctl kube-inject -f -)\n    kubectl create ns blue\n    kubectl -n blue apply -f <(curl -s \\ |SCM_WEB|\\/examples/kubernetes-istio/httpbin.yaml | istioctl kube-inject -f -)\n    kubectl -n blue apply -f <(curl -s \\ |SCM_WEB|\\/examples/kubernetes-istio/netshoot.yaml | istioctl kube-inject -f -)\n    kubectl create ns green\n    kubectl -n green apply -f \\ |SCM_WEB|\\/examples/kubernetes-istio/netshoot.yaml\n\nBy default, Istio works in ``PERMISSIVE`` mode, allowing both Istio-managed and Pods without sidecars\nto send and receive traffic between each other. You can test the connectivity between client and server applications \ndeployed in the preceding example by entering the following commands:\n\n.. code-block:: shell-session\n\n    kubectl exec -n red deploy/netshoot -- curl http://httpbin.red/ip -s -o /dev/null -m 1 -w \"client 'red' to server 'red': %{http_code}\\n\"\n    kubectl exec -n blue deploy/netshoot -- curl http://httpbin.red/ip -s -o /dev/null -m 1 -w \"client 'blue' to server 'red': %{http_code}\\n\"\n    kubectl exec -n green deploy/netshoot -- curl http://httpbin.red/ip -s -o /dev/null -m 1 -w \"client 'green' to server 'red': %{http_code}\\n\"\n    kubectl exec -n red deploy/netshoot -- curl http://httpbin.blue/ip -s -o /dev/null -m 1 -w \"client 'red' to server 'blue': %{http_code}\\n\"\n    kubectl exec -n blue deploy/netshoot -- curl http://httpbin.blue/ip -s -o /dev/null -m 1 -w \"client 'blue' to server 'blue': %{http_code}\\n\"\n    kubectl exec -n green deploy/netshoot -- curl http://httpbin.blue/ip -s -o /dev/null -m 1 -w \"client 'green' to server 'blue': %{http_code}\\n\"\n\nAll commands should complete successfully:\n\n.. code-block:: shell-session\n\n    client 'red' to server 'red': 200\n    client 'blue' to server 'red': 200\n    client 'green' to server 'red': 200\n    client 'red' to server 'blue': 200\n    client 'blue' to server 'blue': 200\n    client 'green' to server 'blue': 200\n\nYou can apply network policies to restrict communication between namespaces. \nThe following command applies a Cilium-managed L4 network policy that restricts communication\nin the ``blue`` namespace to clients located only in ``blue`` and ``red`` namespaces.\n\n.. parsed-literal::\n    kubectl -n blue apply -f \\ |SCM_WEB|\\/examples/kubernetes-istio/l4-policy.yaml\n\nRe-run the same connectivity checks to confirm the expected result:\n\n.. code-block:: shell-session\n\n    client 'red' to server 'red': 200\n    client 'blue' to server 'red': 200\n    client 'green' to server 'red': 200\n    client 'red' to server 'blue': 200\n    client 'blue' to server 'blue': 200\n    client 'green' to server 'blue': 000\n    command terminated with exit code 28\n\nYou can then decide to enhance the L4 network policy to perform additional Cilium-managed HTTP-based checks.\nThe following command applies Cilium L7 network policy allowing communication only with the ``/ip`` URL path:\n\n.. parsed-literal::\n    kubectl -n blue apply -f \\ |SCM_WEB|\\/examples/kubernetes-istio/l7-policy.yaml\n\nAt this point, all communication with the ``blue`` namespace is broken since the Cilium proxy (HTTP) interferes with\nIstio's mTLS-based HTTPs connections:\n\n.. code-block:: shell-session\n\n    client 'red' to server 'red': 200\n    client 'blue' to server 'red': 200\n    client 'green' to server 'red': 200\n    client 'red' to server 'blue': 503\n    client 'blue' to server 'blue': 503\n    client 'green' to server 'blue': 000\n    command terminated with exit code 28\n\nTo solve the problem and allow Cilium to manage L7 policy, you must disable Istio's mTLS authentication by configuring a new policy:\n\n.. literalinclude:: ../../../examples/kubernetes-istio/authn.yaml\n     :language: yaml\n\nYou must apply this policy to the same namespace where you implement the HTTP-based network policy:\n\n.. parsed-literal::\n    kubectl -n blue apply -f \\ |SCM_WEB|\\/examples/kubernetes-istio/authn.yaml\n\nRe-run a connectivity check to confirm that communication with the ``blue`` namespaces has been restored. \nYou can verify that Cilium is enforcing the L7 network policy by accessing a different URL path, for example ``/deny``:\n\n.. code-block:: shell-session\n\n    $ kubectl exec -n red deploy/netshoot -- curl http://httpbin.blue/deny -s -o /dev/null -m 1 -w \"client 'red' to server 'blue': %{http_code}\\n\"\n    client 'red' to server 'blue': 403\n",
  "item_type": "unknown",
  "module_path": "/tmp/cilium-repo/Documentation/network/servicemesh/istio.rst",
  "extracted_at": "2025-09-03T01:13:29.147735Z"
}