{
  "url": "file:///tmp/cilium-repo/Documentation/reference-guides/bpf/progtypes.rst",
  "content": ".. only:: not (epub or latex or html) \n WARNING: You are looking at unreleased Cilium documentation.\nPlease use the official rendered version released here:\nhttps://docs.cilium.io\n \n .. _bpf_program: \n Program Types \n At the time of this writing, there are eighteen different BPF program types\navailable, two of the main types for networking are further explained in below\nsubsections, namely XDP BPF programs as well as tc BPF programs. Extensive\nusage examples for the two program types for LLVM, iproute2 or other tools\nare spread throughout the toolchain section and not covered here. Instead,\nthis section focuses on their architecture, concepts and use cases. \n XDP \n XDP stands for eXpress Data Path and provides a framework for BPF that enables\nhigh-performance programmable packet processing in the Linux kernel. It runs\nthe BPF program at the earliest possible point in software, namely at the moment\nthe network driver receives the packet. \n At this point in the fast-path the driver just picked up the packet from its\nreceive rings, without having done any expensive operations such as allocating\nan  skb  for pushing the packet further up the networking stack, without\nhaving pushed the packet into the GRO engine, etc. Thus, the XDP BPF program\nis executed at the earliest point when it becomes available to the CPU for\nprocessing. \n XDP works in concert with the Linux kernel and its infrastructure, meaning\nthe kernel is not bypassed as in various networking frameworks that operate\nin user space only. Keeping the packet in kernel space has several major\nadvantages: \n \n XDP is able to reuse all the upstream developed kernel networking drivers,\nuser space tooling, or even other available in-kernel infrastructure such\nas routing tables, sockets, etc in BPF helper calls itself. \n Residing in kernel space, XDP has the same security model as the rest of\nthe kernel for accessing hardware. \n There is no need for crossing kernel / user space boundaries since the\nprocessed packet already resides in the kernel and can therefore flexibly\nforward packets into other in-kernel entities like namespaces used by\ncontainers or the kernel's networking stack itself. This is particularly\nrelevant in times of Meltdown and Spectre. \n Punting packets from XDP to the kernel's robust, widely used and efficient\nTCP/IP stack is trivially possible, allows for full reuse and does not\nrequire maintaining a separate TCP/IP stack as with user space frameworks. \n The use of BPF allows for full programmability, keeping a stable ABI with\nthe same 'never-break-user-space' guarantees as with the kernel's system\ncall ABI and compared to modules it also provides safety measures thanks to\nthe BPF verifier that ensures the stability of the kernel's operation. \n XDP trivially allows for atomically swapping programs during runtime without\nany network traffic interruption or even kernel / system reboot. \n XDP allows for flexible structuring of workloads integrated into\nthe kernel. For example, it can operate in \"busy polling\" or \"interrupt\ndriven\" mode. Explicitly dedicating CPUs to XDP is not required. There\nare no special hardware requirements and it does not rely on hugepages. \n XDP does not require any third-party kernel modules or licensing. It is\na long-term architectural solution, a core part of the Linux kernel, and\ndeveloped by the kernel community. \n XDP is already enabled and shipped everywhere with major distributions\nrunning a kernel equivalent to 4.8 or higher and supports most major 10G\nor higher networking drivers. \n \n As a framework for running BPF in the driver, XDP additionally ensures that\npackets are laid out linearly and fit into a single DMA'ed page which is\nreadable and writable by the BPF program. XDP also ensures that additional\nheadroom of 256 bytes is available to the program for implementing custom\nencapsulation headers with the help of the  bpf_xdp_adjust_head()  BPF helper\nor adding custom metadata in front of the packet through  bpf_xdp_adjust_meta() . \n The framework contains XDP action codes further described in the section\nbelow which a BPF program can return in order to instruct the driver how\nto proceed with the packet, and it enables the possibility to atomically\nreplace BPF programs running at the XDP layer. XDP is tailored for\nhigh-performance by design. BPF allows to access the packet data through\n'direct packet access' which means that the program holds data pointers\ndirectly in registers, loads the content into registers, respectively\nwrites from there into the packet. \n The packet representation in XDP that is passed to the BPF program as\nthe BPF context looks as follows: \n .. code-block:: c \n struct xdp_buff {\n    void *data;\n    void *data_end;\n    void *data_meta;\n    void *data_hard_start;\n    struct xdp_rxq_info *rxq;\n};\n \n data  points to the start of the packet data in the page, and as the\nname suggests,  data_end  points to the end of the packet data. Since XDP\nallows for a headroom,  data_hard_start  points to the maximum possible\nheadroom start in the page, meaning, when the packet should be encapsulated,\nthen  data  is moved closer towards  data_hard_start  via  bpf_xdp_adjust_head() .\nThe same BPF helper function also allows for decapsulation in which case\n data  is moved further away from  data_hard_start . \n data_meta  initially points to the same location as  data  but\n bpf_xdp_adjust_meta()  is able to move the pointer towards  data_hard_start \nas well in order to provide room for custom metadata which is invisible to\nthe normal kernel networking stack but can be read by tc BPF programs since\nit is transferred from XDP to the  skb . Vice versa, it can remove or reduce\nthe size of the custom metadata through the same BPF helper function by\nmoving  data_meta  away from  data_hard_start  again.  data_meta  can\nalso be used solely for passing state between tail calls similarly to the\n skb->cb[]  control block case that is accessible in tc BPF programs. \n This gives the following relation respectively invariant for the  struct xdp_buff \npacket pointers:  data_hard_start  <=  data_meta  <=  data  <  data_end . \n The  rxq  field points to some additional per receive queue metadata which\nis populated at ring setup time (not at XDP runtime): \n .. code-block:: c \n struct xdp_rxq_info {\n    struct net_device *dev;\n    u32 queue_index;\n    u32 reg_state;\n} ____cacheline_aligned;\n \n The BPF program can retrieve  queue_index  as well as additional data\nfrom the netdevice itself such as  ifindex , etc. \n BPF program return codes \n After running the XDP BPF program, a verdict is returned from the program in\norder to tell the driver how to process the packet next. In the  linux/bpf.h \nsystem header file all available return verdicts are enumerated: \n .. code-block:: c \n enum xdp_action {\n    XDP_ABORTED = 0,\n    XDP_DROP,\n    XDP_PASS,\n    XDP_TX,\n    XDP_REDIRECT,\n};\n \n XDP_DROP  as the name suggests will drop the packet right at the driver\nlevel without wasting any further resources. This is in particular useful\nfor BPF programs implementing DDoS mitigation mechanisms or firewalling in\ngeneral. The  XDP_PASS  return code means that the packet is allowed to\nbe passed up to the kernel's networking stack. Meaning, the current CPU\nthat was processing this packet now allocates a  skb , populates it, and\npasses it onwards into the GRO engine. This would be equivalent to the\ndefault packet handling behavior without XDP. With  XDP_TX  the BPF program\nhas an efficient option to transmit the network packet out of the same NIC it\njust arrived on again. This is typically useful when few nodes are implementing,\nfor example, firewalling with subsequent load balancing in a cluster and\nthus, act as a hairpinned load balancer pushing the incoming packets back\ninto the switch after rewriting them in XDP BPF.  XDP_REDIRECT  is similar\nto  XDP_TX  in that it is able to transmit the XDP packet, but through\nanother NIC. Another option for the  XDP_REDIRECT  case is to redirect\ninto a BPF cpumap, meaning, the CPUs serving XDP on the NIC's receive queues\ncan continue to do so and push the packet for processing the upper kernel\nstack to a remote CPU. This is similar to  XDP_PASS , but with the ability\nthat the XDP BPF program can keep serving the incoming high load as opposed\nto temporarily spend work on the current packet for pushing into upper\nlayers. Last but not least,  XDP_ABORTED  which serves denoting an exception\nlike state from the program and has the same behavior as  XDP_DROP  only\nthat  XDP_ABORTED  passes the  trace_xdp_exception  tracepoint which\ncan be additionally monitored to detect misbehavior. \n Use cases for XDP \n Some of the main use cases for XDP are presented in this subsection. The\nlist is non-exhaustive and given the programmability and efficiency XDP\nand BPF enables, it can easily be adapted to solve very specific use\ncases. \n \n \n DDoS mitigation, firewalling \n One of the basic XDP BPF features is to tell the driver to drop a packet\nwith  XDP_DROP  at this early stage which allows for any kind of efficient\nnetwork policy enforcement with having an extremely low per-packet cost.\nThis is ideal in situations when needing to cope with any sort of DDoS\nattacks, but also more general allows to implement any sort of firewalling\npolicies with close to no overhead in BPF e.g. in either case as stand alone\nappliance (e.g. scrubbing 'clean' traffic through  XDP_TX ) or widely\ndeployed on nodes protecting end hosts themselves (via  XDP_PASS  or\ncpumap  XDP_REDIRECT  for good traffic). Offloaded XDP takes this even\none step further by moving the already small per-packet cost entirely\ninto the NIC with processing at line-rate. \n \n \n .. \n \n \n Forwarding and load-balancing \n Another major use case of XDP is packet forwarding and load-balancing\nthrough either  XDP_TX  or  XDP_REDIRECT  actions. The packet can\nbe arbitrarily mangled by the BPF program running in the XDP layer,\neven BPF helper functions are available for increasing or decreasing\nthe packet's headroom in order to arbitrarily encapsulate respectively\ndecapsulate the packet before sending it out again. With  XDP_TX \nhairpinned load-balancers can be implemented that push the packet out\nof the same networking device it originally arrived on, or with the\n XDP_REDIRECT  action it can be forwarded to another NIC for\ntransmission. The latter return code can also be used in combination\nwith BPF's cpumap to load-balance packets for passing up the local\nstack, but on remote, non-XDP processing CPUs. \n \n \n .. \n \n \n Pre-stack filtering / processing \n Besides policy enforcement, XDP can also be used for hardening the\nkernel's networking stack with the help of  XDP_DROP  case, meaning,\nit can drop irrelevant packets for a local node right at the earliest\npossible point before the networking stack sees them e.g. given we\nknow that a node only serves TCP traffic, any UDP, SCTP or other L4\ntraffic can be dropped right away. This has the advantage that packets\ndo not need to traverse various entities like GRO engine, the kernel's\nflow dissector and others before it can be determined to drop them and\nthus, this allows for reducing the kernel's attack surface. Thanks to\nXDP's early processing stage, this effectively 'pretends' to the kernel's\nnetworking stack that these packets have never been seen by the networking\ndevice. Additionally, if a potential bug in the stack's receive path\ngot uncovered and would cause a 'ping of death' like scenario, XDP can be\nutilized to drop such packets right away without having to reboot the\nkernel or restart any services. Due to the ability to atomically swap\nsuch programs to enforce a drop of bad packets, no network traffic is\neven interrupted on a host. \n Another use case for pre-stack processing is that given the kernel has not\nyet allocated an  skb  for the packet, the BPF program is free to modify\nthe packet and, again, have it 'pretend' to the stack that it was received\nby the networking device this way. This allows for cases such as having\ncustom packet mangling and encapsulation protocols where the packet can be\ndecapsulated prior to entering GRO aggregation in which GRO otherwise would\nnot be able to perform any sort of aggregation due to not being aware of\nthe custom protocol. XDP also allows to push metadata (non-packet data) in\nfront of the packet. This is 'invisible' to the normal kernel stack, can\nbe GRO aggregated (for matching metadata) and later on processed in\ncoordination with a tc ingress BPF program where it has the context of\na  skb  available for e.g. setting various skb fields. \n \n \n .. \n \n \n Flow sampling, monitoring \n XDP can also be used for cases such as packet monitoring, sampling or any\nother network analytics, for example, as part of an intermediate node in\nthe path or on end hosts in combination also with prior mentioned use cases.\nFor complex packet analysis, XDP provides a facility to efficiently push\nnetwork packets (truncated or with full payload) and custom metadata into\na fast lockless per CPU memory mapped ring buffer provided from the Linux\nperf infrastructure to a user space application. This also allows for\ncases where only a flow's initial data can be analyzed and once determined\nas good traffic having the monitoring bypassed. Thanks to the flexibility\nbrought by BPF, this allows for implementing any sort of custom monitoring\nor sampling. \n \n \n .. \n One example of XDP BPF production usage is Facebook's SHIV and Droplet\ninfrastructure which implements their L4 load-balancing and DDoS countermeasures.\nMigrating their production infrastructure away from netfilter's IPVS\n(IP Virtual Server) over to XDP BPF allowed for a 10x speedup compared\nto their previous IPVS setup. This was first presented at the netdev 2.1\nconference: \n \n Slides: https://netdevconf.info/2.1/slides/apr6/zhou-netdev-xdp-2017.pdf \n Video: https://youtu.be/YEU2ClcGqts \n \n Another example is the integration of XDP into Cloudflare's DDoS mitigation\npipeline, which originally was using cBPF instead of eBPF for attack signature\nmatching through iptables'  xt_bpf  module. Due to use of iptables this\ncaused severe performance problems under attack where a user space bypass\nsolution was deemed necessary but came with drawbacks as well such as needing\nto busy poll the NIC and expensive packet re-injection into the kernel's stack.\nThe migration over to eBPF and XDP combined best of both worlds by having\nhigh-performance programmable packet processing directly inside the kernel: \n \n Slides: https://netdevconf.info/2.1/slides/apr6/bertin_Netdev-XDP.pdf \n Video: https://youtu.be/7OuOukmuivg \n \n XDP operation modes \n XDP has three operation modes where 'native' XDP is the default mode. When\ntalked about XDP this mode is typically implied. \n \n \n Native XDP \n This is the default mode where the XDP BPF program is run directly out\nof the networking driver's early receive path. Most widespread used NICs\nfor 10G and higher support native XDP already. \n \n \n .. \n \n \n Offloaded XDP \n In the offloaded XDP mode the XDP BPF program is directly offloaded into\nthe NIC instead of being executed on the host CPU. Thus, the already\nextremely low per-packet cost is pushed off the host CPU entirely and\nexecuted on the NIC, providing even higher performance than running in\nnative XDP. This offload is typically implemented by SmartNICs\ncontaining multi-threaded, multicore flow processors where an in-kernel\nJIT compiler translates BPF into native instructions for the latter.\nDrivers supporting offloaded XDP usually also support native XDP for\ncases where some BPF helpers may not yet or only be available for the\nnative mode. \n \n \n .. \n \n \n Generic XDP \n For drivers not implementing native or offloaded XDP yet, the kernel\nprovides an option for generic XDP which does not require any driver\nchanges since run at a much later point out of the networking stack.\nThis setting is primarily targeted at developers who want to write and\ntest programs against the kernel's XDP API, and will not operate at the\nperformance rate of the native or offloaded modes. For XDP usage in a\nproduction environment either the native or offloaded mode is better\nsuited and the recommended way to run XDP. \n \n \n .. _xdp_drivers: \n Driver support \n Drivers supporting native XDP \n A list of drivers supporting native XDP can be found in the table below. The\ncorresponding network driver name of an interface can be determined as follows: \n .. code-block:: shell-session \n # ethtool -i eth0\ndriver: nfp\n[...]\n \n +------------------------+-------------------------+-------------+\n| Vendor                 | Driver                  | XDP Support |\n+========================+=========================+=============+\n| Amazon                 | ena                     | >= 5.6      |\n+------------------------+-------------------------+-------------+\n| Aquantia               | atlantic                | >= 5.19     |\n+------------------------+-------------------------+-------------+\n| Broadcom               | bnxt_en                 | >= 4.11     |\n+------------------------+-------------------------+-------------+\n| Cavium                 | thunderx                | >= 4.12     |\n+------------------------+-------------------------+-------------+\n| Engleder               | tsne                    | >= 6.3      |\n|                        | (TSN Express Path)      |             |\n+------------------------+-------------------------+-------------+\n| Freescale              | dpaa                    | >= 5.11     |\n|                        +-------------------------+-------------+\n|                        | dpaa2                   | >= 5.0      |\n|                        +-------------------------+-------------+\n|                        | enetc                   | >= 5.13     |\n|                        +-------------------------+-------------+\n|                        | fec_enet                | >= 6.2      |\n+------------------------+-------------------------+-------------+\n| Fungible               | fun                     | >= 5.18     |\n+------------------------+-------------------------+-------------+\n| Google                 | gve                     | >= 6.4      |\n+------------------------+-------------------------+-------------+\n| Intel                  | ice                     | >= 5.5      |\n|                        +-------------------------+-------------+\n|                        | igb                     | >= 5.10     |\n|                        +-------------------------+-------------+\n|                        | igc                     | >= 5.13     |\n|                        +-------------------------+-------------+\n|                        | i40e                    | >= 4.13     |\n|                        +-------------------------+-------------+\n|                        | ixgbe                   | >= 4.12     |\n|                        +-------------------------+-------------+\n|                        | ixgbevf                 | >= 4.17     |\n+------------------------+-------------------------+-------------+\n| Marvell                | mvneta                  | >= 5.5      |\n|                        +-------------------------+-------------+\n|                        | mvpp2                   | >= 5.9      |\n|                        +-------------------------+-------------+\n|                        | otx2                    | >= 5.16     |\n+------------------------+-------------------------+-------------+\n| Mediatek               | mtk                     | >= 6.0      |\n+------------------------+-------------------------+-------------+\n| Mellanox               | mlx4                    | >= 4.8      |\n|                        +-------------------------+-------------+\n|                        | mlx5                    | >= 4.9      |\n+------------------------+-------------------------+-------------+\n| Microchip              | lan966x                 | >= 6.2      |\n+------------------------+-------------------------+-------------+\n| Microsoft              | hv_netvsc (Hyper-V)     | >= 5.6      |\n|                        +-------------------------+-------------+\n|                        | mana                    | >= 5.17     |\n+------------------------+-------------------------+-------------+\n| Netronome              | nfp                     | >= 4.10     |\n+------------------------+-------------------------+-------------+\n| Others                 | bonding                 | >= 5.15     |\n|                        +-------------------------+-------------+\n|                        | netdevsim               | >= 4.16     |\n|                        +-------------------------+-------------+\n|                        | tun/tap                 | >= 4.14     |\n|                        +-------------------------+-------------+\n|                        | virtio_net              | >= 4.10     |\n|                        +-------------------------+-------------+\n|                        | xen-netfront            | >= 5.9      |\n|                        +-------------------------+-------------+\n|                        | veth                    | >= 4.19     |\n+------------------------+-------------------------+-------------+\n| QLogic                 | qede                    | >= 4.10     |\n+------------------------+-------------------------+-------------+\n| Socionext              | netsec                  | >= 5.3      |\n+------------------------+-------------------------+-------------+\n| Solarflare             | SFC Efx                 | >= 5.5      |\n+------------------------+-------------------------+-------------+\n| STMicro                | stmmac                  | >= 5.13     |\n+------------------------+-------------------------+-------------+\n| Texas Instruments      | cpsw                    | >= 5.3      |\n+------------------------+-------------------------+-------------+\n| VMware                 | vmxnet3                 | >= 6.6      |\n+------------------------+-------------------------+-------------+ \n Drivers supporting offloaded XDP \n \n \n Netronome \n \n nfp [2]_ \n \n \n \n .. note:: \n Examples for writing and loading XDP programs are included in the `bpf_dev` section under the respective tools.\n \n .. [2] Some BPF helper functions such as retrieving the current CPU number\nwill not be available in an offloaded setting. \n tc (traffic control) \n Aside from other program types such as XDP, BPF can also be used out of the\nkernel's tc (traffic control) layer in the networking data path. On a high-level\nthere are three major differences when comparing XDP BPF programs to tc BPF\nones: \n \n \n The BPF input context is a  sk_buff  not a  xdp_buff . When the kernel's\nnetworking stack receives a packet, after the XDP layer, it allocates a buffer\nand parses the packet to store metadata about the packet. This representation\nis known as the  sk_buff . This structure is then exposed in the BPF input\ncontext so that BPF programs from the tc ingress layer can use the metadata that\nthe stack extracts from the packet. This can be useful, but comes with an\nassociated cost of the stack performing this allocation and metadata extraction,\nand handling the packet until it hits the tc hook. By definition, the  xdp_buff \ndoesn't have access to this metadata because the XDP hook is called before\nthis work is done. This is a significant contributor to the performance\ndifference between the XDP and tc hooks. \n Therefore, BPF programs attached to the tc BPF hook can, for instance, read or\nwrite the skb's  mark ,  pkt_type ,  protocol ,  priority ,\n queue_mapping ,  napi_id ,  cb[]  array,  hash ,  tc_classid  or\n tc_index , vlan metadata, the XDP transferred custom metadata and various\nother information. All members of the  struct __sk_buff  BPF context used\nin tc BPF are defined in the  linux/bpf.h  system header. \n Generally, the  sk_buff  is of a completely different nature than\n xdp_buff  where both come with advantages and disadvantages. For example,\nthe  sk_buff  case has the advantage that it is rather straight forward to\nmangle its associated metadata, however, it also contains a lot of protocol\nspecific information (e.g. GSO related state) which makes it difficult to\nsimply switch protocols by solely rewriting the packet data. This is due to\nthe stack processing the packet based on the metadata rather than having the\ncost of accessing the packet contents each time. Thus, additional conversion\nis required from BPF helper functions taking care that  sk_buff  internals\nare properly converted as well. The  xdp_buff  case however does not\nface such issues since it comes at such an early stage where the kernel\nhas not even allocated an  sk_buff  yet, thus packet rewrites of any\nkind can be realized trivially. However, the  xdp_buff  case has the\ndisadvantage that  sk_buff  metadata is not available for mangling\nat this stage. The latter is overcome by passing custom metadata from\nXDP BPF to tc BPF, though. In this way, the limitations of each program\ntype can be overcome by operating complementary programs of both types\nas the use case requires. \n \n \n .. \n \n \n Compared to XDP, tc BPF programs can be triggered out of ingress and also\negress points in the networking data path as opposed to ingress only in\nthe case of XDP. \n The two hook points  sch_handle_ingress()  and  sch_handle_egress()  in\nthe kernel are triggered out of  __netif_receive_skb_core()  and\n __dev_queue_xmit() , respectively. The latter two are the main receive\nand transmit functions in the data path that, setting XDP aside, are triggered\nfor every network packet going in or coming out of the node allowing for\nfull visibility for tc BPF programs at these hook points. \n \n \n .. \n \n \n The tc BPF programs do not require any driver changes since they are run\nat hook points in generic layers in the networking stack. Therefore, they\ncan be attached to any type of networking device. \n While this provides flexibility, it also trades off performance compared\nto running at the native XDP layer. However, tc BPF programs still come\nat the earliest point in the generic kernel's networking data path after\nGRO has been run but  before  any protocol processing, traditional iptables\nfirewalling such as iptables PREROUTING or nftables ingress hooks or other\npacket processing takes place. Likewise on egress, tc BPF programs execute\nat the latest point before handing the packet to the driver itself for\ntransmission, meaning  after  traditional iptables firewalling hooks like\niptables POSTROUTING, but still before handing the packet to the kernel's\nGSO engine. \n One exception which does require driver changes however are offloaded tc\nBPF programs, typically provided by SmartNICs in a similar way as offloaded\nXDP just with differing set of features due to the differences in the BPF\ninput context, helper functions and verdict codes. \n \n \n .. \n BPF programs run in the tc layer are run from the  cls_bpf  classifier.\nWhile the tc terminology describes the BPF attachment point as a \"classifier\",\nthis is a bit misleading since it under-represents what  cls_bpf  is\ncapable of. That is to say, a fully programmable packet processor being able\nnot only to read the  skb  metadata and packet data, but to also arbitrarily\nmangle both and terminate the tc processing with an action verdict.  cls_bpf \ncan thus be regarded as a self-contained entity that manages and executes tc\nBPF programs. \n cls_bpf  can hold one or more tc BPF programs. In the case where Cilium\ndeploys  cls_bpf  programs, it attaches only a single program for a given hook\nin  direct-action  mode. Typically, in the traditional tc scheme, there is a\nsplit between classifier and action modules, where the classifier has one\nor more actions attached to it that are triggered once the classifier has a\nmatch. In the modern world for using tc in the software data path this model\ndoes not scale well for complex packet processing. Given tc BPF programs\nattached to  cls_bpf  are fully self-contained, they effectively fuse the\nparsing and action process together into a single unit. Thanks to  cls_bpf 's\n direct-action  mode, it will just return the tc action verdict and\nterminate the processing pipeline immediately. This allows for implementing\nscalable programmable packet processing in the networking data path by avoiding\nlinear iteration of actions.  cls_bpf  is the only such \"classifier\" module\nin the tc layer capable of such a fast-path. \n Like XDP BPF programs, tc BPF programs can be atomically updated at runtime\nvia  cls_bpf  without interrupting any network traffic or having to restart\nservices. \n Both the tc ingress and the egress hook where  cls_bpf  itself can be\nattached to is managed by a pseudo qdisc called  sch_clsact . This is a\ndrop-in replacement and proper superset of the ingress qdisc since it\nis able to manage both, ingress and egress tc hooks. For tc's egress hook\nin  __dev_queue_xmit()  it is important to stress that it is not executed\nunder the kernel's qdisc root lock. Thus, both tc ingress and egress hooks\nare executed in a lockless manner in the fast-path. In either case, preemption\nis disabled and execution happens under RCU read side. \n Typically, on egress there are qdiscs attached to netdevices such as  sch_mq ,\n sch_fq ,  sch_fq_codel  or  sch_htb  where some of them are classful\nqdiscs that contain subclasses and thus require a packet classification\nmechanism to determine a verdict where to demux the packet. This is handled\nby a call to  tcf_classify()  which calls into tc classifiers if present.\n cls_bpf  can also be attached and used in such cases. Such operation usually\nhappens under the qdisc root lock and can be subject to lock contention. The\n sch_clsact  qdisc's egress hook comes at a much earlier point however which\ndoes not fall under that and operates completely independent from conventional\negress qdiscs. Thus, for cases like  sch_htb  the  sch_clsact  qdisc could\nperform the heavy lifting packet classification through tc BPF outside of the\nqdisc root lock, setting the  skb->mark  or  skb->priority  from there such\nthat  sch_htb  only requires a flat mapping without expensive packet\nclassification under the root lock thus reducing contention. \n Offloaded tc BPF programs are supported for the case of  sch_clsact  in\ncombination with  cls_bpf  where the prior loaded BPF program was JITed\nfrom a SmartNIC driver to be run natively on the NIC. Only  cls_bpf \nprograms operating in  direct-action  mode are supported to be offloaded.\n cls_bpf  only supports offloading a single program and cannot offload\nmultiple programs. Furthermore, only the ingress hook supports offloading\nBPF programs. \n One  cls_bpf  instance is able to hold multiple tc BPF programs internally.\nIf this is the case, then the  TC_ACT_UNSPEC  program return code will\ncontinue execution with the next tc BPF program in that list. However, this\nhas the drawback that several programs would need to parse the packet over\nand over again resulting in degraded performance. \n BPF program return codes \n Both the tc ingress and egress hook share the same action return verdicts\nthat tc BPF programs can use. They are defined in the  linux/pkt_cls.h \nsystem header: \n .. code-block:: c \n #define TC_ACT_UNSPEC         (-1)\n#define TC_ACT_OK               0\n#define TC_ACT_SHOT             2\n#define TC_ACT_STOLEN           4\n#define TC_ACT_REDIRECT         7\n \n There are a few more action  TC_ACT_*  verdicts available in the system\nheader file which are also used in the two hooks. However, they share the\nsame semantics with the ones above. Meaning, from a tc BPF perspective,\n TC_ACT_OK  and  TC_ACT_RECLASSIFY  have the same semantics, as well as\nthe three  TC_ACT_STOLEN ,  TC_ACT_QUEUED  and  TC_ACT_TRAP  opcodes.\nTherefore, for these cases we only describe  TC_ACT_OK  and the  TC_ACT_STOLEN \nopcode for the two groups. \n Starting out with  TC_ACT_UNSPEC . It has the meaning of \"unspecified action\"\nand is used in three cases, i) when an offloaded tc BPF program is attached\nand the tc ingress hook is run where the  cls_bpf  representation for the\noffloaded program will return  TC_ACT_UNSPEC , ii) in order to continue\nwith the next tc BPF program in  cls_bpf  for the multi-program case. The\nlatter also works in combination with offloaded tc BPF programs from point i)\nwhere the  TC_ACT_UNSPEC  from there continues with a next tc BPF program\nsolely running in non-offloaded case. Last but not least, iii)  TC_ACT_UNSPEC \nis also used for the single program case to simply tell the kernel to continue\nwith the  skb  without additional side-effects.  TC_ACT_UNSPEC  is very\nsimilar to the  TC_ACT_OK  action code in the sense that both pass the\n skb  onwards either to upper layers of the stack on ingress or down to\nthe networking device driver for transmission on egress, respectively. The\nonly difference to  TC_ACT_OK  is that  TC_ACT_OK  sets  skb->tc_index \nbased on the classid the tc BPF program set. The latter is set out of the\ntc BPF program itself through  skb->tc_classid  from the BPF context. \n TC_ACT_SHOT  instructs the kernel to drop the packet, meaning, upper\nlayers of the networking stack will never see the  skb  on ingress and\nsimilarly, the packet will never be submitted for transmission on egress.\n TC_ACT_SHOT  and  TC_ACT_STOLEN  are both similar in nature with few\ndifferences:  TC_ACT_SHOT  will indicate to the kernel that the  skb \nwas released through  kfree_skb()  and return  NET_XMIT_DROP  to the\ncallers for immediate feedback, whereas  TC_ACT_STOLEN  will release\nthe  skb  through  consume_skb()  and pretend to upper layers that\nthe transmission was successful through  NET_XMIT_SUCCESS . The perf's\ndrop monitor which records traces of  kfree_skb()  will therefore\nalso not see any drop indications from  TC_ACT_STOLEN  since its\nsemantics are such that the  skb  has been \"consumed\" or queued but\ncertainly not \"dropped\". \n Last but not least the  TC_ACT_REDIRECT  action which is available for\ntc BPF programs as well. This allows to redirect the  skb  to the same\nor another's device ingress or egress path together with the  bpf_redirect() \nhelper. Being able to inject the packet into another device's ingress or\negress direction allows for full flexibility in packet forwarding with\nBPF. There are no requirements on the target networking device other than\nbeing a networking device itself, there is no need to run another instance\nof  cls_bpf  on the target device or other such restrictions. \n tc BPF FAQ \n This section contains a few miscellaneous question and answer pairs related to\ntc BPF programs that are asked from time to time. \n \n Question:  What about  act_bpf  as a tc action module, is it still relevant? \n Answer:  Not really. Although  cls_bpf  and  act_bpf  share the same\nfunctionality for tc BPF programs,  cls_bpf  is more flexible since it is a\nproper superset of  act_bpf . The way tc works is that tc actions need to be\nattached to tc classifiers. In order to achieve the same flexibility as  cls_bpf ,\n act_bpf  would need to be attached to the  cls_matchall  classifier. As the\nname says, this will match on every packet in order to pass them through for attached\ntc action processing. For  act_bpf , this is will result in less efficient packet\nprocessing than using  cls_bpf  in  direct-action  mode directly. If  act_bpf \nis used in a setting with other classifiers than  cls_bpf  or  cls_matchall \nthen this will perform even worse due to the nature of operation of tc classifiers.\nMeaning, if classifier A has a mismatch, then the packet is passed to classifier\nB, reparsing the packet, etc, thus in the typical case there will be linear\nprocessing where the packet would need to traverse N classifiers in the worst\ncase to find a match and execute  act_bpf  on that. Therefore,  act_bpf  has\nnever been largely relevant. Additionally,  act_bpf  does not provide a tc\noffloading interface either compared to  cls_bpf . \n \n .. \n \n Question:  Is it recommended to use  cls_bpf  not in  direct-action  mode? \n Answer:  No. The answer is similar to the one above in that this is otherwise\nunable to scale for more complex processing. tc BPF can already do everything needed\nby itself in an efficient manner and thus there is no need for anything other than\n direct-action  mode. \n \n .. \n \n Question:  Is there any performance difference in offloaded  cls_bpf  and\noffloaded XDP? \n Answer:  No. Both are JITed through the same compiler in the kernel which\nhandles the offloading to the SmartNIC and the loading mechanism for both is\nvery similar as well. Thus, the BPF program gets translated into the same target\ninstruction set in order to be able to run on the NIC natively. The two tc BPF\nand XDP BPF program types have a differing set of features, so depending on the\nuse case one might be picked over the other due to availability of certain helper\nfunctions in the offload case, for example. \n \n Use cases for tc BPF \n Some of the main use cases for tc BPF programs are presented in this subsection.\nAlso here, the list is non-exhaustive and given the programmability and efficiency\nof tc BPF, it can easily be tailored and integrated into orchestration systems\nin order to solve very specific use cases. While some use cases with XDP may overlap,\ntc BPF and XDP BPF are mostly complementary to each other and both can also be\nused at the same time or one over the other depending on which is most suitable for a\ngiven problem to solve. \n \n \n Policy enforcement for containers \n One application which tc BPF programs are suitable for is to implement policy\nenforcement, custom firewalling or similar security measures for containers or\npods, respectively. In the conventional case, container isolation is implemented\nthrough network namespaces with veth networking devices connecting the host's\ninitial namespace with the dedicated container's namespace. Since one end of\nthe veth pair has been moved into the container's namespace whereas the other\nend remains in the initial namespace of the host, all network traffic from the\ncontainer has to pass through the host-facing veth device allowing for attaching\ntc BPF programs on the tc ingress and egress hook of the veth. Network traffic\ngoing into the container will pass through the host-facing veth's tc egress\nhook whereas network traffic coming from the container will pass through the\nhost-facing veth's tc ingress hook. \n For virtual devices like veth devices XDP is unsuitable in this case since the\nkernel operates solely on a  skb  here and generic XDP has a few limitations\nwhere it does not operate with cloned  skb 's. The latter is heavily used\nfrom the TCP/IP stack in order to hold data segments for retransmission where\nthe generic XDP hook would simply get bypassed instead. Moreover, generic XDP\nneeds to linearize the entire  skb  resulting in heavily degraded performance.\ntc BPF on the other hand is more flexible as it specializes on the  skb \ninput context case and thus does not need to cope with the limitations from\ngeneric XDP. \n \n \n .. \n \n \n Forwarding and load-balancing \n The forwarding and load-balancing use case is quite similar to XDP, although\nslightly more targeted towards east-west container workloads rather than\nnorth-south traffic (though both technologies can be used in either case).\nSince XDP is only available on ingress side, tc BPF programs allow for\nfurther use cases that apply in particular on egress, for example, container\nbased traffic can already be NATed and load-balanced on the egress side\nthrough BPF out of the initial namespace such that this is done transparent\nto the container itself. Egress traffic is already based on the  sk_buff \nstructure due to the nature of the kernel's networking stack, so packet\nrewrites and redirects are suitable out of tc BPF. By utilizing the\n bpf_redirect()  helper function, BPF can take over the forwarding logic\nto push the packet either into the ingress or egress path of another networking\ndevice. Thus, any bridge-like devices become unnecessary to use as well by\nutilizing tc BPF as forwarding fabric. \n \n \n .. \n \n \n Flow sampling, monitoring \n Like in XDP case, flow sampling and monitoring can be realized through a\nhigh-performance lockless per-CPU memory mapped perf ring buffer where the\nBPF program is able to push custom data, the full or truncated packet\ncontents, or both up to a user space application. From the tc BPF program\nthis is realized through the  bpf_skb_event_output()  BPF helper function\nwhich has the same function signature and semantics as  bpf_xdp_event_output() .\nGiven tc BPF programs can be attached to ingress and egress as opposed to\nonly ingress in XDP BPF case plus the two tc hooks are at the lowest layer\nin the (generic) networking stack, this allows for bidirectional monitoring\nof all network traffic from a particular node. This might be somewhat related\nto the cBPF case which tcpdump and Wireshark makes use of, though, without\nhaving to clone the  skb  and with being a lot more flexible in terms of\nprogrammability where, for example, BPF can already perform in-kernel\naggregation rather than pushing everything up to user space as well as\ncustom annotations for packets pushed into the ring buffer. The latter is\nalso heavily used in Cilium where packet drops can be further annotated\nto correlate container labels and reasons for why a given packet had to\nbe dropped (such as due to policy violation) in order to provide a richer\ncontext. \n \n \n .. \n \n \n Packet scheduler pre-processing \n The  sch_clsact 's egress hook which is called  sch_handle_egress() \nruns right before taking the kernel's qdisc root lock, thus tc BPF programs\ncan be utilized to perform all the heavy lifting packet classification\nand mangling before the packet is transmitted into a real full blown\nqdisc such as  sch_htb . This type of interaction of  sch_clsact \nwith a real qdisc like  sch_htb  coming later in the transmission phase\nallows to reduce the lock contention on transmission since  sch_clsact 's\negress hook is executed without taking locks. \n \n \n .. \n One concrete example user of tc BPF but also XDP BPF programs is Cilium.\nCilium is open source software for transparently securing the network\nconnectivity between application services deployed using Linux container\nmanagement platforms like Docker and Kubernetes and operates at Layer 3/4\nas well as Layer 7. At the heart of Cilium operates BPF in order to\nimplement the policy enforcement as well as load balancing and monitoring. \n \n Slides: https://www.slideshare.net/ThomasGraf5/dockercon-2017-cilium-network-and-application-security-with-bpf-and-xdp \n Video: https://youtu.be/ilKlmTDdFgk \n Github: https://github.com/cilium/cilium \n \n Driver support \n Since tc BPF programs are triggered from the kernel's networking stack\nand not directly out of the driver, they do not require any extra driver\nmodification and therefore can run on any networking device. The only\nexception listed below is for offloading tc BPF programs to the NIC. \n Drivers supporting offloaded tc BPF \n \n \n Netronome \n \n nfp [2]_ \n \n \n \n .. note:: \n Examples for writing and loading tc BPF programs are included in the `bpf_dev` section under the respective tools.",
  "item_type": "unknown",
  "module_path": "/tmp/cilium-repo/Documentation/reference-guides/bpf/progtypes.rst",
  "extracted_at": "2025-09-03T00:53:44.748255Z"
}