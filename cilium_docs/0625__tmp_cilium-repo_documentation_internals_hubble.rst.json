{
  "url": "file:///tmp/cilium-repo/Documentation/internals/hubble.rst",
  "content": ".. only:: not (epub or latex or html)\n\n    WARNING: You are looking at unreleased Cilium documentation.\n    Please use the official rendered version released here:\n    https://docs.cilium.io\n\n.. _hubble_internals:\n\n****************\nHubble internals\n****************\n\n.. note:: This documentation section is targeted at developers who are\n          interested in contributing to Hubble. For this purpose, it describes\n          Hubble internals.\n\n.. note:: This documentation covers the Hubble server (sometimes referred as\n          \"Hubble embedded\") and Hubble Relay components but does not cover the\n          Hubble UI and CLI.\n\nHubble builds on top of Cilium and eBPF to enable deep visibility into the\ncommunication and behavior of services as well as the networking infrastructure\nin a completely transparent manner. One of the design goals of Hubble is to\nachieve all of this at large scale.\n\nHubble's server component is embedded into the Cilium agent in order to achieve\nhigh performance with low-overhead. The gRPC services offered by Hubble server\nmay be consumed locally via a Unix domain socket or, more typically, through\nHubble Relay. Hubble Relay is a standalone component which is aware of all\nHubble instances and offers full cluster visibility by connecting to their\nrespective gRPC APIs. This capability is usually referred to as multi-node.\nHubble Relay's main goal is to offer a rich API that can be safely exposed and\nconsumed by the Hubble UI and CLI.\n\nHubble Architecture\n===================\n\nHubble exposes gRPC services from the Cilium process that allows clients to\nreceive flows and other type of data.\n\nHubble server\n-------------\n\nThe Hubble server component implements two gRPC services. The **Observer\nservice** which may optionally be exposed via a TCP socket in addition to a\nlocal Unix domain socket and the **Peer service**, which is served on both\nas well as being exposed as a Kubernetes Service when enabled via TCP.\n\nThe Observer service\n^^^^^^^^^^^^^^^^^^^^\n\nThe Observer service is the principal service. It provides four RPC endpoints:\n``GetFlows``, ``GetNodes``, ``GetNamespaces``  and ``ServerStatus``.\n\n* ``GetNodes`` returns a list of metrics and other information related to each Hubble instance.\n* ``ServerStatus`` returns a summary of the information in ``GetNodes``.\n* ``GetNamespaces`` returns a list of namespaces that had network flows within the last one hour.\n* ``GetFlows`` returns a stream of flow related events.\n\nUsing ``GetFlows``, callers get a stream of payloads. Request parameters allow\ncallers to specify filters in the form of allow lists and deny lists to provide\nfine-grained filtering of data. When multiple flow filters are provided, only\none of them has to match for a flow to be included/excluded. When both allow and\ndeny filters are specified, the result will contain all flows matched by the allow\nlist that are not also simultaneously matched by the deny list.\n\nIn order to answer ``GetFlows`` requests, Hubble stores monitoring events from\nCilium's event monitor into a user-space ring buffer structure. Monitoring\nevents are obtained by registering a new listener on Cilium monitor. The\nring buffer is capable of storing a configurable amount of events in memory.\nEvents are continuously consumed, overriding older ones once the ring buffer is\nfull.\n\nAdditionally, the Observer service also provides the ``GetAgentEvents`` and\n``GetDebugEvents`` RPC endpoints to expose data about the Cilium agent events\nand Cilium datapath debug events, respectively. Both are similar to ``GetFlows``\nexcept they do not implement filtering capabilities.\n\n.. image:: ./../images/hubble_getflows.png\n\nFor efficiency, the internal buffer length is a bit mask of ones + 1. The most\nsignificant bit of this bit mask is the same position of the most significant\nbit position of 'n'. In other terms, the internal buffer size is always a power\nof 2 with 1 slot reserved for the writer. In effect, from a user perspective,\nthe ring buffer capacity is one less than a power of 2. As the ring buffer is a\nhot code path, it has been designed to not employ any locking mechanisms and\nuses atomic operations instead. While this approach has performance benefits,\nit also has the downsides of being a complex component.\n\nDue to its complex nature, the ring buffer is typically accessed via a ring\nreader that abstracts the complexity of this data structure for reading. The\nring reader allows reading one event at the time with 'previous' and 'next'\nmethods but also implements a follow mode where events are continuously read as\nthey are written to the ring buffer.\n\nThe Peer service\n^^^^^^^^^^^^^^^^\n\nThe Peer service sends information about Hubble peers in the cluster in a\nstream. When the ``Notify`` method is called, it reports information about all\nthe peers in the cluster and subsequently sends information about peers that\nare updated, added, or removed from the cluster. Thus, it allows the caller to\nkeep track of all Hubble instances and query their respective gRPC services.\n\nThis service is exposed as a Kubernetes Service and is primarily used by Hubble\nRelay in order to have a cluster-wide view of all Hubble instances.\n\nThe Peer service obtains peer change notifications by subscribing to Cilium's\nnode manager. To this end, it internally defines a handler that implements\nCilium's datapath node handler interface.\n\n.. _hubble_relay:\n\nHubble Relay\n------------\n\nHubble Relay is the Hubble component that brings multi-node support. It\nleverages the Peer service to obtain information about Hubble instances and\nconsume their gRPC API in order to provide a more rich API that covers events\nfrom across the entire cluster (or even multiple clusters in a ClusterMesh\nscenario).\n\nHubble Relay was first introduced as a technology preview with the release of\nCilium v1.8 and was declared stable with the release of Cilium v1.9.\n\nHubble Relay implements the Observer service for multi-node. To that end, it\nmaintains a persistent connection with every Hubble peer in a cluster with a\npeer manager. This component provides callers with the list of peers. Callers\nmay report when a peer is unreachable, in which case the peer manager will\nattempt to reconnect.\n\nAs Hubble Relay connects to every node in a cluster, the Hubble server\ninstances must make their API available (by default on port 4244). By default,\nHubble server endpoints are secured using mutual TLS (mTLS) when exposed on a\nTCP port in order to limit access to Hubble Relay only.\n",
  "item_type": "unknown",
  "module_path": "/tmp/cilium-repo/Documentation/internals/hubble.rst",
  "extracted_at": "2025-09-03T01:13:29.361234Z"
}