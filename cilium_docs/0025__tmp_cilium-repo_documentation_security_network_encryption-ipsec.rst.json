{
  "url": "file:///tmp/cilium-repo/Documentation/security/network/encryption-ipsec.rst",
  "content": ".. only:: not (epub or latex or html)\n\n    WARNING: You are looking at unreleased Cilium documentation.\n    Please use the official rendered version released here:\n    https://docs.cilium.io\n\n.. _encryption_ipsec:\n\n****************************\nIPsec Transparent Encryption\n****************************\n\nThis guide explains how to configure Cilium to use IPsec based transparent\nencryption using Kubernetes secrets to distribute the IPsec keys. After this\nconfiguration is complete, all traffic between Cilium-managed endpoints will be\nencrypted using IPsec. This guide uses Kubernetes secrets to distribute keys.\nAlternatively, keys may be manually distributed, but that is not shown here.\n\nPackets are not encrypted when they are destined to the same node from which\nthey were sent. This behavior is intended. Encryption would provide no benefits\nin that case, given that the raw traffic can be observed on the node anyway.\n\nv1.18 Encrypted Overlay\n=========================\nPrior to v1.18, IPsec encryption was performed before tunnel encapsulation.\nFrom Cilium v1.18 and forward, Cilium's IPsec encryption datapath will send\ntraffic for overlay encapsulation prior to IPsec encryption when tunnel mode is\nenabled.\n\nWith this change, the security identities used for policy enforcement are\nencrypted on the wire. This is a security benefit.\n\nA disruption-less upgrade from v1.17 to v1.18 can only be achieved by fully\npatching v1.17 to its latest version. Migration specific code was added to\nnewer v1.17 releases to support a disruption-less upgrade to v1.18.\n\nOnce patched to the newest v1.17 stable release, a normal upgrade to v1.18 can\nbe performed.\n\n.. note::\n\n   Because VXLAN is encrypted before being sent, operators see ESP\n   traffic between Kubernetes nodes.\n\n   This may result in the need to update firewall rules to allow ESP traffic\n   between nodes.\n   This is especially important in Google Cloud GKE environments.\n   The default firewall rules for the cluster's subnet may not allow ESP.\n\n\nGenerate & Import the PSK\n=========================\n\nFirst, create a Kubernetes secret for the IPsec configuration to be stored. The\nexample below demonstrates generation of the necessary IPsec configuration\nwhich will be distributed as a Kubernetes secret called ``cilium-ipsec-keys``.\nA Kubernetes secret should consist of one key-value pair where the key is the\nname of the file to be mounted as a volume in cilium-agent pods, and the\nvalue is an IPsec configuration in the following format::\n\n    key-id encryption-algorithms PSK-in-hex-format key-size\n\n.. note::\n\n    ``Secret`` resources need to be deployed in the same namespace as Cilium!\n    In our example, we use ``kube-system``.\n\nIn the example below, GCM-128-AES is used. However, any of the algorithms\nsupported by Linux may be used. To generate the secret, you may use the\nfollowing command:\n\n.. tabs::\n\n    .. group-tab:: Cilium CLI\n\n       .. parsed-literal::\n\n          $ cilium encrypt create-key --auth-algo rfc4106-gcm-aes\n\n    .. group-tab:: Kubectl CLI\n\n       .. parsed-literal::\n\n          $ kubectl create -n kube-system secret generic cilium-ipsec-keys \\\n              --from-literal=keys=\"3+ rfc4106(gcm(aes)) $(dd if=/dev/urandom count=20 bs=1 2> /dev/null | xxd -p -c 64) 128\"\n\n       .. attention::\n\n           The ``+`` sign in the secret is strongly recommended. It will force the use\n           of per-tunnel IPsec keys. The former global IPsec keys are considered\n           insecure (cf. `GHSA-pwqm-x5x6-5586`_) and were deprecated in v1.16. When\n           using ``+``, the per-tunnel keys will be derived from the secret you\n           generated.\n\n.. _GHSA-pwqm-x5x6-5586: https://github.com/cilium/cilium/security/advisories/GHSA-pwqm-x5x6-5586\n\nThe secret can be seen with ``kubectl -n kube-system get secrets`` and will be\nlisted as ``cilium-ipsec-keys``.\n\n.. code-block:: shell-session\n\n    $ kubectl -n kube-system get secrets cilium-ipsec-keys\n    NAME                TYPE     DATA   AGE\n    cilium-ipsec-keys   Opaque   1      176m\n\nEnable Encryption in Cilium\n===========================\n\n.. tabs::\n\n    .. group-tab:: Cilium CLI\n\n       If you are deploying Cilium with the Cilium CLI, pass the following\n       options:\n\n       .. parsed-literal::\n\n          cilium install |CHART_VERSION| \\\n             --set encryption.enabled=true \\\n             --set encryption.type=ipsec\n\n    .. group-tab:: Helm\n\n       If you are deploying Cilium with Helm by following\n       :ref:`k8s_install_helm`, pass the following options:\n\n       .. parsed-literal::\n\n           helm install cilium |CHART_RELEASE| \\\\\n             --namespace kube-system \\\\\n             --set encryption.enabled=true \\\\\n             --set encryption.type=ipsec\n\n       ``encryption.enabled`` enables encryption of the traffic between\n       Cilium-managed pods. ``encryption.type`` specifies the encryption method\n       and can be omitted as it defaults to ``ipsec``.\n\n.. attention::\n\n   When using Cilium in any direct routing configuration, ensure that the\n   native routing CIDR is set properly. This is done using\n   ``--ipv4-native-routing-cidr=CIDR`` with the CLI or ``--set\n   ipv4NativeRoutingCIDR=CIDR`` with Helm.\n\nAt this point the Cilium managed nodes will be using IPsec for all traffic. For further\ninformation on Cilium's transparent encryption, see :ref:`ebpf_datapath`.\n\nDependencies\n============\n\nWhen L7 proxy support is enabled (``--enable-l7-proxy=true``), IPsec requires that the\nDNS proxy operates in transparent mode (``--dnsproxy-enable-transparent-mode=true``).\n\nEncryption interface\n--------------------\n\nAn additional argument can be used to identify the network-facing interface.\nIf direct routing is used and no interface is specified, the default route\nlink is chosen by inspecting the routing tables. This will work in many cases,\nbut depending on routing rules, users may need to specify the encryption\ninterface as follows:\n\n.. tabs::\n\n    .. group-tab:: Cilium CLI\n\n       .. parsed-literal::\n\n          cilium install |CHART_VERSION| \\\n             --set encryption.enabled=true \\\n             --set encryption.type=ipsec \\\n             --set encryption.ipsec.interface=ethX\n\n    .. group-tab:: Helm\n\n       .. code-block:: shell-session\n\n           --set encryption.ipsec.interface=ethX\n\nValidate the Setup\n==================\n\nRun a ``bash`` shell in one of the Cilium pods with\n``kubectl -n kube-system exec -ti ds/cilium -- bash`` and execute the following\ncommands:\n\n1. Install tcpdump\n\n   .. code-block:: shell-session\n\n       $ apt-get update\n       $ apt-get -y install tcpdump\n\n2. Check that traffic is encrypted. In the example below, this can be verified\n   by the fact that packets carry the IP Encapsulating Security Payload (ESP).\n   In the example below, ``eth0`` is the interface used for pod-to-pod\n   communication. Replace this interface with e.g. ``cilium_vxlan`` if\n   tunneling is enabled.\n\n   .. code-block:: shell-session\n\n       tcpdump -l -n -i eth0 esp\n       tcpdump: verbose output suppressed, use -v or -vv for full protocol decode\n       listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes\n       15:16:21.626416 IP 10.60.1.1 > 10.60.0.1: ESP(spi=0x00000001,seq=0x57e2), length 180\n       15:16:21.626473 IP 10.60.1.1 > 10.60.0.1: ESP(spi=0x00000001,seq=0x57e3), length 180\n       15:16:21.627167 IP 10.60.0.1 > 10.60.1.1: ESP(spi=0x00000001,seq=0x579d), length 100\n       15:16:21.627296 IP 10.60.0.1 > 10.60.1.1: ESP(spi=0x00000001,seq=0x579e), length 100\n       15:16:21.627523 IP 10.60.0.1 > 10.60.1.1: ESP(spi=0x00000001,seq=0x579f), length 180\n       15:16:21.627699 IP 10.60.1.1 > 10.60.0.1: ESP(spi=0x00000001,seq=0x57e4), length 100\n       15:16:21.628408 IP 10.60.1.1 > 10.60.0.1: ESP(spi=0x00000001,seq=0x57e5), length 100\n\n.. _ipsec_key_rotation:\n\nKey Rotation\n============\n\n.. attention::\n\n   Key rotations should not be performed during upgrades and downgrades. That\n   is, all nodes in the cluster (or clustermesh) should be on the same Cilium\n   version before rotating keys.\n\n.. attention::\n\n   It is not recommended to change algorithms that involve different authentication\n   key lengths during key rotations. If this is attempted, Cilium will delay the\n   application of the new key until the agent restarts and will continue using the\n   previous key. This is designed to maintain uninterrupted IPv6 pod-to-pod connectivity.\n\nTo replace cilium-ipsec-keys secret with a new key:\n\n.. code-block:: shell-session\n\n    KEYID=$(kubectl get secret -n kube-system cilium-ipsec-keys -o go-template --template={{.data.keys}} | base64 -d | grep -oP \"^\\d+\")\n    if [[ $KEYID -ge 15 ]]; then KEYID=0; fi\n    data=$(echo \"{\\\"stringData\\\":{\\\"keys\\\":\\\"$((($KEYID+1)))+ \"rfc4106\\(gcm\\(aes\\)\\)\" $(dd if=/dev/urandom count=20 bs=1 2> /dev/null | xxd -p -c 64) 128\\\"}}\")\n    kubectl patch secret -n kube-system cilium-ipsec-keys -p=\"${data}\" -v=1\n\nDuring transition the new and old keys will be in use. The Cilium agent keeps\nper endpoint data on which key is used by each endpoint and will use the correct\nkey if either side has not yet been updated. In this way encryption will work as\nnew keys are rolled out.\n\nThe ``KEYID`` environment variable in the above example stores the current key\nID used by Cilium. The key variable is a uint8 with value between 1 and 15\nincluded and should be monotonically increasing every re-key with a rollover\nfrom 15 to 1. The Cilium agent will default to ``KEYID`` of zero if its not\nspecified in the secret.\n\nIf you are using Cluster Mesh, you must apply the key rotation procedure\nto all clusters in the mesh. You might need to increase the transition time to\nallow for the new keys to be deployed and applied across all clusters,\nwhich you can do with the agent flag ``ipsec-key-rotation-duration``.\n\nMonitoring\n==========\n\nWhen monitoring network traffic on a node with IPSec enabled, it is normal to observe\nin the same interface both the outer packet (node-to-node) carrying the ESP-encrypted\npayload and then the decrypted inner packet (pod-to-pod). This occurs as, once a packet\nis decrypted, it is recirculated back to the same interface for further processing.\nTherefore, depending on the ``tcpdump`` filter applied, the capture might differ, but this\n**does not** indicate that encryption is not functioning correctly. In particular, to observe:\n\n1. Only the encrypted packet: use the filter ``esp``.\n2. Only the decrypted packet: use a specific filter for the protocol used by the pods (such as ``icmp`` for ping).\n3. Both encrypted and decrypted packets: use no filter or combine the filters for both (such as ``esp or icmp``).\n\nThe following capture was taken on a Kind cluster with no filter applied (replace ``eth0``\nwith ``cilium_vxlan`` if tunneling is enabled). The nodes have IP addresses ``10.244.2.92``\nand ``10.244.1.148``, while the pods have IP addresses ``10.244.2.189`` and ``10.244.1.7``,\nusing ping (ICMP) for communication.\n\n.. code-block:: shell-session\n\n  tcpdump -l -n -i eth0\n  tcpdump: verbose output suppressed, use -v[v]... for full protocol decode\n  listening on cilium_vxlan, link-type EN10MB (Ethernet), snapshot length 262144 bytes\n  09:22:16.379908 IP 10.244.2.92 > 10.244.1.148: ESP(spi=0x00000003,seq=0x8), length 120\n  09:22:16.379908 IP 10.244.2.189 > 10.244.1.7: ICMP echo request, id 33, seq 1, length 64\n\n\nTroubleshooting\n===============\n\n * If the ``cilium`` Pods fail to start after enabling encryption, double-check if\n   the IPsec ``Secret`` and Cilium are deployed in the same namespace together.\n\n * Check for ``level=warning`` and ``level=error`` messages in the Cilium log files\n\n   * If there is a warning message similar to ``Device eth0 does not exist``,\n     use ``--set encryption.ipsec.interface=ethX`` to set the encryption\n     interface.\n\n * Run ``cilium-dbg encrypt status`` in the Cilium Pod:\n\n   .. code-block:: shell-session\n\n       $ cilium-dbg encrypt status\n       Encryption: IPsec\n       Decryption interface(s): eth0, eth1, eth2\n       Keys in use: 4\n       Max Seq. Number: 0x1e3/0xffffffffffffffff\n       Errors: 0\n\n   If the error counter is non-zero, additional information will be displayed\n   with the specific errors the kernel encountered.\n\n   The number of keys in use should be 2 per remote node per enabled IP family.\n   During a key rotation, it can double to 4 per remote node per IP family. For\n   example, in a 3-nodes cluster, if both IPv4 and IPv6 are enabled and no key\n   rotation is ongoing, there should be 8 keys in use on each node.\n\n   The list of decryption interfaces should have all native devices that may\n   receive pod traffic (for example, ENI interfaces).\n\nAll XFRM errors correspond to a packet drop in the kernel. The following\ndetails operational mistakes and expected behaviors that can cause those\nerrors.\n\n * When a node reboots, the key used to communicate with it is expected to\n   change on other nodes. You may notice the ``XfrmInNoStates`` and\n   ``XfrmOutNoStates`` counters increase while the new node key is being\n   deployed.\n\n * After a key rotation, if the old key is cleaned up before the\n   configuration of the new key is installed on all nodes, it results in\n   ``XfrmInNoStates`` errors. The old key is removed from nodes after a default\n   interval of 5 minutes by default. By default, all agents watch for key\n   updates and update their configuration within 1 minute after the key is\n   changed, leaving plenty of time before the old key is removed. If you expect\n   the key rotation to take longer for some reason (for example, in the case of\n   Cluster Mesh where several clusters need to be updated), you can increase the\n   delay before cleanup with agent flag ``ipsec-key-rotation-duration``.\n\n * ``XfrmInStateProtoError`` errors can happen for the following reasons:\n   1. If the key is updated without incrementing the SPI (also called ``KEYID``\n   in :ref:`ipsec_key_rotation` instructions above). It can be fixed by\n   performing a new key rotation, properly.\n   2. If the source node encrypts the packets using a different anti-replay seq\n   from the anti-reply oseq on the destination node. This can be fixed by\n   properly performing a new key rotation.\n\n * ``XfrmFwdHdrError`` and ``XfrmInError`` happen when the kernel fails to\n   lookup the route for a packet it decrypted. This can legitimately happen\n   when a pod was deleted but some packets are still in transit. Note these\n   errors can also happen under memory pressure when the kernel fails to\n   allocate memory.\n\n * ``XfrmInStateInvalid`` can happen on rare occasions if packets are received\n   while an XFRM state is being deleted. XFRM states get deleted as part of\n   node scale-downs and for some upgrades and downgrades.\n\n * The following table documents the known explanations for several XFRM errors\n   that were observed in the past. Many other error types exist, but they are\n   usually for Linux subfeatures that Cilium doesn't use (e.g., XFRM\n   expiration).\n\n   =======================  ==================================================\n   Error                    Known explanation\n   =======================  ==================================================\n   XfrmInError              The kernel (1) decrypted and tried to route a\n                            packet for a pod that was deleted or (2) failed to\n                            allocate memory.\n   XfrmInNoStates           Bug in the XFRM configuration for decryption.\n   XfrmInStateProtoError    There is a key or anti-replay seq mismatch between\n                            nodes.\n   XfrmInStateInvalid       A received packet matched an XFRM state that is\n                            being deleted.\n   XfrmInTmplMismatch       Bug in the XFRM configuration for decryption.\n   XfrmInNoPols             Bug in the XFRM configuration for decryption.\n   XfrmInPolBlock           Explicit drop, not used by Cilium.\n   XfrmOutNoStates          Bug in the XFRM configuration for encryption.\n   XfrmOutStateSeqError     The sequence number of an encryption XFRM\n                            configuration reached its maximum value.\n   XfrmOutPolBlock          Cilium dropped packets that would have otherwise\n                            left the node in plain-text.\n   XfrmFwdHdrError          The kernel (1) decrypted and tried to route a\n                            packet for a pod that was deleted or (2) failed to\n                            allocate memory.\n   =======================  ==================================================\n\n * In addition to the above XFRM errors, packet drops of type ``No node ID\n   found`` (code 197) may also occur under normal operations. These drops can\n   happen if a pod attempts to send traffic to a pod on a new node for which\n   the Cilium agent didn't yet receive the CiliumNode object or to a pod on a\n   node that was recently deleted. It can also happen if the IP address of the\n   destination node changed and the agent didn't receive the updated CiliumNode\n   object yet. In both cases, the IPsec configuration in the kernel isn't ready\n   yet, so Cilium drops the packets at the source. These drops will stop once\n   the CiliumNode information is propagated across the cluster.\n\n.. _xfrm_state_staling_in_cilium:\n\nXFRM State Staling in Cilium\n============================\n\nControl plane disruptions can lead to connectivity issues due to stale XFRM\nstates with out-of-sync IPsec anti-replay counters. This typically results in\npermanent connectivity disruptions between pods managed by Cilium. This section\nexplains how these issues occur and what you can do about them.\n\nIdentified Causes\n-----------------\n\nIn KVStore Mode (e.g., etcd), you might encounter stale XFRM states:\n\n  * If a Cilium agent is down for prolonged time, the corresponding node entry\n    in the kvstore will be deleted due to lease expiration (see\n    :ref:`kvstore_leases`), resulting in stale XFRM states.\n\n  * If you manually recreate your key-value store, a Cilium agent might connect\n    too late to the new instance. This delay can cause the agent to miss crucial\n    node delete and create events, leading Cilium to retain outdated XFRM states\n    for those nodes.\n\nIn CRD Mode, stale XFRM states can occur if you delete a CiliumNode resource and\nrestart the Cilium agent DaemonSet. While other agents create fresh XFRM states\nfor the new CiliumNode, the agent on that new node may retain obsolete XFRM\nstates for all the other peer nodes.\n\nMitigation\n----------\n\nTo restore connectivity in those cases, perform a key rotation (see\n:ref:`ipsec_key_rotation`). This action ensures new consistent and valid XFRM\nstates across all your nodes.\n\nDisabling Encryption\n====================\n\nTo disable the encryption, regenerate the YAML with the option\n``encryption.enabled=false``\n\nLimitations\n===========\n\n    * Transparent encryption is not currently supported when chaining Cilium on\n      top of other CNI plugins. For more information, see :gh-issue:`15596`.\n    * :ref:`HostPolicies` are not currently supported with IPsec encryption.\n    * IPsec encryption currently does not work with BPF Host Routing.\n    * IPsec encryption is not supported on clusters or clustermeshes with more\n      than 65535 nodes.\n    * Decryption with Cilium IPsec is limited to a single CPU core per IPsec\n      tunnel. This may affect performance in case of high throughput between\n      two nodes.\n",
  "item_type": "unknown",
  "module_path": "/tmp/cilium-repo/Documentation/security/network/encryption-ipsec.rst",
  "extracted_at": "2025-09-03T00:53:44.696816Z"
}