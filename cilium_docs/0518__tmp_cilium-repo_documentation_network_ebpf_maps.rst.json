{
  "url": "file:///tmp/cilium-repo/Documentation/network/ebpf/maps.rst",
  "content": ".. only:: not (epub or latex or html)\n\n    WARNING: You are looking at unreleased Cilium documentation.\n    Please use the official rendered version released here:\n    https://docs.cilium.io\n\n.. _bpf_map_limitations:\n\neBPF Maps\n=========\n\nAll BPF maps are created with upper capacity limits. Insertion beyond the limit\nwill fail and thus limits the scalability of the datapath. The following table\nshows the default values of the maps. Each limit can be bumped in the source\ncode. Configuration options will be added on request if demand arises.\n\n======================== ================ =============== =====================================================\nMap Name                 Scope            Default Limit   Scale Implications\n======================== ================ =============== =====================================================\nAuth                     node             512k            Max 512k authenticated relations per node\nConnection Tracking      node or endpoint 1M TCP/256k UDP Max 1M concurrent TCP connections, max 256k expected UDP answers\nNAT                      node             512k            Max 512k NAT entries\nNeighbor Table           node             512k            Max 512k neighbor entries\nEndpoints                node             64k             Max 64k local endpoints + host IPs per node\nIP cache                 node             512k            Max 256k endpoints (IPv4+IPv6), max 512k endpoints (IPv4 or IPv6) across all clusters\nService Load Balancer    node             64k             Max ~3k clusterIP/nodePort Services across all clusters (see: `service map sizing <#service-lb-map-sizing>`_ section for details).\nService Backends         node             64k             Max 64k cumulative unique backends across all services across all clusters\nPolicy                   endpoint         16k             Max 16k allowed identity + port + protocol pairs for specific endpoint\nProxy Map                node             512k            Max 512k concurrent redirected TCP connections to proxy\nTunnel                   node             64k             Max 32k nodes (IPv4+IPv6) or 64k nodes (IPv4 or IPv6) across all clusters\nIPv4 Fragmentation       node             8k              Max 8k fragmented datagrams in flight simultaneously on the node\nSession Affinity         node             64k             Max 64k affinities from different clients\nIPv4 Masq                node             16k             Max 16k IPv4 cidrs used by BPF-based ip-masq-agent\nIPv6 Masq                node             16k             Max 16k IPv6 cidrs used by BPF-based ip-masq-agent\nService Source Ranges    node             64k             Max 64k cumulative LB source ranges across all services\nEgress Policy            endpoint         16k             Max 16k endpoints across all destination CIDRs across all clusters \nNode                     node             16k             Max 16k distinct node IPs (IPv4 & IPv6) across all clusters.\n======================== ================ =============== =====================================================\n\nFor some BPF maps, the upper capacity limit can be overridden using command\nline options for ``cilium-agent``. A given capacity can be set using\n``--bpf-auth-map-max``, ``--bpf-ct-global-tcp-max``, ``--bpf-ct-global-any-max``,\n``--bpf-nat-global-max``, ``--bpf-neigh-global-max``, ``--bpf-policy-map-max``,\n``--bpf-fragments-map-max`` and ``--bpf-lb-map-max``.\n\n.. Note::\n\n   In case the ``--bpf-ct-global-tcp-max`` and/or ``--bpf-ct-global-any-max``\n   are specified, the NAT table size (``--bpf-nat-global-max``) must not exceed\n   2/3 of the combined CT table size (TCP + UDP). This will automatically be set\n   if either ``--bpf-nat-global-max`` is not explicitly set or if dynamic BPF\n   map sizing is used (see below).\n\nUsing the ``--bpf-map-dynamic-size-ratio`` flag, the upper capacity limits of\nseveral large BPF maps are determined at agent startup based on the given ratio\nof the total system memory. For example, a given ratio of 0.0025 leads to 0.25%\nof the total system memory to be used for these maps.\n\nThis flag affects the following BPF maps that consume most memory in the system:\n``cilium_ct_{4,6}_global``, ``cilium_ct_{4,6}_any``,\n``cilium_nodeport_neigh{4,6}``, ``cilium_snat_v{4,6}_external`` and\n``cilium_lb{4,6}_reverse_sk``.\n\n``kube-proxy`` sets as the maximum number entries in the linux's connection\ntracking table based on the number of cores the machine has. ``kube-proxy`` has\na default of ``32768`` maximum entries per core with a minimum of ``131072``\nentries regardless of the number of cores the machine has.\n\nCilium has its own connection tracking tables as BPF Maps and the number of\nentries of such maps is calculated based on the amount of total memory in the\nnode with a minimum of ``131072`` entries regardless the amount of memory the\nmachine has.\n\nThe following table presents the value that ``kube-proxy`` and Cilium sets for\ntheir own connection tracking tables when Cilium is configured with\n``--bpf-map-dynamic-size-ratio: 0.0025``.\n\n+------+--------------+-----------------------+-------------------+\n| vCPU | Memory (GiB) | Kube-proxy CT entries | Cilium CT entries |\n+------+--------------+-----------------------+-------------------+\n|    1 |         3.75 |                131072 |            131072 |\n+------+--------------+-----------------------+-------------------+\n|    2 |          7.5 |                131072 |            131072 |\n+------+--------------+-----------------------+-------------------+\n|    4 |           15 |                131072 |            131072 |\n+------+--------------+-----------------------+-------------------+\n|    8 |           30 |                262144 |            284560 |\n+------+--------------+-----------------------+-------------------+\n|   16 |           60 |                524288 |            569120 |\n+------+--------------+-----------------------+-------------------+\n|   32 |          120 |               1048576 |           1138240 |\n+------+--------------+-----------------------+-------------------+\n|   64 |          240 |               2097152 |           2276480 |\n+------+--------------+-----------------------+-------------------+\n|   96 |          360 |               3145728 |           4552960 |\n+------+--------------+-----------------------+-------------------+\n\n.. _svc_lb_tuning:\n\nService LB Map Sizing\n=====================\n\nCilium uses the LB services maps named ``cilium_lb{4,6}_services_v2`` to hold Service load balancer entries for clusterIP and nodePort service types.\nThese maps are configured via the ``--bpf-lb-map-max`` flag and are set to 64k by default. If this map is full, Cilium may be unable to reconcile Service\nupdates which may affect connectivity to service IPs or the ability to create new services.\n\nThe required size of service LB maps depends on multiple factors. Each clusterIP/nodePort service will create\na number of entries equal to the number of Pods backends selected by the service, times the number of port/protocol entries in the respective Service spec.\n\n:math:`\\text{LB map entries per Service} = (\\text{number of endpoints per service}) * (\\text{number of port/protocols per service})`\n\nUsing this, we can roughly the required map size as:\n\n:math:`\\text{LB map entries} \\approx (\\text{number of LB services}) * (\\text{avg number of endpoints per service}) * (\\text{avg number of port/protocols per service})`\n\n.. note::\n\n   This heuristic assumes that number of selected Pods and ports/protocol entries per service are roughly normally distributed. If your use case\n   has large outliers (ex. such as a service that selects a very large set of Pod backends) it may be necessary to do a more detailed estimate.\n\nOnce Cilium has created the service LB maps for a Node (i.e. upon first running Cilium agent on a Node), attempting to resize the map size\nparameter and restarting Cilium results in connection disruptions as the new map is repopulated with existing service entries.\nTherefore it is important to carefully consider map requirements prior to installing Cilium if such disruptions are a concern.\n\n",
  "item_type": "unknown",
  "module_path": "/tmp/cilium-repo/Documentation/network/ebpf/maps.rst",
  "extracted_at": "2025-09-03T01:13:29.220466Z"
}