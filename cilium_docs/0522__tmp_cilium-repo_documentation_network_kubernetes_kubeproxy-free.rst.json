{
  "url": "file:///tmp/cilium-repo/Documentation/network/kubernetes/kubeproxy-free.rst",
  "content": ".. only:: not (epub or latex or html)\n\n    WARNING: You are looking at unreleased Cilium documentation.\n    Please use the official rendered version released here:\n    https://docs.cilium.io\n\n.. _kubeproxy-free:\n\n*****************************\nKubernetes Without kube-proxy\n*****************************\n\nThis guide explains how to provision a Kubernetes cluster without ``kube-proxy``,\nand to use Cilium to fully replace it. For simplicity, we will use ``kubeadm`` to\nbootstrap the cluster.\n\nFor help with installing ``kubeadm`` and for more provisioning options please refer to\n`the official Kubeadm documentation  `_.\n\n.. note::\n\n   Cilium's kube-proxy replacement depends on the socket-LB feature.\n\nQuick-Start\n###########\n\nInitialize the control-plane node via ``kubeadm init`` and skip the\ninstallation of the ``kube-proxy`` add-on:\n\n.. note::\n    Depending on what CRI implementation you are using, you may need to use the\n    ``--cri-socket`` flag with your ``kubeadm init ...`` command.\n    For example: if you're using Docker CRI you would use\n    ``--cri-socket unix:///var/run/cri-dockerd.sock``.\n\n.. code-block:: shell-session\n\n    $ kubeadm init --skip-phases=addon/kube-proxy\n\nAfterwards, join worker nodes by specifying the control-plane node IP address and\nthe token returned by ``kubeadm init``\n(for this tutorial, you will want to add at least one worker node to the cluster):\n\n.. code-block:: shell-session\n\n    $ kubeadm join <..>\n\n.. note::\n\n    Please ensure that\n    `kubelet  `_'s\n    ``--node-ip`` is set correctly on each worker if you have multiple interfaces.\n    Cilium's kube-proxy replacement may not work correctly otherwise.\n    You can validate this by running ``kubectl get nodes -o wide`` to see whether\n    each node has an ``InternalIP`` which is assigned to a device with the same\n    name on each node.\n\nFor existing installations with ``kube-proxy`` running as a DaemonSet, remove it\nby using the following commands below.\n\n.. warning::\n   Be aware that removing ``kube-proxy`` will break existing service connections. It will also stop service related traffic\n   until the Cilium replacement has been installed.\n\n.. warning::\n   When deploying the eBPF kube-proxy replacement under co-existence with\n   kube-proxy on the system, be aware that both mechanisms operate independent of each\n   other. Meaning, if the eBPF kube-proxy replacement is added or removed on an already\n   *running* cluster in order to delegate operation from respectively back to kube-proxy,\n   then it must be expected that existing connections will break since, for example,\n   both NAT tables are not aware of each other. If deployed in co-existence on a newly\n   spawned up node/cluster which does not yet serve user traffic, then this is not an\n   issue.\n\n.. code-block:: shell-session\n\n   $ kubectl -n kube-system delete ds kube-proxy\n   $ # Delete the configmap as well to avoid kube-proxy being reinstalled during a Kubeadm upgrade\n   $ kubectl -n kube-system delete cm kube-proxy\n   $ # Run on each node with root permissions:\n   $ iptables-save | grep -v KUBE | iptables-restore\n\n.. include:: ../../installation/k8s-install-download-release.rst\n\nNext, generate the required YAML files and deploy them.\n\n.. important::\n\n   Make sure you correctly set your ``API_SERVER_IP`` and ``API_SERVER_PORT``\n   below with the control-plane node IP address and the kube-apiserver port\n   number reported by ``kubeadm init`` (Kubeadm will use port ``6443`` by default).\n\nSpecifying this is necessary as ``kubeadm init`` is run explicitly without setting\nup kube-proxy and as a consequence, although it exports ``KUBERNETES_SERVICE_HOST``\nand ``KUBERNETES_SERVICE_PORT`` with a ClusterIP of the kube-apiserver service\nto the environment, there is no kube-proxy in our setup provisioning that service.\nTherefore, the Cilium agent needs to be made aware of this information with the following configuration:\n\n.. parsed-literal::\n\n    API_SERVER_IP= \n    # Kubeadm default is 6443\n    API_SERVER_PORT= \n    helm install cilium |CHART_RELEASE| \\\\\n        --namespace kube-system \\\\\n        --set kubeProxyReplacement=true \\\\\n        --set k8sServiceHost=${API_SERVER_IP} \\\\\n        --set k8sServicePort=${API_SERVER_PORT}\n\n.. note::\n\n    Cilium will automatically mount cgroup v2 filesystem required to attach BPF\n    cgroup programs by default at the path ``/run/cilium/cgroupv2``. To do that,\n    it needs to mount the host ``/proc`` inside an init container\n    launched by the DaemonSet temporarily. If you need to disable the auto-mount,\n    specify ``--set cgroup.autoMount.enabled=false``, and set the host mount point\n    where cgroup v2 filesystem is already mounted by using ``--set cgroup.hostRoot``.\n    For example, if not already mounted, you can mount cgroup v2 filesystem by\n    running the below command on the host, and specify ``--set cgroup.hostRoot=/sys/fs/cgroup``.\n\n    .. code:: shell-session\n\n        mount -t cgroup2 none /sys/fs/cgroup\n\nThis will install Cilium as a CNI plugin with the eBPF kube-proxy replacement to\nimplement handling of Kubernetes services of type ClusterIP, NodePort, LoadBalancer\nand services with externalIPs. As well, the eBPF kube-proxy replacement also\nsupports hostPort for containers such that using portmap is not necessary anymore.\n\nFinally, as a last step, verify that Cilium has come up correctly on all nodes and\nis ready to operate:\n\n.. code-block:: shell-session\n\n    $ kubectl -n kube-system get pods -l k8s-app=cilium\n    NAME                READY     STATUS    RESTARTS   AGE\n    cilium-fmh8d        1/1       Running   0          10m\n    cilium-mkcmb        1/1       Running   0          10m\n\nNote, in above Helm configuration, the ``kubeProxyReplacement`` has been set to\n``true`` mode. This means that the Cilium agent will bail out in case the\nunderlying Linux kernel support is missing.\n\nBy default, Helm sets ``kubeProxyReplacement=false``, which only enables\nper-packet in-cluster load-balancing of ClusterIP services.\n\nCilium's eBPF kube-proxy replacement is supported in direct routing as well as in\ntunneling mode.\n\nValidate the Setup\n##################\n\nAfter deploying Cilium with above Quick-Start guide, we can first validate that\nthe Cilium agent is running in the desired mode:\n\n.. code-block:: shell-session\n\n    $ kubectl -n kube-system exec ds/cilium -- cilium-dbg status | grep KubeProxyReplacement\n    KubeProxyReplacement:   True\t[eth0 (Direct Routing), eth1]\n\nUse ``--verbose`` for full details:\n\n.. code-block:: shell-session\n\n    $ kubectl -n kube-system exec ds/cilium -- cilium-dbg status --verbose\n    [...]\n    KubeProxyReplacement Details:\n      Status:                True\n      Socket LB:             Enabled\n      Protocols:             TCP, UDP\n      Devices:               eth0 (Direct Routing), eth1\n      Mode:                  SNAT\n      Backend Selection:     Random\n      Session Affinity:      Enabled\n      Graceful Termination:  Enabled\n      NAT46/64 Support:      Enabled\n      XDP Acceleration:      Disabled\n      Services:\n      - ClusterIP:      Enabled\n      - NodePort:       Enabled (Range: 30000-32767)\n      - LoadBalancer:   Enabled\n      - externalIPs:    Enabled\n      - HostPort:       Enabled\n    [...]\n\nAs an optional next step, we will create an Nginx Deployment. Then we'll create a new NodePort service and\nvalidate that Cilium installed the service correctly.\n\nThe following YAML is used for the backend pods:\n\n.. code-block:: yaml\n\n    apiVersion: apps/v1\n    kind: Deployment\n    metadata:\n      name: my-nginx\n    spec:\n      selector:\n        matchLabels:\n          run: my-nginx\n      replicas: 2\n      template:\n        metadata:\n          labels:\n            run: my-nginx\n        spec:\n          containers:\n          - name: my-nginx\n            image: nginx\n            ports:\n            - containerPort: 80\n\nVerify that the Nginx pods are up and running:\n\n.. code-block:: shell-session\n\n    $ kubectl get pods -l run=my-nginx -o wide\n    NAME                        READY   STATUS    RESTARTS   AGE   IP             NODE   NOMINATED NODE   READINESS GATES\n    my-nginx-756fb87568-gmp8c   1/1     Running   0          62m   10.217.0.149   apoc                \n    my-nginx-756fb87568-n5scv   1/1     Running   0          62m   10.217.0.107   apoc                \n\nIn the next step, we create a NodePort service for the two instances:\n\n.. code-block:: shell-session\n\n    $ kubectl expose deployment my-nginx --type=NodePort --port=80\n    service/my-nginx exposed\n\nVerify that the NodePort service has been created:\n\n.. code-block:: shell-session\n\n    $ kubectl get svc my-nginx\n    NAME       TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE\n    my-nginx   NodePort   10.104.239.135            80:31940/TCP   24m\n\nWith the help of the ``cilium-dbg service list`` command, we can validate that\nCilium's eBPF kube-proxy replacement created the new NodePort service.\nIn this example, services with port ``31940`` were created (one for each of devices ``eth0`` and ``eth1``):\n\n.. code-block:: shell-session\n\n    $ kubectl -n kube-system exec ds/cilium -- cilium-dbg service list\n    ID   Frontend               Service Type   Backend\n    [...]\n    4    10.104.239.135:80/TCP      ClusterIP      1 => 10.217.0.107:80/TCP\n                                                   2 => 10.217.0.149:80/TCP\n    5    0.0.0.0:31940/TCP          NodePort       1 => 10.217.0.107:80/TCP\n                                                   2 => 10.217.0.149:80/TCP\n    6    192.168.178.29:31940/TCP   NodePort       1 => 10.217.0.107:80/TCP\n                                                   2 => 10.217.0.149:80/TCP\n    7    172.16.0.29:31940/TCP      NodePort       1 => 10.217.0.107:80/TCP\n                                                   2 => 10.217.0.149:80/TCP\n\nCreate a variable with the node port for testing:\n\n.. code-block:: shell-session\n\n    $ node_port=$(kubectl get svc my-nginx -o=jsonpath='{@.spec.ports[0].nodePort}')\n\nAt the same time we can verify, using ``iptables`` in the host namespace,\nthat no ``iptables`` rule for the service is present:\n\n.. code-block:: shell-session\n\n    $ iptables-save | grep KUBE-SVC\n    [ empty line ]\n\nLast but not least, a simple ``curl`` test shows connectivity for the exposed\nNodePort as well as for the ClusterIP:\n\n.. code-block:: shell-session\n\n    $ curl 127.0.0.1:$node_port\n    \n    \n    \n     Welcome to nginx! \n    [....]\n\n.. code-block:: shell-session\n\n    $ curl 192.168.178.29:$node_port\n    \n    \n    \n     welcome to nginx! \n    [....]\n\n.. code-block:: shell-session\n\n    $ curl 172.16.0.29:$node_port\n    \n    \n    \n     welcome to nginx! \n    [....]\n\n.. code-block:: shell-session\n\n    $ curl 10.104.239.135:80\n    \n    \n    \n     Welcome to nginx! \n    [....]\n\nAs can be seen, Cilium's eBPF kube-proxy replacement is set up correctly.\n\nAdvanced Configuration\n######################\n\nThis section covers a few advanced configuration modes for the kube-proxy replacement\nthat go beyond the above Quick-Start guide and are entirely optional.\n\nClient Source IP Preservation\n*****************************\n\nCilium's eBPF kube-proxy replacement implements various options to avoid\nperforming SNAT on NodePort requests where the client source IP address would otherwise\nbe lost on its path to the service endpoint.\n\n- ``externalTrafficPolicy=Local``: The ``Local`` policy is generally supported through\n  the eBPF implementation. In-cluster connectivity for services with ``externalTrafficPolicy=Local``\n  is possible and can also be reached from nodes which have no local backends, meaning,\n  given SNAT does not need to be performed, all service endpoints are available for\n  load balancing from in-cluster side.\n\n- ``externalTrafficPolicy=Cluster``: For the ``Cluster`` policy which is the default\n  upon service creation, multiple options exist for achieving client source IP preservation\n  for external traffic, that is, operating the kube-proxy replacement in :ref:`DSR `\n  or :ref:`Hybrid ` mode if only TCP-based services are exposed to the outside\n  world for the latter.\n\nInternal Traffic Policy\n***********************\n\nSimilar to ``externalTrafficPolicy`` described above, Cilium's eBPF kube-proxy replacement\nsupports ``internalTrafficPolicy``, which translates the above semantics to in-cluster traffic.\n\n- For services with ``internalTrafficPolicy=Local``, traffic originated from pods in the\n  current cluster is routed only to endpoints within the same node the traffic originated from.\n\n- ``internalTrafficPolicy=Cluster`` is the default, and it doesn't restrict the endpoints that\n  can handle internal (in-cluster) traffic.\n\nThe following table gives an idea of what backends are used to serve connections to a service,\ndepending on the external and internal traffic policies:\n\n+---------------------+-------------------------------------------------+\n| Traffic policy      | Service backends used                           |\n+----------+----------+-------------------------+-----------------------+\n| Internal | External | for North-South traffic | for East-West traffic |\n+==========+==========+=========================+=======================+\n| Cluster  | Cluster  | All (default)           | All (default)         |\n+----------+----------+-------------------------+-----------------------+\n| Cluster  | Local    | Node-local only         | All (default)         |\n+----------+----------+-------------------------+-----------------------+\n| Local    | Cluster  | All (default)           | Node-local only       |\n+----------+----------+-------------------------+-----------------------+\n| Local    | Local    | Node-local only         | Node-local only       |\n+----------+----------+-------------------------+-----------------------+\n\nSelective Service Type Exposure\n*******************************\n\nBy default, for a ``LoadBalancer`` service Cilium exposes corresponding\n``NodePort`` and ``ClusterIP`` services. Likewise, for a new ``NodePort``\nservice, Cilium exposes the corresponding ``ClusterIP`` service.\n\nIf this behavior is not desired, then the ``service.cilium.io/type``\nannotation can be used to pin the service creation only to a specific\nservice type:\n\n.. code-block:: yaml\n\n  apiVersion: v1\n  kind: Service\n  metadata:\n    name: example-service\n    annotations:\n      service.cilium.io/type: LoadBalancer\n  spec:\n    ports:\n      - port: 80\n        targetPort: 80\n    type: LoadBalancer\n    allocateLoadBalancerNodePorts: false\n\nIn the above example only the ``LoadBalancer`` service is created without\ncorresponding ``NodePort`` and ``ClusterIP`` services. If the annotation\nwould be set to e.g. ``service.cilium.io/type: NodePort``, then only the\n``NodePort`` service would be installed.\n\nHost Proxy Delegation\n*********************\n\nIf the selected service backend IP for a given service matches the local\nnode IP, the annotation ``service.cilium.io/proxy-delegation: delegate-if-local``\nwill pass the received packet unmodified to the upper stack, so that a\nL7 proxy such as Envoy (if present) can handle the request in the host\nnamespace. This mechanism is mainly targeted for north/south traffic.\n\nIf the selected service backend is a remote IP, then the received packet\nis not pushed to the upper stack and instead the BPF code forwards the\npacket natively with the configured forwarding method to the remote IP.\n\n.. code-block:: yaml\n\n  apiVersion: v1\n  kind: Service\n  metadata:\n    name: example-service\n    annotations:\n      service.cilium.io/proxy-delegation: delegate-if-local\n  spec:\n    ports:\n      - port: 80\n        targetPort: 80\n    type: LoadBalancer\n\nIn combination with ``externalTrafficPolicy=Local`` this mechanism also allows\nfor pushing all traffic to the upper proxy.\n\nFor east/west traffic, the service translation is skipped and the packet goes\nout of the node without any DNAT.\n\nNon-presence of the ``service.cilium.io/proxy-delegation`` annotation leaves\nall forwarding to BPF natively which is also the default for the kube-proxy\nreplacement case.\n\nSelective Service Node Exposure\n*******************************\n\nBy default, Cilium exposes Kubernetes services on all nodes in the cluster. To expose a\nservice only on a subset of the nodes instead, use the ``service.cilium.io/node`` label for\nthe relevant nodes. For example, label a node as follows:\n\n.. code-block:: shell-session\n\n  $ kubectl label node node_name service.cilium.io/node=beefy\n\nTo add a new service that should only be exposed to nodes with label ``service.cilium.io/node=beefy``, install the service as follows:\n\n.. code-block:: yaml\n\n  apiVersion: v1\n  kind: Service\n  metadata:\n    name: example-service\n    annotations:\n      service.cilium.io/node: beefy\n  spec:\n    selector:\n      app: example\n    ports:\n      - port: 8765\n        targetPort: 9376\n    type: LoadBalancer\n\nIt's also possible to control the service node exposure via the annotation ``service.cilium.io/node-selector`` - where\nthe annotation value contains the label selector. This way, the service is only exposed on nodes that match the\nnode label selector. The annotation ``service.cilium.io/node-selector`` always has priority over \n``service.cilium.io/node`` if both exist on the same service.\n\n.. code-block:: yaml\n\n  apiVersion: v1\n  kind: Service\n  metadata:\n    name: example-service\n    annotations:\n      service.cilium.io/node-selector: \"service.cilium.io/node in ( beefy , slow )\"\n  spec:\n    selector:\n      app: example\n    ports:\n      - port: 8765\n        targetPort: 9376\n    type: LoadBalancer\n\nNote that changing a node label after a service has been exposed matching that label does not\nautomatically update the list of nodes where the service is exposed. To update exposure of the\nservice after changing node labels, restart the Cilium agent. Generally it is advised to fixate the\nnode label upon joining the Kubernetes cluster and retain it throughout the node's lifetime.\n\n.. _maglev:\n\nMaglev Consistent Hashing\n*************************\n\nCilium's eBPF kube-proxy replacement supports consistent hashing by implementing a variant\nof `The Maglev hashing  `_\nin its load balancer for backend selection. This improves resiliency in case of\nfailures. As well, it provides better load balancing properties since Nodes added to the cluster will\nmake consistent backend selection throughout the cluster for a given 5-tuple without\nhaving to synchronize state with the other Nodes. Similarly, upon backend removal the backend\nlookup tables are reprogrammed with minimal disruption for unrelated backends (at most 1%\ndifference in the reassignments) for the given service.\n\nMaglev hashing for services load balancing can be enabled by setting ``loadBalancer.algorithm=maglev``:\n\n.. parsed-literal::\n\n    helm install cilium |CHART_RELEASE| \\\\\n        --namespace kube-system \\\\\n        --set kubeProxyReplacement=true \\\\\n        --set loadBalancer.algorithm=maglev \\\\\n        --set k8sServiceHost=${API_SERVER_IP} \\\\\n        --set k8sServicePort=${API_SERVER_PORT}\n\nNote that Maglev hashing is applied only to external (N-S) traffic. For\nin-cluster service connections (E-W), sockets are assigned to service backends\ndirectly, e.g. at TCP connect time, without any intermediate hop and thus are\nnot subject to Maglev. Maglev hashing is also supported for Cilium's\n:ref:`XDP ` acceleration.\n\nThere are two more Maglev-specific configuration settings: ``maglev.tableSize``\nand ``maglev.hashSeed``.\n\n``maglev.tableSize`` specifies the size of the Maglev lookup table for each single service.\n`Maglev  `__\nrecommends the table size (``M``) to be significantly larger than the number of maximum expected\nbackends (``N``). In practice that means that ``M`` should be larger than ``100 * N`` in\norder to guarantee the property of at most 1% difference in the reassignments on backend\nchanges. ``M`` must be a prime number. Cilium uses a default size of ``16381`` for ``M``.\nThe following sizes for ``M`` are supported as ``maglev.tableSize`` Helm option:\n\n+----------------------------+\n| ``maglev.tableSize`` value |\n+============================+\n| 251                        |\n+----------------------------+\n| 509                        |\n+----------------------------+\n| 1021                       |\n+----------------------------+\n| 2039                       |\n+----------------------------+\n| 4093                       |\n+----------------------------+\n| 8191                       |\n+----------------------------+\n| 16381                      |\n+----------------------------+\n| 32749                      |\n+----------------------------+\n| 65521                      |\n+----------------------------+\n| 131071                     |\n+----------------------------+\n\nFor example, a ``maglev.tableSize`` of ``16381`` is suitable for a maximum of ``~160`` backends\nper service. If a higher number of backends are provisioned under this setting, then the\ndifference in reassignments on backend changes will increase. Note that changing the table\nsize (``M``) triggers a recalculation of the lookup table and can temporarily lead to inconsistent\nbackend selection for new traffic until all nodes have converged and completed their agent restart.\n\nThe ``maglev.hashSeed`` option is recommended to be set in order for Cilium to not rely on the\nfixed built-in seed. The seed is a base64-encoded 12 byte-random number, and can be\ngenerated once through ``head -c12 /dev/urandom | base64 -w0``, for example.\nEvery Cilium agent in the cluster must use the same hash seed for Maglev to work.\n\nThe below deployment example is generating and passing such seed to Helm as well as setting the\nMaglev table size to ``65521`` to allow for ``~650`` maximum backends for a\ngiven service (with the property of at most 1% difference on backend reassignments):\n\n.. parsed-literal::\n\n    SEED=$(head -c12 /dev/urandom | base64 -w0)\n    helm install cilium |CHART_RELEASE| \\\\\n        --namespace kube-system \\\\\n        --set kubeProxyReplacement=true \\\\\n        --set loadBalancer.algorithm=maglev \\\\\n        --set maglev.tableSize=65521 \\\\\n        --set maglev.hashSeed=$SEED \\\\\n        --set k8sServiceHost=${API_SERVER_IP} \\\\\n        --set k8sServicePort=${API_SERVER_PORT}\n\n\nNote that enabling Maglev will have a higher memory consumption on each Cilium-managed Node compared\nto the default of ``loadBalancer.algorithm=random`` given ``random`` does not need the extra lookup\ntables. However, ``random`` won't have consistent backend selection.\n\n.. _DSR mode:\n\nDirect Server Return (DSR)\n**************************\n\nBy default, Cilium's eBPF NodePort implementation operates in SNAT mode. That is,\nwhen node-external traffic arrives and the node determines that the backend for\nthe LoadBalancer, NodePort, or services with externalIPs is at a remote node, then the\nnode is redirecting the request to the remote backend on its behalf by performing\nSNAT. This does not require any additional MTU changes. The cost is that replies\nfrom the backend need to make the extra hop back to that node to perform the\nreverse SNAT translation there before returning the packet directly to the external\nclient.\n\nThis setting can be changed through the ``loadBalancer.mode`` Helm option to\n``dsr`` in order to let Cilium's eBPF NodePort implementation operate in DSR mode.\nIn this mode, the backends reply directly to the external client without taking\nthe extra hop, meaning, backends reply by using the service IP/port as a source.\n\nAnother advantage in DSR mode is that the client's source IP is preserved, so policy\ncan match on it at the backend node. In the SNAT mode this is not possible.\nGiven a specific backend can be used by multiple services, the backends need to be\nmade aware of the service IP/port which they need to reply with. Cilium encodes this\ninformation into the packet (using one of the dispatch mechanisms described below),\nat the cost of advertising a lower MTU. For TCP services, Cilium\nonly encodes the service IP/port for the SYN packet, but not subsequent ones. This\noptimization also allows to operate Cilium in a hybrid mode as detailed in the later\nsubsection where DSR is used for TCP and SNAT for UDP in order to avoid an otherwise\nneeded MTU reduction.\n\nIn some public cloud provider environments that implement source /\ndestination IP address checking (e.g. AWS), the checking has to be disabled in\norder for the DSR mode to work.\n\nBy default Cilium uses special ExternalIP mitigation for CVE-2020-8554 MITM vulnerability.\nThis may affect connectivity targeted to ExternalIP on the same cluster.\nThis mitigation can be disabled by setting ``bpf.disableExternalIPMitigation`` to ``true``.\n\n.. _DSR mode with Option:\n\nDirect Server Return (DSR) with IPv4 option / IPv6 extension Header\n*******************************************************************\n\nIn this DSR dispatch mode, the service IP/port information is transported to the\nbackend through a Cilium-specific IPv4 Option or IPv6 Destination Option extension header.\nIt requires Cilium to be deployed in :ref:`arch_direct_routing`, i.e.\nit will not work in :ref:`arch_overlay` mode.\n\nThis DSR mode might not work in some public cloud provider environments\ndue to the Cilium-specific IP options that could be dropped by an underlying network fabric.\nIn case of connectivity issues to services where backends are located on\na remote node from the node that is processing the given NodePort request,\nfirst check whether the NodePort request actually arrived on the node\ncontaining the backend. If this was not the case, then consider either switching to\nDSR with Geneve (as described below), or switching back to the default SNAT mode.\n\nThe above Helm example configuration in a kube-proxy-free environment with DSR-only mode\nenabled would look as follows:\n\n.. parsed-literal::\n\n    helm install cilium |CHART_RELEASE| \\\\\n        --namespace kube-system \\\\\n        --set routingMode=native \\\\\n        --set kubeProxyReplacement=true \\\\\n        --set loadBalancer.mode=dsr \\\\\n        --set loadBalancer.dsrDispatch=opt \\\\\n        --set k8sServiceHost=${API_SERVER_IP} \\\\\n        --set k8sServicePort=${API_SERVER_PORT}\n\n.. _DSR mode with Geneve:\n\nDirect Server Return (DSR) with Geneve\n**************************************\nBy default, Cilium with DSR mode encodes the service IP/port in a Cilium-specific\nIPv4 option or IPv6 Destination Option extension so that the backends are aware of\nthe service IP/port, which they need to reply with.\n\nHowever, some data center routers pass packets with unknown IP options to software\nprocessing called \"Layer 2 slow path\". Those routers drop the packets if the amount\nof packets with IP options exceeds a given threshold, which may significantly affect\nnetwork performance.\n\nCilium offers another dispatch mode, DSR with Geneve, to avoid this problem.\nIn DSR with Geneve, Cilium encapsulates packets to the Loadbalancer with the Geneve\nheader that includes the service IP/port in the Geneve option and redirects them\nto the backends.\n\nYou can use DSR with Geneve for encapsulating LoadBalancer traffic regardless\nof whether you have configured the routing mode to route traffic using Geneve\n:ref:`encapsulation` or :ref:`native_routing`.\n\n.. tabs::\n\n    .. group-tab:: Native (Routing)\n\n        Install Cilium via ``helm install`` with DSR Geneve dispatch and Native (routing) Mode\n\n        .. parsed-literal::\n\n            helm install cilium |CHART_RELEASE| \\\\\n                --namespace kube-system \\\\\n                --set routingMode=native \\\\\n                --set tunnelProtocol=geneve \\\\\n                --set kubeProxyReplacement=true \\\\\n                --set loadBalancer.mode=dsr \\\\\n                --set loadBalancer.dsrDispatch=geneve \\\\\n                --set k8sServiceHost=${API_SERVER_IP} \\\\\n                --set k8sServicePort=${API_SERVER_PORT}\n\n    .. group-tab:: Tunnel (encapsulation)\n\n        Install Cilium via ``helm install`` with DSR Geneve dispatch and tunneling (encapsulation) mode\n\n        .. parsed-literal::\n\n            helm install cilium |CHART_RELEASE| \\\\\n                --namespace kube-system \\\\\n                --set routingMode=tunnel \\\\\n                --set tunnelProtocol=geneve \\\\\n                --set kubeProxyReplacement=true \\\\\n                --set loadBalancer.mode=dsr \\\\\n                --set loadBalancer.dsrDispatch=geneve \\\\\n                --set k8sServiceHost=${API_SERVER_IP} \\\\\n                --set k8sServicePort=${API_SERVER_PORT}\n\n.. _Hybrid mode:\n\nHybrid DSR and SNAT Mode\n************************\n\nCilium also supports a hybrid DSR and SNAT mode, that is, DSR is performed for TCP\nand SNAT for UDP connections.\n\nThis removes the need for manual MTU changes in the network while still benefiting\nfrom the latency improvements through the removed extra hop for replies, in particular,\nwhen TCP is the main transport for workloads.\n\nThe mode setting ``loadBalancer.mode`` allows to control the behavior through the\noptions ``dsr``, ``snat``, ``annotation``, and ``hybrid``. By default the ``snat``\nmode is used in the agent.\n\nA Helm example configuration in a kube-proxy-free environment with DSR enabled in\nhybrid mode would look as follows:\n\n.. parsed-literal::\n\n    helm install cilium |CHART_RELEASE| \\\\\n        --namespace kube-system \\\\\n        --set routingMode=native \\\\\n        --set kubeProxyReplacement=true \\\\\n        --set loadBalancer.mode=hybrid \\\\\n        --set k8sServiceHost=${API_SERVER_IP} \\\\\n        --set k8sServicePort=${API_SERVER_PORT}\n\nAnnotation-based DSR and SNAT Mode\n**********************************\n\nCilium also supports an annotation-based DSR and SNAT mode, that is, services\ncan be exposed by default via SNAT and on-demand as DSR (or vice versa):\n\n.. code-block:: yaml\n\n  apiVersion: v1\n  kind: Service\n  metadata:\n    name: example-service\n    annotations:\n      service.cilium.io/type: LoadBalancer\n      service.cilium.io/forwarding-mode: dsr\n  spec:\n    ports:\n      - port: 80\n        targetPort: 80\n    type: LoadBalancer\n\nNote that the ``forwarding-mode`` annotation must be set at service creation time\nand should not be changed during the lifetime of that service. Changing the value\nof the annotation or removing the annotation while the service is installed breaks\nconnections.\n\nThe above example installs the Kubernetes service only as type ``LoadBalancer``,\nthat is, without the corresponding ``NodePort`` and ``ClusterIP`` services, and\nuses the configured DSR method to forward the packets instead of default SNAT.\nThe Helm setting ``loadBalancer.mode=snat`` defines the default as SNAT in this\nexample. A ``loadBalancer.mode=dsr`` would have switched the default to DSR instead\nand then ``service.cilium.io/forwarding-mode: snat`` annotation can be used to\nswitch to SNAT instead.\n\nA Helm example configuration in a kube-proxy-free environment with DSR enabled in\nannotation mode with SNAT default would look as follows:\n\n.. parsed-literal::\n\n    helm install cilium |CHART_RELEASE| \\\\\n        --namespace kube-system \\\\\n        --set routingMode=native \\\\\n        --set kubeProxyReplacement=true \\\\\n        --set loadBalancer.mode=snat \\\\\n        --set loadBalancer.dsrDispatch=geneve \\\\\n        --set bpf.lbModeAnnotation=true \\\\\n        --set k8sServiceHost=${API_SERVER_IP} \\\\\n        --set k8sServicePort=${API_SERVER_PORT}\n\n.. note::\n\n    When using annotation-based DSR mode (``bpf.lbModeAnnotation=true``), as in the previous example, you must explicitly specify the ``loadBalancer.dsrDispatch`` parameter to define how DSR packets are dispatched to backends. Valid options are ``opt``, ``ipip``, and ``geneve``.\n\n    For example, for environments where Geneve encapsulation is not suitable, you can use IPIP instead:\n\n    .. parsed-literal::\n\n        helm install cilium |CHART_RELEASE| \\\\\n            --namespace kube-system \\\\\n            --set routingMode=native \\\\\n            --set kubeProxyReplacement=true \\\\\n            --set loadBalancer.mode=snat \\\\\n            --set loadBalancer.dsrDispatch=ipip \\\\\n            --set bpf.lbModeAnnotation=true \\\\\n            --set k8sServiceHost=${API_SERVER_IP} \\\\\n            --set k8sServicePort=${API_SERVER_PORT}\n\nAnnotation-based Load Balancing Algorithm Selection\n***************************************************\n\nCilium has the ability to specify the load balancing algorithm on a per-service\nbasis through the ``service.cilium.io/lb-algorithm`` annotation. Setting\n``bpf.lbAlgorithmAnnotation=true`` opts into this ability for the BPF and\ncorresponding agent code. A typical use-case is to reduce the memory footprint\nwhich comes with Maglev given the latter requires large lookup tables for each\nservice. Thus, if not all services need consistent hashing, then these can\nfallback to a random selection instead.\n\nBy default, if no service annotation is provided, the logic falls back to use\nwhichever method was specified globally through ``loadBalancer.algorithm``. The\nlatter supports either ``random`` or ``maglev`` as values today with ``random``\nbeing the default if ``loadBalancer.algorithm`` was not explicitly set via Helm.\n\nTo add a new service which must use ``random`` as its load balancing algorithm:\n\n.. code-block:: yaml\n\n  apiVersion: v1\n  kind: Service\n  metadata:\n    name: example-service\n    annotations:\n      service.cilium.io/lb-algorithm: random\n  spec:\n    selector:\n      app: example\n    ports:\n      - port: 8765\n        targetPort: 9376\n    type: LoadBalancer\n\nSimilarly, for opting into ``maglev``, use the following:\n\n .. code-block:: yaml\n\n  apiVersion: v1\n  kind: Service\n  metadata:\n    name: example-service\n    annotations:\n      service.cilium.io/lb-algorithm: maglev\n  spec:\n    selector:\n      app: example\n    ports:\n      - port: 8765\n        targetPort: 9376\n    type: LoadBalancer\n\nAll north-south traffic is now subsequently subject to ``maglev``-based load\nbalancing for the latter example.\n\nNote that ``service.cilium.io/lb-algorithm`` only takes effect upon initial\nservice creation and cannot be changed during the lifetime of the given\nKubernetes service. Switching between load balancing algorithms requires\nrecreation of a service.\n\n.. _socketlb-host-netns-only:\n\nSocket LoadBalancer Bypass in Pod Namespace\n*******************************************\n\nThe socket-level loadbalancer acts transparent to Cilium's lower layer datapath\nin that upon ``connect`` (TCP, connected UDP), ``sendmsg`` (UDP), or ``recvmsg``\n(UDP) system calls, the destination IP is checked for an existing service IP and\none of the service backends is selected as a target. This means that although\nthe application assumes it is connected to the service address, the\ncorresponding kernel socket is actually connected to the backend address and\ntherefore no additional lower layer NAT is required.\n\nCilium has built-in support for bypassing the socket-level loadbalancer and falling back\nto the tc loadbalancer at the veth interface when a custom redirection/operation relies\non the original ClusterIP within pod namespace (e.g., Istio sidecar) or due to the Pod's\nnature the socket-level loadbalancer is ineffective (e.g., KubeVirt, Kata Containers,\ngVisor).\n\nSetting ``socketLB.hostNamespaceOnly=true`` enables this bypassing mode. When enabled,\nthis circumvents socket rewrite in the ``connect()`` and ``sendmsg()`` syscall bpf hook and\nwill pass the original packet to next stage of operation (e.g., stack in\n``per-endpoint-routing`` mode) and re-enables service lookup in the tc bpf program.\n\nA Helm example configuration in a kube-proxy-free environment with socket LB bypass\nlooks as follows:\n\n.. parsed-literal::\n\n    helm install cilium |CHART_RELEASE| \\\\\n        --namespace kube-system \\\\\n        --set routingMode=native \\\\\n        --set kubeProxyReplacement=true \\\\\n        --set socketLB.hostNamespaceOnly=true\n\n.. _XDP acceleration:\n\nLoadBalancer & NodePort XDP Acceleration\n****************************************\n\nCilium has built-in support for accelerating NodePort, LoadBalancer services and\nservices with externalIPs for the case where the arriving request needs to be\nforwarded and the backend is located on a remote node. This feature was introduced\nin Cilium version `1.8  `_ at\nthe XDP (eXpress Data Path) layer where eBPF is operating directly in the networking\ndriver instead of a higher layer.\n\nSetting ``loadBalancer.acceleration`` to option ``native`` enables this acceleration.\nThe option ``disabled`` is the default and disables the acceleration. The majority\nof drivers supporting 10G or higher rates also support ``native`` XDP on a recent\nkernel. For cloud based deployments most of these drivers have SR-IOV variants that\nsupport native XDP as well. For on-prem deployments the Cilium XDP acceleration can\nbe used in combination with LoadBalancer service implementations for Kubernetes such\nas `MetalLB  `_. The acceleration can be enabled only\non a single device which is used for direct routing.\n\nFor high-scale environments, also consider tweaking the default map sizes to a larger\nnumber of entries e.g. through setting a higher ``config.bpfMapDynamicSizeRatio``.\nSee :ref:`bpf_map_limitations` for further details.\n\nThe ``loadBalancer.acceleration`` setting is supported for DSR, SNAT and hybrid\nmodes and can be enabled as follows for ``loadBalancer.mode=hybrid`` in this example:\n\n.. parsed-literal::\n\n    helm install cilium |CHART_RELEASE| \\\\\n        --namespace kube-system \\\\\n        --set routingMode=native \\\\\n        --set kubeProxyReplacement=true \\\\\n        --set loadBalancer.acceleration=native \\\\\n        --set loadBalancer.mode=hybrid \\\\\n        --set k8sServiceHost=${API_SERVER_IP} \\\\\n        --set k8sServicePort=${API_SERVER_PORT}\n\n\nIn case of a multi-device environment, where Cilium's device auto-detection selects\nmore than a single device to expose NodePort or a user specifies multiple devices\nwith ``devices``, the XDP acceleration is enabled on all devices. This means that\neach underlying device's driver must have native XDP support on all Cilium managed\nnodes. If you have an environment where some devices support XDP but others do not\nyou can have XDP enabled on the supported devices by setting\n``loadBalancer.acceleration`` to ``best-effort``.\n\nA list of drivers supporting XDP can be found in :ref:`the XDP documentation `.\n\nThe current Cilium kube-proxy XDP acceleration mode can also be introspected through\nthe ``cilium-dbg status`` CLI command. If it has been enabled successfully, ``Native``\nis shown:\n\n.. code-block:: shell-session\n\n    $ kubectl -n kube-system exec ds/cilium -- cilium-dbg status --verbose | grep XDP\n      XDP Acceleration:    Native\n\nNote that packets which have been pushed back out of the device for NodePort handling\nright at the XDP layer are not visible in tcpdump since packet taps come at a much\nlater stage in the networking stack. Cilium's monitor command or metric counters can be used\ninstead for gaining visibility.\n\nNodePort XDP on AWS\n===================\n\nIn order to run with NodePort XDP on AWS, follow the instructions in the :ref:`k8s_install_quick`\nguide to set up an EKS cluster or use any other method of your preference to set up a\nKubernetes cluster.\n\nIf you are following the EKS guide, make sure to create a node group with SSH access, since\nwe need few additional setup steps as well as create a larger instance type which supports\nthe `Elastic Network Adapter  `__ (ena).\nAs an instance example, ``m5n.xlarge`` is used in the config ``nodegroup-config.yaml``:\n\n.. code-block:: yaml\n\n  apiVersion: eksctl.io/v1alpha5\n  kind: ClusterConfig\n\n  metadata:\n    name: test-cluster\n    region: us-west-2\n\n  nodeGroups:\n    - name: ng-1\n      instanceType: m5n.xlarge\n      desiredCapacity: 2\n      ssh:\n        allow: true\n      ## taint nodes so that application pods are\n      ## not scheduled/executed until Cilium is deployed.\n      ## Alternatively, see the note below.\n      taints:\n        - key: \"node.cilium.io/agent-not-ready\"\n          value: \"true\"\n          effect: \"NoExecute\"\n\n.. note::\n\n  Please make sure to read and understand the documentation page on :ref:`taint effects and unmanaged pods `.\n\nThe nodegroup is created with:\n\n.. code-block:: shell-session\n\n  $ eksctl create nodegroup -f nodegroup-config.yaml\n\nEach of the nodes need the ``kernel-ng`` and ``ethtool`` package installed. The former is\nneeded in order to run a sufficiently recent kernel for eBPF in general and native XDP\nsupport on the ena driver. The latter is needed to configure channel parameters for the NIC.\n\n.. code-block:: shell-session\n\n  $ IPS=$(kubectl get no -o jsonpath='{$.items[*].status.addresses[?(@.type==\"ExternalIP\")].address }{\"\\\\n\"}' | tr ' ' '\\\\n')\n\n  $ for ip in $IPS ; do ssh ec2-user@$ip \"sudo amazon-linux-extras install -y kernel-ng && sudo yum install -y ethtool && sudo reboot\"; done\n\nOnce the nodes come back up their kernel version should say ``5.4.58-27.104.amzn2.x86_64`` or\nsimilar through ``uname -r``. In order to run XDP on ena, make sure the driver version is at\nleast `2.2.8  `__.\nThe driver version can be inspected through ``ethtool -i eth0``. For the given kernel version\nthe driver version should be reported as ``2.2.10g``.\n\nBefore Cilium's XDP acceleration can be deployed, there are two settings needed on the\nnetwork adapter side, that is, MTU needs to be lowered in order to be able to operate\nwith XDP, and number of combined channels need to be adapted.\n\nThe default MTU is set to 9001 on the ena driver. Given XDP buffers are linear, they\noperate on a single page. A driver typically reserves some headroom for XDP as well\n(e.g. for encapsulation purpose), therefore, the highest possible MTU for XDP would\nbe 3498.\n\nIn terms of ena channels, the settings can be gathered via ``ethtool -l eth0``. For the\n``m5n.xlarge`` instance, the default output should look like::\n\n  Channel parameters for eth0:\n  Pre-set maximums:\n  RX:             0\n  TX:             0\n  Other:          0\n  Combined:       4\n  Current hardware settings:\n  RX:             0\n  TX:             0\n  Other:          0\n  Combined:       4\n\nIn order to use XDP the channels must be set to at most 1/2 of the value from\n``Combined`` above. Both, MTU and channel changes are applied as follows:\n\n.. code-block:: shell-session\n\n  $ for ip in $IPS ; do ssh ec2-user@$ip \"sudo ip link set dev eth0 mtu 3498\"; done\n  $ for ip in $IPS ; do ssh ec2-user@$ip \"sudo ethtool -L eth0 combined 2\"; done\n\nIn order to deploy Cilium, the Kubernetes API server IP and port is needed:\n\n.. code-block:: shell-session\n\n  $ export API_SERVER_IP=$(kubectl get ep kubernetes -o jsonpath='{$.subsets[0].addresses[0].ip}')\n  $ export API_SERVER_PORT=443\n\nFinally, the deployment can be upgraded and later rolled-out with the\n``loadBalancer.acceleration=native`` setting to enable XDP in Cilium:\n\n.. parsed-literal::\n\n  helm upgrade cilium |CHART_RELEASE| \\\\\n        --namespace kube-system \\\\\n        --reuse-values \\\\\n        --set kubeProxyReplacement=true \\\\\n        --set loadBalancer.acceleration=native \\\\\n        --set loadBalancer.mode=snat \\\\\n        --set k8sServiceHost=${API_SERVER_IP} \\\\\n        --set k8sServicePort=${API_SERVER_PORT}\n\n\nNodePort XDP on Azure\n=====================\n\nTo enable NodePort XDP on Azure AKS or a self-managed Kubernetes running on Azure, the virtual\nmachines running Kubernetes must have `Accelerated Networking\n `_\nenabled. In addition, the Linux kernel on the nodes must also have support for\nnative XDP in the ``hv_netvsc`` driver, which is available in kernel >= 5.6 and was backported to\nthe Azure Linux kernel in 5.4.0-1022.\n\nOn AKS, make sure to use the AKS Ubuntu 22.04 node image with Kubernetes version v1.26 which will\nprovide a Linux kernel with the necessary backports to the ``hv_netvsc`` driver. Please refer to the\ndocumentation on `how to configure an AKS cluster\n `_ for more details.\n\nTo enable accelerated networking when creating a virtual machine or\nvirtual machine scale set, pass the ``--accelerated-networking`` option to the\nAzure CLI. Please refer to the guide on how to `create a Linux virtual machine\nwith Accelerated Networking using Azure CLI\n `_\nfor more details.\n\nWhen *Accelerated Networking* is enabled, ``lspci`` will show a\nMellanox ConnectX NIC:\n\n.. code-block:: shell-session\n\n    $ lspci | grep Ethernet\n    2846:00:02.0 Ethernet controller: Mellanox Technologies MT27710 Family [ConnectX-4 Lx Virtual Function] (rev 80)\n\nXDP acceleration can only be enabled on NICs ConnectX-4 Lx and onwards.\n\nIn order to run XDP, large receive offload (LRO) needs to be disabled on the\n``hv_netvsc`` device. If not the case already, this can be achieved by:\n\n.. code-block:: shell-session\n\n   $ ethtool -K eth0 lro off\n\nIt is recommended to use Azure IPAM for the pod IP address allocation, which\nwill automatically configure your virtual network to route pod traffic correctly:\n\n.. parsed-literal::\n\n   helm install cilium |CHART_RELEASE| \\\\\n     --namespace kube-system \\\\\n     --set ipam.mode=azure \\\\\n     --set azure.enabled=true \\\\\n     --set azure.resourceGroup=$AZURE_NODE_RESOURCE_GROUP \\\\\n     --set azure.subscriptionID=$AZURE_SUBSCRIPTION_ID \\\\\n     --set azure.tenantID=$AZURE_TENANT_ID \\\\\n     --set azure.clientID=$AZURE_CLIENT_ID \\\\\n     --set azure.clientSecret=$AZURE_CLIENT_SECRET \\\\\n     --set routingMode=native \\\\\n     --set enableIPv4Masquerade=false \\\\\n     --set devices=eth0 \\\\\n     --set kubeProxyReplacement=true \\\\\n     --set loadBalancer.acceleration=native \\\\\n     --set loadBalancer.mode=snat \\\\\n     --set k8sServiceHost=${API_SERVER_IP} \\\\\n     --set k8sServicePort=${API_SERVER_PORT}\n\n\nWhen running Azure IPAM on a self-managed Kubernetes cluster, each ``v1.Node``\nmust have the resource ID of its VM in the ``spec.providerID`` field.\nRefer to the :ref:`ipam_azure` reference for more information.\n\nNodePort XDP on GCP\n===================\n\nNodePort XDP on the Google Cloud Platform is currently not supported. Both\nvirtual network interfaces available on Google Compute Engine (the older\nvirtIO-based interface and the newer `gVNIC\n `_) are\ncurrently lacking support for native XDP.\n\n.. _NodePort Devices:\n\nNodePort Devices, Port and Bind settings\n****************************************\n\nWhen running Cilium's eBPF kube-proxy replacement, by default, a NodePort or\nLoadBalancer service or a service with externalIPs will be accessible through\nthe IP addresses of native devices which have the default route on the host or\nhave Kubernetes InternalIP or ExternalIP assigned. InternalIP is preferred over\nExternalIP if both exist. To change the devices, set their names in the\n``devices`` Helm option, e.g. ``devices='{eth0,eth1,eth2}'``. Each\nlisted device has to be named the same on all Cilium managed nodes. Alternatively\nif the devices do not match across different nodes, the wildcard option can be\nused, e.g. ``devices=eth+``, which would match any device starting with prefix\n``eth``. If no device can be matched the Cilium agent will try to perform auto\ndetection.\n\nWhen multiple devices are used, only one device can be used for direct routing\nbetween Cilium nodes. By default, if a single device was detected or specified\nvia ``devices`` then Cilium will use that device for direct routing.\nOtherwise, Cilium will use a device with Kubernetes InternalIP or ExternalIP\nset. InternalIP is preferred over ExternalIP if both exist. To change\nthe direct routing device, set the ``nodePort.directRoutingDevice`` Helm\noption, e.g. ``nodePort.directRoutingDevice=eth1``. The wildcard option can be\nused as well as the devices option, e.g. ``directRoutingDevice=eth+``.\nIf more than one devices match the wildcard option, Cilium will sort them\nin increasing alphanumerical order and pick the first one. If the direct routing\ndevice does not exist within ``devices``, Cilium will add the device to the latter\nlist. The direct routing device is used for\n:ref:`the NodePort XDP acceleration ` as well (if enabled).\n\nIn addition, thanks to the socket-LB feature, the NodePort service can\nbe accessed by default from a host or a pod within a cluster via its public, any\nlocal (except for ``docker*`` prefixed names) or loopback address, e.g.\n``127.0.0.1:NODE_PORT``.\n\nIf ``kube-apiserver`` was configured to use a non-default NodePort port range,\nthen the same range must be passed to Cilium via the ``nodePort.range``\noption, for example, as ``nodePort.range=\"10000\\,32767\"`` for a\nrange of ``10000-32767``. The default Kubernetes NodePort range is ``30000-32767``.\n\nIf the NodePort port range overlaps with the ephemeral port range\n(``net.ipv4.ip_local_port_range``), Cilium will append the NodePort range to\nthe reserved ports (``net.ipv4.ip_local_reserved_ports``). This is needed to\nprevent a NodePort service from hijacking traffic of a host local application\nwhich source port matches the service port. To disable the modification of\nthe reserved ports, set ``nodePort.autoProtectPortRanges`` to ``false``.\n\nBy default, the NodePort implementation prevents application ``bind(2)`` requests\nto NodePort service ports. In such case, the application will typically see a\n``bind: Operation not permitted`` error. By default this happens only for the host\nnamespace and therefore does not affect any application pod ``bind(2)`` requests.\nIn order to opt-out from this behavior in general, this setting can be changed for\nexpert users by switching ``nodePort.bindProtection`` to ``false``.\n\n.. _Configuring Maps:\n\nConfiguring BPF Map Sizes\n*************************\n\nFor high-scale environments, Cilium's BPF maps can be configured to have higher\nlimits on the number of entries. Overriding Helm options can be used to tweak\nthese limits.\n\nTo increase the number of entries in Cilium's BPF LB service, backend and\naffinity maps consider overriding ``bpf.lbMapMax`` Helm option.\nThe default value of this LB map size is 65536.\n\n.. parsed-literal::\n\n    helm install cilium |CHART_RELEASE| \\\\\n        --namespace kube-system \\\\\n        --set kubeProxyReplacement=true \\\\\n        --set bpf.lbMapMax=131072\n\n.. _kubeproxyfree_hostport:\n\nContainer HostPort Support\n**************************\n\nAlthough not part of kube-proxy, Cilium's eBPF kube-proxy replacement also\nnatively supports ``hostPort`` service mapping without having to use the\nHelm CNI chaining option of ``cni.chainingMode=portmap``.\n\nBy specifying ``kubeProxyReplacement=true`` the native hostPort support is\nautomatically enabled and therefore no further action is required.\n\nIf the ``hostPort`` is specified without an additional ``hostIP``, then the\nPod will be exposed to the outside world with the same local addresses from\nthe node that were detected and used for exposing NodePort services, e.g.\nthe Kubernetes InternalIP or ExternalIP if set.\n\nAdditionally, the Pod is also accessible through the loopback address on the\nnode such as ``127.0.0.1:hostPort``. If in addition to ``hostPort`` also\na ``hostIP`` has been specified for the Pod, then the Pod will only be\nexposed on the given ``hostIP`` instead. A ``hostIP`` of ``0.0.0.0`` will\nhave the same behavior as if a ``hostIP`` was not specified.\n\nThe ``hostPort`` must not reside in the configured NodePort port range to\navoid collisions.\n\nNote that ``hostPort`` support relies on Cilium's eBPF kube-proxy replacement\nand in the background plumbs service entries to direct traffic to the local\nhost port backend. Given host port is not configured through a Kubernetes\nservice object, the full feature set of Kubernetes services (such as custom\nCilium service annotations) is not available. Instead, host port piggy-backs\non user-configured defaults of the service handling behavior.\n\nAn example deployment in a kube-proxy-free environment therefore is the same\nas in the earlier getting started deployment:\n\n.. parsed-literal::\n\n    helm install cilium |CHART_RELEASE| \\\\\n        --namespace kube-system \\\\\n        --set kubeProxyReplacement=true \\\\\n        --set k8sServiceHost=${API_SERVER_IP} \\\\\n        --set k8sServicePort=${API_SERVER_PORT}\n\nAlso, ensure that each node IP is known via ``INTERNAL-IP`` or ``EXTERNAL-IP``,\nfor example:\n\n.. code-block:: shell-session\n\n    $ kubectl get nodes -o wide\n    NAME   STATUS   ROLES    AGE     VERSION   INTERNAL-IP      EXTERNAL-IP   [...]\n    apoc   Ready    master   6h15m   v1.17.3   192.168.178.29            [...]\n    tank   Ready        6h13m   v1.17.3   192.168.178.28            [...]\n\nIf this is not the case, then ``kubelet`` needs to be made aware of it through\nspecifying ``--node-ip`` through ``KUBELET_EXTRA_ARGS``. Assuming ``eth0`` is\nthe public facing interface, this can be achieved by:\n\n.. code-block:: shell-session\n\n    $ echo KUBELET_EXTRA_ARGS=\\\"--node-ip=$(ip -4 -o a show eth0 | awk '{print $4}' | cut -d/ -f1)\\\" | tee -a /etc/default/kubelet\n\nAfter updating ``/etc/default/kubelet``, kubelet needs to be restarted.\n\nIn order to verify whether the HostPort feature has been enabled in Cilium, the\n``cilium-dbg status`` CLI command provides visibility through the ``KubeProxyReplacement``\ninfo line. If it has been enabled successfully, ``HostPort`` is shown as ``Enabled``,\nfor example:\n\n.. code-block:: shell-session\n\n    $ kubectl -n kube-system exec ds/cilium -- cilium-dbg status --verbose | grep HostPort\n      - HostPort:       Enabled\n\nThe following modified example yaml from the setup validation with an additional\n``hostPort: 8080`` parameter can be used to verify the mapping:\n\n.. code-block:: yaml\n\n    apiVersion: apps/v1\n    kind: Deployment\n    metadata:\n      name: my-nginx\n    spec:\n      selector:\n        matchLabels:\n          run: my-nginx\n      replicas: 1\n      template:\n        metadata:\n          labels:\n            run: my-nginx\n        spec:\n          containers:\n          - name: my-nginx\n            image: nginx\n            ports:\n            - containerPort: 80\n              hostPort: 8080\n\nAfter deployment, we can validate that Cilium's eBPF kube-proxy replacement\nexposed the container as HostPort under the specified port ``8080``:\n\n.. code-block:: shell-session\n\n    $ kubectl exec -it -n kube-system cilium-fmh8d -- cilium-dbg service list\n    ID   Frontend               Service Type   Backend\n    [...]\n    5    192.168.178.29:8080    HostPort       1 => 10.29.207.199:80\n\nSimilarly, we can inspect through ``iptables`` in the host namespace that\nno ``iptables`` rule for the HostPort service is present:\n\n.. code-block:: shell-session\n\n    $ iptables-save | grep HOSTPORT\n    [ empty line ]\n\nLast but not least, a simple ``curl`` test shows connectivity for the\nexposed HostPort container under the node's IP:\n\n.. code-block:: shell-session\n\n    $ curl 192.168.178.29:8080\n    \n    \n    \n     Welcome to nginx! \n    [....]\n\nRemoving the deployment also removes the corresponding HostPort from\nthe ``cilium-dbg service list`` dump:\n\n.. code-block:: shell-session\n\n    $ kubectl delete deployment my-nginx\n\nGraceful Termination\n********************\n\nCilium's eBPF kube-proxy replacement supports graceful termination of service\nendpoint pods. The Cilium agent detects such terminating Pod events, and\nincrements the metric ``k8s_terminating_endpoints_events_total``.\n\nWhen Cilium agent receives a Kubernetes update event that marks an endpoint as\nterminating Cilium will retain the datapath state necessary for existing connections.\nThe terminating endpoint will be used as fallback for new connections only if\n1) no active endpoints exist for the service and 2) terminating endpoint has condition ``serving``\n(e.g. pod is still passing `readinessProbes  `_).\n\nIf ``publishNotReadyAddresses`` is set on the Service the endpoints received by Cilium\nmay have both the ``ready`` and ``terminating`` conditions set. In this case Cilium follows\nkube-proxy and uses these for new connections, ignoring the ``terminating`` condition.\n\nThe endpoint state is fully removed when the agent receives a Kubernetes delete\nevent for the endpoint. The `Kubernetes pod termination  `_\ndocumentation contains more background on the behavior and configuration using ``terminationGracePeriodSeconds``.\nThere are some special cases, like zero disruption during rolling updates, that require to be able to send traffic\nto Terminating Pods that are still Serving traffic during the Terminating period, the Kubernetes blog\n`Advancements in Kubernetes Traffic Engineering\n `_\nexplains it in detail.\n\n.. admonition:: Video\n  :class: attention\n\n  To learn more about Cilium's graceful termination support, check out `eCHO Episode 49: Graceful Termination Support with Cilium 1.11  `__.\n\n.. _session-affinity:\n\nSession Affinity\n****************\n\nCilium's eBPF kube-proxy replacement supports Kubernetes service session affinity.\nEach connection from the same pod or host to a service configured with\n``sessionAffinity: ClientIP`` will always select the same service endpoint.\nThe default timeout for the affinity is three hours (updated by each request to\nthe service), but it can be configured through Kubernetes' ``sessionAffinityConfig``\nif needed.\n\nThe source for the affinity depends on the origin of a request. If a request is\nsent from outside the cluster to the service, the request's source IP address is\nused for determining the endpoint affinity. If a request is sent from inside\nthe cluster, then the source depends on whether the socket-LB feature\nis used to load balance ClusterIP services. If yes, then the client's network\nnamespace cookie is used as the source - it allows to implement affinity at the\nsocket layer at which the socket-LB operates (a source IP is not available there,\nas the endpoint selection happens before a network packet has been built by the\nkernel). If the socket-LB is not used (i.e. the loadbalancing is done\nat the pod network interface, on a per-packet basis), then the request's source\nIP address is used as the source.\n\nThe session affinity support is enabled by default. To disable the feature,\nset ``config.sessionAffinity=false``.\n\nThe session affinity of a service with multiple ports is per service IP and port.\nMeaning that all requests for a given service sent from the same source and to the\nsame service port will be routed to the same service endpoints; but two requests\nfor the same service, sent from the same source but to different service ports may\nbe routed to distinct service endpoints.\n\nNote that if the session affinity feature is used in combination with Maglev\nconsistent hashing to select backends, then Maglev will not take the source\nport as input for its hashing in order to respect the user's ClientIP choice\n(see also `GH#26709  `__ for\nfurther details).\n\nkube-proxy Replacement Health Check server\n******************************************\nTo enable health check server for the kube-proxy replacement, the\n``kubeProxyReplacementHealthzBindAddr`` option has to be set (disabled by\ndefault). The option accepts the IP address with port for the health check server\nto serve on.\nE.g. to enable for IPv4 interfaces set ``kubeProxyReplacementHealthzBindAddr='0.0.0.0:10256'``,\nfor IPv6 - ``kubeProxyReplacementHealthzBindAddr='[::]:10256'``. The health check server is\naccessible via the HTTP ``/healthz`` endpoint.\n\nLoadBalancer Source Ranges Checks\n*********************************\n\nWhen a ``LoadBalancer`` service is configured with ``spec.loadBalancerSourceRanges``,\nCilium's eBPF kube-proxy replacement restricts access from outside (e.g. external\nworld traffic) to the service to the white-listed CIDRs specified in the field. If\nthe field is empty, no restrictions for the access will be applied.\n\nWhen accessing the service from inside a cluster, the kube-proxy replacement will\nignore the field regardless whether it is set. This means that any pod or any host\nprocess in the cluster will be able to access the ``LoadBalancer`` service internally.\n\nBy default the specified white-listed CIDRs in ``spec.loadBalancerSourceRanges``\nonly apply to the ``LoadBalancer`` service, but not the corresponding ``NodePort``\nor ``ClusterIP`` service which get installed along with the ``LoadBalancer`` service.\n\nIf this behavior is not desired, then there are two options available: One possibility\nis to avoid the creation of corresponding ``NodePort`` and ``ClusterIP`` services via\n``service.cilium.io/type`` annotation:\n\n.. code-block:: yaml\n\n  apiVersion: v1\n  kind: Service\n  metadata:\n    name: example-service\n    annotations:\n      service.cilium.io/type: LoadBalancer\n  spec:\n    ports:\n      - port: 80\n        targetPort: 80\n    type: LoadBalancer\n    loadBalancerSourceRanges:\n    - 192.168.1.0/24\n\nThe other possibility is to propagate the white-listed CIDRs to all externally\nexposed service types. Meaning, ``NodePort`` as well as ``ClusterIP`` (if\nexternally accessible, see :ref:`External Access To ClusterIP Services  `\nsection) also filter traffic based on the source IP addresses.\nThis option can be enabled in Helm via ``bpf.lbSourceRangeAllTypes=true``.\n\nThe ``loadBalancerSourceRanges`` by default specifies an allow-list of CIDRs,\nmeaning, traffic originating not from those CIDRs is automatically dropped.\n\nCilium also supports the option to turn this list into a deny-list, in order\nto block traffic from certain CIDRs while allowing everything else. This\nbehavior can be achieved through the ``service.cilium.io/src-ranges-policy``\nannotation which accepts the values of ``allow`` or ``deny``.\n\nThe default ``loadBalancerSourceRanges`` behavior equals to\n``service.cilium.io/src-ranges-policy: allow``:\n\n.. code-block:: yaml\n\n  apiVersion: v1\n  kind: Service\n  metadata:\n    name: example-service\n    annotations:\n      service.cilium.io/type: LoadBalancer\n      service.cilium.io/src-ranges-policy: allow\n  spec:\n    ports:\n      - port: 80\n        targetPort: 80\n    type: LoadBalancer\n    loadBalancerSourceRanges:\n    - 192.168.1.0/24\n\nIn order to turn the CIDR list into a deny-list while allowing traffic not\noriginating from this set, this can be changed into ``service.cilium.io/src-ranges-policy: deny``:\n\n.. code-block:: yaml\n\n  apiVersion: v1\n  kind: Service\n  metadata:\n    name: example-service\n    annotations:\n      service.cilium.io/type: LoadBalancer\n      service.cilium.io/src-ranges-policy: deny\n  spec:\n    ports:\n      - port: 80\n        targetPort: 80\n    type: LoadBalancer\n    loadBalancerSourceRanges:\n    - 192.168.1.0/24\n\nService Proxy Name Configuration\n********************************\n\nLike kube-proxy, Cilium also honors the ``service.kubernetes.io/service-proxy-name`` service annotation\nand only manages services that contain a matching service-proxy-name label. This name can be configured\nby setting ``k8s.serviceProxyName`` option and the behavior is identical to that of\nkube-proxy. The service proxy name defaults to an empty string which instructs Cilium to\nonly manage services not having ``service.kubernetes.io/service-proxy-name`` label.\n\nFor more details on the usage of ``service.kubernetes.io/service-proxy-name`` label and its\nworking, take a look at `this KEP\n `__.\n\n.. note::\n\n    If Cilium with a non-empty service proxy name is meant to manage all services in kube-proxy\n    free mode, make sure that default Kubernetes services like ``kube-dns`` and ``kubernetes``\n    have the required label value.\n\nTraffic Distribution and Topology Aware Hints\n*********************************************\n\nThe kube-proxy replacement implements both Kubernetes `Topology Aware Routing\n `__,\nand the more recent `Traffic Distribution\n `__\nfeatures.\n\nBoth of these features work by setting ``hints`` on EndpointSlices that enable\nCilium to route to endpoints residing in the same zone. To enable the feature,\nset ``loadBalancer.serviceTopology=true``.\n\nNeighbor Discovery\n******************\n\nWhen kube-proxy replacement and XDP acceleration are enabled, Cilium does L2 neighbor discovery \nof nodes and service backends in the cluster. This is required for the service load-balancing \nto populate L2 addresses for backends since it is not possible to dynamically resolve neighbors\non demand in the fast-path.\n\nL2 neighbor discovery is automatically enabled when the agent detects that XDP is in use, but\ncan also be manually turned on by setting the ``--enable-l2-neigh-discovery=true`` flag or\n``l2NeighDiscovery.enabled=true`` Helm option.\n\nThe agent fully relies on the Linux kernel to discover gateways or hosts on the same L2 network. \nBoth IPv4 and IPv6 neighbor discovery is supported in the Cilium agent. As per our\nkernel work `presented at Plumbers  `__,\n\"managed\" neighbor entries have been `upstreamed  `__\nand will be available in Linux kernel v5.16 or later which the Cilium agent will detect\nand transparently use. In this case, the agent pushes down L3 addresses of new nodes\njoining the cluster as externally learned \"managed\" neighbor entries. For introspection,\niproute2 displays them as \"managed extern_learn\". The ``extern_learn`` attribute prevents\ngarbage collection of the entries by the kernel's neighboring subsystem. Such \"managed\"\nneighbor entries are dynamically resolved and periodically refreshed by the Linux kernel\nitself in case there is no active traffic for a certain period of time. That is, the\nkernel attempts to always keep them in ``REACHABLE`` state. For Linux kernels v5.15 or\nearlier where \"managed\" neighbor entries are not present, the Cilium agent similarly\npushes L3 addresses of new nodes into the kernel for dynamic resolution. For introspection, \niproute2 displays them only as ``extern_learn`` in this case. If there is no active traffic \nfor a certain period of time and entries become state, the Cilium agent triggers the \nLinux kernel-based re-resolution for attempting to keep them in ``REACHABLE`` state.\n\nThe Cilium agent actively monitors devices, routes, and neighbors and reconciles the\nneighbor entries in the kernel. For example if a device is added new neighbor entries \nfor the device are added. When routes change, such as a change to the next-hop, the \nCilium agent updates the neighbor entries accordingly. And when neighbor entries\nare flushed due to for example a carrier-down event, the Cilium agent restores the \nneighbor entries as soon as possible.\n\nThe neighbor discovery supports multi-device environments where each node has multiple devices\nand multiple next-hops to another node. The Cilium agent pushes neighbor entries for all target\ndevices, including the direct routing device. Currently, it supports one next-hop per device.\nThe following example illustrates how the neighbor discovery works in a multi-device environment.\nEach node has two devices connected to different L3 networks (10.69.0.64/26 and 10.69.0.128/26),\nand global scope addresses each (10.69.0.1/26 and 10.69.0.2/26). A next-hop from node1 to node2 is\neither ``10.69.0.66 dev eno1`` or ``10.69.0.130 dev eno2``. The Cilium agent pushes neighbor\nentries for both ``10.69.0.66 dev eno1`` and ``10.69.0.130 dev eno2`` in this case.\n\n::\n\n    +---------------+     +---------------+\n    |    node1      |     |    node2      |\n    | 10.69.0.1/26  |     | 10.69.0.2/26  |\n    |           eno1+-----+eno1           |\n    |           |   |     |   |           |\n    | 10.69.0.65/26 |     |10.69.0.66/26  |\n    |               |     |               |\n    |           eno2+-----+eno2           |\n    |           |   |     | |             |\n    | 10.69.0.129/26|     | 10.69.0.130/26|\n    +---------------+     +---------------+\n\nWith, on node1:\n\n.. code-block:: shell-session\n\n    $ ip route show\n    10.69.0.2\n            nexthop via 10.69.0.66 dev eno1 weight 1\n            nexthop via 10.69.0.130 dev eno2 weight 1\n\n    $ ip neigh show\n    10.69.0.66 dev eno1 lladdr 96:eb:75:fd:89:fd extern_learn  REACHABLE\n    10.69.0.130 dev eno2 lladdr 52:54:00:a6:62:56 extern_learn  REACHABLE\n\n.. _external_access_to_clusterip_services:\n\nExternal Access To ClusterIP Services\n*************************************\n\nAs per `k8s Service  `__,\nCilium's eBPF kube-proxy replacement by default disallows access to a ClusterIP service from outside the cluster.\nThis can be allowed by setting ``bpf.lbExternalClusterIP=true``.\n\nKubernetes API server high availability\n***************************************\n\nIf you are running multiple instances of Kubernetes API servers in your cluster, you can set the ``k8s-api-server-urls`` flag\nso that Cilium can fail over to an active instance. Cilium switches to the ``kubernetes`` service address so that\nAPI requests are load-balanced to API server endpoints during runtime. However, if the initially configured API servers\nare rotated while the agent is down, you can update the ``k8s-api-server-urls`` flag with the updated API servers.\n\n.. parsed-literal::\n\n    helm install cilium |CHART_RELEASE| \\\\\n        --namespace kube-system \\\\\n        --set kubeProxyReplacement=true \\\\\n        --set k8s.apiServerURLs=\"https://172.21.0.4:6443 https://172.21.0.5:6443 https://172.21.0.6:6443\"\n\nObservability\n*************\n\nYou can trace socket LB related datapath events using Hubble and cilium monitor.\n\nApply the following pod and service:\n\n.. code-block:: yaml\n\n    apiVersion: v1\n    kind: Pod\n    metadata:\n      name: nginx\n      labels:\n        app: proxy\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:stable\n        ports:\n          - containerPort: 80\n    ---\n    apiVersion: v1\n    kind: Service\n    metadata:\n      name: nginx-service\n    spec:\n      selector:\n        app: proxy\n      ports:\n      - port: 80\n\nDeploy a client pod to start traffic.\n\n.. parsed-literal::\n\n    $ kubectl create -f \\ |SCM_WEB|\\/examples/kubernetes-dns/dns-sw-app.yaml\n\n.. code-block:: shell-session\n\n    $ kubectl get svc | grep nginx\n      nginx-service   ClusterIP   10.96.128.44            80/TCP    140m\n\n    $ kubectl exec -it mediabot -- curl -v --connect-timeout 5 10.96.128.44\n\nFollow the Hubble :ref:`hubble_cli` guide  to see the network flows. The Hubble\noutput prints datapath events before and after socket LB translation between service\nand selected service endpoint.\n\n.. code-block:: shell-session\n\n    $ hubble observe --all | grep mediabot\n    Jan 13 13:47:20.932: default/mediabot (ID:5618) <> default/nginx-service:80 (world) pre-xlate-fwd TRACED (TCP)\n    Jan 13 13:47:20.932: default/mediabot (ID:5618) <> default/nginx:80 (ID:35772) post-xlate-fwd TRANSLATED (TCP)\n    Jan 13 13:47:20.932: default/nginx:80 (ID:35772) <> default/mediabot (ID:5618) pre-xlate-rev TRACED (TCP)\n    Jan 13 13:47:20.932: default/nginx-service:80 (world) <> default/mediabot (ID:5618) post-xlate-rev TRANSLATED (TCP)\n    Jan 13 13:47:20.932: default/mediabot:38750 (ID:5618) <> default/nginx (ID:35772) pre-xlate-rev TRACED (TCP)\n\nSocket LB tracing with Hubble requires cilium agent to detect pod cgroup paths.\nIf you see a message in cilium agent ``Failed to setup socket load-balancing tracing with Hubble.``,\nyou can trace packets using ``cilium-dbg monitor`` instead.\n\n.. note::\n\n    If you observe the message about socket load-balancing setup failure in the logs,\n    please file a GitHub issue with the cgroup path for any of your pods,\n    obtained by running the following command on a Kubernetes node in your\n    cluster: ``sudo crictl inspectp -o=json $POD_ID | grep cgroup``.\n\n.. code-block:: shell-session\n\n    $ kubectl get pods -o wide\n    NAME       READY   STATUS    RESTARTS   AGE     IP             NODE          NOMINATED NODE   READINESS GATES\n    mediabot   1/1     Running   0          54m     10.244.1.237   kind-worker                \n    nginx      1/1     Running   0          3h25m   10.244.1.246   kind-worker                \n\n    $ kubectl exec -n kube-system cilium-rt2jh -- cilium-dbg monitor -v -t trace-sock\n    CPU 11: [pre-xlate-fwd] cgroup_id: 479586 sock_cookie: 7123674, dst [10.96.128.44]:80 tcp\n    CPU 11: [post-xlate-fwd] cgroup_id: 479586 sock_cookie: 7123674, dst [10.244.1.246]:80 tcp\n    CPU 11: [pre-xlate-rev] cgroup_id: 479586 sock_cookie: 7123674, dst [10.244.1.246]:80 tcp\n    CPU 11: [post-xlate-rev] cgroup_id: 479586 sock_cookie: 7123674, dst [10.96.128.44]:80 tcp\n\nYou can identify the client pod using its printed ``cgroup id`` metadata. The pod\n``cgroup path`` corresponding to the ``cgroup id`` has its UUID. The socket\ncookie is a unique socket identifier allocated in the Linux kernel. The socket\ncookie metadata can be used to identify all the trace events from a socket.\n\n.. code-block:: shell-session\n\n    $ kubectl get pods -o custom-columns=PodName:.metadata.name,PodUID:.metadata.uid\n    PodName    PodUID\n    mediabot   b620703c-c446-49c7-84c8-e23f4ba5626b\n    nginx      73b9938b-7e4b-4cbd-8c4c-67d4f253ccf4\n\n    $ kubectl exec -n kube-system cilium-rt2jh -- find /run/cilium/cgroupv2/ -inum 479586\n    Defaulted container \"cilium-agent\" out of: cilium-agent, mount-cgroup (init), apply-sysctl-overwrites (init), clean-cilium-state (init)\n    /run/cilium/cgroupv2/kubelet.slice/kubelet-kubepods.slice/kubelet-kubepods-besteffort.slice/kubelet-kubepods-besteffort-podb620703c_c446_49c7_84c8_e23f4ba5626b.slice/cri-containerd-4e7fc71c8bef8c05c9fb76d93a186736fca266e668722e1239fe64503b3e80d3.scope\n\nTroubleshooting\n***************\n\nValidate BPF cgroup programs attachment\n=======================================\n\nCilium attaches BPF ``cgroup`` programs to enable socket-based load-balancing (aka\n``host-reachable`` services). If you see connectivity issues for ``clusterIP`` services,\ncheck if the programs are attached to the host ``cgroup root``. The default ``cgroup``\nroot is set to ``/run/cilium/cgroupv2``.\nRun the following commands from a Cilium agent pod as well as the underlying\nkubernetes node where the pod is running. If the container runtime in your cluster\nis running in the cgroup namespace mode, Cilium agent pod can attach BPF ``cgroup``\nprograms to the ``virtualized cgroup root``. In such cases, Cilium kube-proxy replacement\nbased load-balancing may not be effective leading to connectivity issues.\nFor more information, ensure that you have the fix `Pull Request  `__.\n\n.. code-block:: shell-session\n\n    $ mount | grep cgroup2\n    none on /run/cilium/cgroupv2 type cgroup2 (rw,relatime)\n\n    $ bpftool cgroup tree /run/cilium/cgroupv2/\n    CgroupPath\n    ID       AttachType      AttachFlags     Name\n    /run/cilium/cgroupv2\n    10613    device          multi\n    48497    connect4\n    48493    connect6\n    48499    sendmsg4\n    48495    sendmsg6\n    48500    recvmsg4\n    48496    recvmsg6\n    48498    getpeername4\n    48494    getpeername6\n\nKnown Issues\n############\n\nFor clusters deployed with Cilium version 1.11.14 or earlier, service backend entries could\nbe leaked in the BPF maps in some instances. The known cases that could lead\nto such leaks are due to race conditions between deletion of a service backend\nwhile it's terminating, and simultaneous deletion of the service the backend is\nassociated with. This could lead to duplicate backend entries that could eventually\nfill up the ``cilium_lb4_backends_v2`` map.\nIn such cases, you might see error messages like these in the Cilium agent logs::\n\n    Unable to update element for cilium_lb4_backends_v2 map with file descriptor 15: the map is full, please consider resizing it. argument list too long\n\nWhile the leak was fixed in Cilium version 1.11.15, in some cases, any affected clusters upgrading\nfrom the problematic cilium versions 1.11.14 or earlier to any subsequent versions may not\nsee the leaked backends cleaned up from the BPF maps after the Cilium agent restarts.\nThe fixes to clean up leaked duplicate backend entries were backported to older\nreleases, and are available as part of Cilium versions v1.11.16, v1.12.9 and v1.13.2.\nFresh clusters deploying Cilium versions 1.11.15 or later don't experience this leak issue.\n\nFor more information, see `this GitHub issue  `__.\n\nLimitations\n###########\n\n    * Cilium's eBPF kube-proxy replacement relies upon the socket-LB feature\n      which uses eBPF cgroup hooks to implement the service translation. Using it with libceph\n      deployments currently requires support for the getpeername(2) hook address translation in\n      eBPF.\n    * NFS and SMB mounts may break when mounted to a ``Service`` cluster IP while using socket-LB.\n      This issue is known to impact Longhorn, Portworx, and Robin, but may impact other storage\n      systems that implement ``ReadWriteMany`` volumes using this pattern. To avoid this problem,\n      ensure that the following commits are part of your underlying kernel:\n\n      * ``0bdf399342c5 (\"net: Avoid address overwrite in kernel_connect\")``\n      * ``86a7e0b69bd5 (\"net: prevent rewrite of msg_name in sock_sendmsg()\")``\n      * ``01b2885d9415 (\"net: Save and restore msg_namelen in sock_sendmsg\")``\n      * ``cedc019b9f26 (\"smb: use kernel_connect() and kernel_bind()\")`` (SMB only)\n\n      These patches have been backported to all stable kernels and some distro-specific kernels:\n\n      * **Ubuntu**: ``5.4.0-187-generic``, ``5.15.0-113-generic``, ``6.5.0-41-generic`` or newer.\n      * **RHEL 8**: ``4.18.0-553.8.1.el8_10.x86_64`` or newer (RHEL 8.10+).\n      * **RHEL 9**: ``kernel-5.14.0-427.31.1.el9_4`` or newer (RHEL 9.4+).\n\n      For a more detailed discussion see :gh-issue:`21541`.\n    * Cilium's DSR NodePort mode currently does not operate well in environments with\n      TCP Fast Open (TFO) enabled. It is recommended to switch to ``snat`` mode in this\n      situation.\n    * Cilium's eBPF kube-proxy replacement does not support the SCTP transport protocol except\n      in a few basic cases. For more information, see :ref:`sctp`. Only TCP and UDP are fully \n      supported as a transport for services at this time.\n    * Cilium's eBPF kube-proxy replacement does not allow ``hostPort`` port configurations\n      for Pods that overlap with the configured NodePort range. In such case, the ``hostPort``\n      setting will be ignored and a warning emitted to the Cilium agent log. Similarly,\n      explicitly binding the ``hostIP`` to the loopback address in the host namespace is\n      currently not supported and will log a warning to the Cilium agent log.\n    * The neighbor discovery in a multi-device environment doesn't work with the runtime device\n      detection which means that the target devices for the neighbor discovery doesn't follow the\n      device changes.\n    * When socket-LB feature is enabled, pods sending (connected) UDP traffic to services\n      can continue to send traffic to a service backend even after it's deleted. Cilium agent\n      handles such scenarios by forcefully terminating application sockets that are connected\n      to deleted backends, so that the applications can be load-balanced to active backends.\n      This functionality requires these kernel configs to be enabled:\n      ``CONFIG_INET_DIAG``, ``CONFIG_INET_UDP_DIAG`` and ``CONFIG_INET_DIAG_DESTROY``.\n    * Cilium's BPF-based masquerading is recommended over iptables when using the\n      BPF-based NodePort. Otherwise, there is a risk for port collisions between\n      BPF and iptables SNAT, which might result in dropped NodePort\n      connections :gh-issue:`23604`.\n\nFurther Readings\n################\n\nThe following presentations describe inner-workings of the kube-proxy replacement in eBPF\nin great details:\n\n    * \"Liberating Kubernetes from kube-proxy and iptables\" (KubeCon North America 2019, `slides\n       `__,\n      `video  `__)\n    * \"Kubernetes service load-balancing at scale with BPF & XDP\" (Linux Plumbers 2020, `slides\n       `__,\n      `video  `__)\n    * \"eBPF as a revolutionary technology for the container landscape\" (Fosdem 2020, `slides\n       `__,\n      `video  `__)\n    * \"Kernel improvements for Cilium socket LB\" (LSF/MM/BPF 2020, `slides\n       `__)",
  "item_type": "unknown",
  "module_path": "/tmp/cilium-repo/Documentation/network/kubernetes/kubeproxy-free.rst",
  "extracted_at": "2025-09-03T01:13:29.235531Z"
}