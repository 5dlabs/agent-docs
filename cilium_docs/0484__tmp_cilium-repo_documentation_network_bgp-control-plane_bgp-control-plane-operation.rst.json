{
  "url": "file:///tmp/cilium-repo/Documentation/network/bgp-control-plane/bgp-control-plane-operation.rst",
  "content": ".. only:: not (epub or latex or html) \n WARNING: You are looking at unreleased Cilium documentation.\nPlease use the official rendered version released here:\nhttps://docs.cilium.io\n \n .. _bgp_control_plane_operation: \n BGP Control Plane Operation Guide\n################################# \n This document provides guidance on how to operate the BGP Control Plane. \n BGP Cilium CLI \n Installation \n \n.. include:: ../../installation/cli-download.rst\n\nCilium BGP state can be inspected via ``cilium bgp`` subcommand.\n\n.. code-block:: shell-session\n\n    # cilium bgp --help\n    Access to BGP control plane\n\n    Usage:\n      cilium bgp [command]\n\n    Available Commands:\n      peers       Lists BGP peering state\n      routes      Lists BGP routes\n\n    Flags:\n      -h, --help   help for bgp\n\n    Global Flags:\n          --context string             Kubernetes configuration context\n          --helm-release-name string   Helm release name (default \"cilium\")\n          --kubeconfig string          Path to the kubeconfig file\n      -n, --namespace string           Namespace Cilium is running in (default \"kube-system\")\n\n    Use \"cilium bgp [command] --help\" for more information about a command.\n\n\nPeers\n~~~~~\n\n``cilium bgp peers`` command displays current peering states from all nodes in the kubernetes\ncluster.\n\nIn the following example, peering status is displayed for two nodes in the cluster.\n\n.. code-block:: shell-session\n\n    # cilium bgp peers\n    Node                                     Local AS   Peer AS   Peer Address   Session State   Uptime   Family         Received   Advertised\n    bgpv2-cplane-dev-service-control-plane   65001      65000     fd00:10::1     established     33m26s   ipv4/unicast   2          2\n                                                                                                          ipv6/unicast   2          2\n    bgpv2-cplane-dev-service-worker          65001      65000     fd00:10::1     established     33m25s   ipv4/unicast   2          2\n                                                                                                          ipv6/unicast   2          2\n\n\nUsing this command, you can validate BGP session state is ``established`` and expected number\nof routes are being advertised to the peers.\n\nRoutes\n~~~~~~\n``cilium bgp routes`` command displays detailed information about local BGP routing table and per peer\nadvertised routing information.\n\nIn the following example, the local BGP routing table for IPv4/Unicast address family is shown for two nodes in the cluster.\n\n.. code-block:: shell-session\n\n    # cilium bgp routes available ipv4 unicast\n    Node                                     VRouter   Prefix        NextHop   Age      Attrs\n    bgpv2-cplane-dev-service-control-plane   65001     10.1.0.0/24   0.0.0.0   46m45s   [{Origin: i} {Nexthop: 0.0.0.0}]\n    bgpv2-cplane-dev-service-worker          65001     10.1.1.0/24   0.0.0.0   46m45s   [{Origin: i} {Nexthop: 0.0.0.0}]\n\nSimilarly, you can inspect per peer advertisements using following command.\n\n.. code-block:: shell-session\n\n    # cilium bgp routes advertised ipv4 unicast\n    Node                                     VRouter   Peer         Prefix        NextHop          Age     Attrs\n    bgpv2-cplane-dev-service-control-plane   65001     fd00:10::1   10.1.0.0/24   fd00:10:0:1::2   47m0s   [{Origin: i} {AsPath: 65001} {Communities: 65000:99} {MpReach(ipv4-unicast): {Nexthop: fd00:10:0:1::2, NLRIs: [10.1.0.0/24]}}]\n    bgpv2-cplane-dev-service-worker          65001     fd00:10::1   10.1.1.0/24   fd00:10:0:2::2   47m0s   [{Origin: i} {AsPath: 65001} {Communities: 65000:99} {MpReach(ipv4-unicast): {Nexthop: fd00:10:0:2::2, NLRIs: [10.1.1.0/24]}}]\n\n\nYou can validate the BGP attributes are advertised based on configured :ref:`CiliumBGPAdvertisement <bgp-adverts>` resources.\n\n\nPolicies\n~~~~~~~~\n\nCilium BGP installs GoBGP policies for managing per peer advertisement and BGP attributes. As this\nis an internal implementation detail, it is not exposed via Cilium CLI. However, for debugging purpose\nyou can inspect installed BGP policies using cilium-dbg CLI from the Cilium agent pod.\n\n.. code-block:: shell-session\n\n    /home/cilium# cilium-dbg bgp route-policies\n    VRouter   Policy Name          Type     Match Peers      Match Prefixes (Min..Max Len)   RIB Action   Path Actions\n    65001     65000-ipv4-PodCIDR   export   fd00:10::1/128   10.1.0.0/24 (24..24)            accept       AddCommunities: [65000:99]\n    65001     65000-ipv6-PodCIDR   export   fd00:10::1/128   fd00:10:1::/64 (64..64)         accept       AddCommunities: [65000:99]\n    65001     allow-local          import                                                    accept\n\nCiliumBGPClusterConfig Status\n=============================\n\nCiliumBGPClusterConfig may report some configuration errors in the\n``.status.conditions`` caught at runtime. Currently, the following conditions\nare defined.\n\n====================================== ===============================================\nCondition Name                         Description\n====================================== ===============================================\n``cilium.io/NoMatchingNode``           ``.spec.nodeSelector`` doesn't select any node.\n``cilium.io/MissingPeerConfigs``       The PeerConfig specified in the ``spec.bgpInstances[].peers[].peerConfigRef`` doesn't exist.\n``cilium.io/ConflictingClusterConfig`` There is an another CiliumBGPClusterConfig selecting the same node.\n====================================== ===============================================\n\nCiliumBGPPeerConfig Status\n==========================\n\nCiliumBGPPeerConfig may report some configuration errors in the\n``.status.conditions`` caught at runtime. Currently, the following conditions\nare defined.\n\n====================================== ===============================================\nCondition Name                         Description\n====================================== ===============================================\n``cilium.io/MissingAuthSecret``        The Secret specified in the ``.spec.authSecretRef`` doesn't exist.\n====================================== ===============================================\n\nCiliumBGPNodeConfig Status\n==========================\n\nEach Cilium node on which BGP control plane is enabled based on ``CiliumBGPClusterConfig`` node selector gets associated\n``CiliumBGPNodeConfig`` resource. ``CiliumBGPNodeConfig`` resource is the source of BGP configuration for the\nnode, it is managed by Cilium operator.\n\nStatus field of ``CiliumBGPNodeConfig`` maintains real-time BGP operational state. This can be used for\nautomation or monitoring purposes.\n\nIn the following example, you can see BGP instance state from node ``bgpv2-cplane-dev-service-worker``.\n\n.. code-block:: shell-session\n\n    # kubectl describe ciliumbgpnodeconfigs bgpv2-cplane-dev-service-worker\n    Name:         bgpv2-cplane-dev-service-worker\n    Namespace:\n    Labels:       <none>\n    Annotations:  <none>\n    API Version:  cilium.io/v2\n    Kind:         CiliumBGPNodeConfig\n    Metadata:\n      Creation Timestamp:  2024-10-17T13:59:44Z\n      Generation:          1\n      Owner References:\n        API Version:     cilium.io/v2\n        Kind:            CiliumBGPClusterConfig\n        Name:            cilium-bgp\n        UID:             f0c23da8-e5ca-40d7-8c94-91699cf1e03a\n      Resource Version:  1385\n      UID:               fc88be94-37e9-498a-b9f7-a52684090d80\n    Spec:\n      Bgp Instances:\n        Local ASN:  65001\n        Name:       65001\n        Peers:\n          Name:          65000\n          Peer ASN:      65000\n          Peer Address:  fd00:10::1\n          Peer Config Ref:\n            Group:  cilium.io\n            Kind:   CiliumBGPPeerConfig\n            Name:   cilium-peer\n    Status:\n      Bgp Instances:\n        Local ASN:  65001\n        Name:       65001\n        Peers:\n          Established Time:  2024-10-17T13:59:50Z\n          Name:              65000\n          Peer ASN:          65000\n          Peer Address:      fd00:10::1\n          Peering State:     established\n          Route Count:\n            Advertised:  2\n            Afi:         ipv4\n            Received:    2\n            Safi:        unicast\n            Advertised:  2\n            Afi:         ipv6\n            Received:    2\n            Safi:        unicast\n          Timers:\n            Applied Hold Time Seconds:  90\n            Applied Keepalive Seconds:  30\n    Events:                             <none>\n\nDisabling CRD Status Report\n===========================\n\nCRD status reporting is useful for troubleshooting, making it useful to enable\nin general. However, for large clusters with a lot of nodes or BGP policies,\nCRD status reporting may add a significant API server load. To disable status\nreporting, set the ``bgpControlPlane.statusReport.enabled`` Helm value to\n``false``. Doing so disables status reporting and clears the currently reported\nstatus.\n\nLogs\n====\n\nBGP Control Plane logs can be found in the Cilium operator (only for BGPv2) and the Cilium agent logs.\n\nThe operator logs are tagged with ``subsys=bgp-cp-operator``. You can use this tag to filter\nthe logs as in the following example:\n\n.. code-block:: shell-session\n\n   kubectl -n kube-system logs <cilium operator pod name> | grep \"subsys=bgp-cp-operator\"\n\nThe agent logs are tagged with ``subsys=bgp-control-plane``. You can use this tag to filter\nthe logs as in the following example:\n\n.. code-block:: shell-session\n\n   kubectl -n kube-system logs <cilium agent pod name> | grep \"subsys=bgp-control-plane\"\n\nMetrics\n=======\n\nMetrics exposed by BGP Control Plane are listed in the :ref:`metrics document\n<metrics_bgp_control_plane>`.\n\n.. _bgp_control_plane_agent_restart:\n\nRestarting an Agent\n===================\n\nWhen you restart the Cilium agent, the BGP session will be lost because the BGP\nspeaker is integrated within the Cilium agent. The BGP session will be restored\nonce the Cilium agent is restarted. However, while the Cilium agent is down,\nthe advertised routes will be removed from the BGP peer. As a result, you may\ntemporarily lose connectivity to the Pods or Services. You can enable the\n:ref:`Graceful Restart <bgp_control_plane_graceful_restart>` to continue\nforwarding traffic to the Pods or Services during the agent restart.\n\nUpgrading or Downgrading Cilium\n===============================\n\nWhen you upgrade or downgrade Cilium, you must restart the Cilium agent. For\nmore details about the agent restart, see\n:ref:`bgp_control_plane_agent_restart` section.\n\nNote that with BGP Control Plane, it's especially important to pre-pull the\nagent image by following the :ref:`preflight process <pre_flight>` before\nupgrading Cilium. Image pull is time-consuming and error-prone because it\ninvolves network communication. If the image pull takes longer, it may exceed\nthe Graceful Restart time (``restartTimeSeconds``) and cause the BGP peer to\nwithdraw routes.\n\n.. _bgp_control_plane_node_shutdown:\n\nShutting Down a Node\n====================\n\nWhen you need to shut down a node for maintenance, you can follow the steps\nbelow to avoid packet loss as much as possible.\n\n1. Drain the node to evict all workloads. This will remove all Pods on the node\n   from the Service endpoints and prevent Services with\n   ``externalTrafficPolicy=Cluster`` from redirecting traffic to the node.\n\n   .. code-block:: bash\n\n      kubectl drain <node-name> --ignore-daemonsets\n\n2. Reconfigure the BGP sessions by modifying or removing the\n   CiliumBGPPeeringPolicy or CiliumBGPClusterConfig node selector label on the Node object.\n   This will shut down all BGP sessions on the node.\n\n   .. code-block:: bash\n\n      # Assuming you select the node by the label enable-bgp=true\n      kubectl label node <node-name> --overwrite enable-bgp=false\n\n3. Wait for a while until the BGP peer removes routes towards the node. During\n   this period, the BGP peer may still send traffic to the node. If you shut\n   down the node without waiting for the BGP peer to remove routes, it will\n   break the ongoing traffic of ``externalTrafficPolicy=Cluster`` Services.\n\n4. Shut down the node.\n\nIn step 3, you may not be able to check the peer status and may want to wait\nfor a specific period of time without checking the actual peer status. In this\ncase, you can roughly estimate the time like the following:\n\n* If you disable the BGP Graceful Restart feature, the BGP peer should withdraw\n  routes immediately after step 2.\n\n* If you enable the BGP Graceful Restart feature, there are two possible cases.\n\n  * If the BGP peer supports the Graceful Restart with Notification\n    (:rfc:`8538`), it will withdraw routes after the Stale Timer (defined in\n    the :rfc:`8538#section-4.1`) expires.\n\n  * If the BGP peer does not support the Graceful Restart with Notification, it\n    will withdraw routes immediately after step 2 because the BGP Control Plane\n    sends the BGP Notification to the peer when you unselect the node.\n\nThe above estimation is a theoretical value, and the actual time always depends\non the BGP peer's implementation. Ideally, you should check the peer router's\nactual behavior in advance with your network administrator.\n\n.. warning::\n\n   Even if you follow the above steps, some ongoing Service traffic originally\n   destined for the node may be reset because, after the route withdrawal and ECMP\n   rehashing, the traffic is redirected to a different node, and the new node may\n   select a different endpoint.\n\nFailure Scenarios\n=================\n\nThis document describes common failure scenarios that you may encounter when\nusing the BGP Control Plane and provides guidance on how to mitigate them.\n\nCilium Agent Down\n \n If the Cilium agent goes down, the BGP session will be lost because the BGP\nspeaker is integrated within the Cilium agent. The BGP session will be restored\nonce the Cilium agent is restarted. However, while the Cilium agent is down,\nthe advertised routes will be removed from the BGP peer. As a result, you may\ntemporarily lose connectivity to the Pods or Services. \n Mitigation\n'''''''''' \n The recommended way to address this issue is by enabling the\n:ref: bgp_control_plane_graceful_restart  feature. This feature allows the BGP\npeer to retain routes for a specific period of time after the BGP session is\nlost. Since the datapath remains active even when the agent is down, this will\nprevent the loss of connectivity to the Pods or Services. \n When you can't use BGP Graceful Restart, you can take the following actions,\ndepending on the kind of routes you are using: \n PodCIDR routes\n++++++++++++++ \n If you are advertising PodCIDR routes, pods on the failed node will be\nunreachable from the external network. If the failure only occurs on a subset\nof the nodes in the cluster, you can drain the unhealthy nodes to migrate the\npods to other nodes. \n Service routes\n++++++++++++++ \n If you are advertising service routes, the load balancer (KubeProxy or Cilium\nKubeProxyReplacement) may become unreachable from the external network.\nAdditionally, ongoing connections may be redirected to different nodes due to\nECMP rehashing on the upstream routers. When the load balancer encounters\nunknown traffic, it will select a new endpoint. Depending on the load\nbalancer's backend selection algorithm, the traffic may be directed to a\ndifferent endpoint than before, potentially causing the connection to be reset. \n If your upstream routers support ECMP with  Resilient Hashing _, enabling\nit may help to keep the ongoing connections forwarded to the same node.\nEnabling the :ref: maglev  feature in Cilium may also help since it increases\nthe probability that all nodes select the same endpoint for the same flow.\nHowever, it only works for the  externalTrafficPolicy: Cluster . If the\nService's  externalTrafficPolicy  is set to  Local , it is inevitable that\nall ongoing connections with the endpoints on the failed node, and connections\nforwarded to a different node than before, will be reset. \n .. _Resilient Hashing: https://www.juniper.net/documentation/us/en/software/junos/interfaces-ethernet-switches/topics/topic-map/resillient-hashing-lag-ecmp.html \n Node Down \n \nIf the node goes down, the BGP sessions from this node will be lost. The peer\nwill withdraw the routes advertised by the node immediately or takes some time\nto stop forwarding traffic to the node depending on the Graceful Restart settings.\nThe latter case is problematic when you advertise the route to a Service with\n``externalTrafficPolicy=Cluster`` because the peer will continue to forward traffic\nto the unavailable node until the restart timer (which is 120s by default) expires.\n\nMitigation\n''''''''''\n\nInvoluntary Shutdown\n++++++++++++++++++++\n\nWhen a node is involuntarily shut down, there's no direct mitigation. You can\nchoose to not use the BGP Graceful Restart feature, depending on the trade-off\nbetween the failure detection time vs stability provided by graceful restart in\ncases of Cilium pod restarts.\n\nDisabling the Graceful Restart allows the BGP peer to withdraw routes faster.\nEven if the node is shut down without BGP Notification or TCP connection close,\nthe worst case time for peer to withdraw routes is the BGP hold time. When the\nGraceful Restart is enabled, the BGP peer may need hold time + restart time to\nwithdraw routes received from the node.\n\nVoluntary Shutdown\n++++++++++++++++++\n\nWhen you voluntarily shut down a node, you can follow the steps described in the\n:ref:`bgp_control_plane_node_shutdown` section to avoid packet loss as much as\npossible.\n\nPeering Link Down\n \n If the peering link between the BGP peers goes down, usually, both the BGP\nsession and datapath connectivity will be lost. However, there may be a period\nduring which the datapath connectivity is lost while the BGP session remains up\nand routes are still being advertised. This can cause the BGP peer to send\ntraffic over the failed link, resulting in dropped packets. The length of this\nperiod depends on which link is down and the BGP configuration. \n If the link directly connected to the Node goes down, the BGP session will\nlikely be lost immediately because the Linux kernel detects the link failure\nand shuts down the TCP session right away. If a link not directly connected to\nthe Node goes down, the BGP session will be lost after the hold timer expires,\nwhich is set to 90 seconds by default. \n Mitigation\n'''''''''' \n To make link detection failure fast, you can adjust  holdTimeSeconds  and\n keepAliveTimeSeconds  in the BGP configuration to the shorter value.\nHowever, the minimal possible values are  holdTimeSeconds=3  and\n keepAliveTimeSeconds=1 . The general approach to make failure detection faster is to\nuse BFD (Bidirectional Forwarding Detection), but currently, Cilium does not\nsupport it. \n Cilium Operator Down \n \nThe Cilium operator is responsible for translating ``CiliumBGPClusterConfig`` to\nthe per node ``CiliumBGPNodeConfig`` resource. If the Cilium operator is down,\nprovisioning of BGP control plane will be stopped.\n\nSimilarly, PodCIDR allocation by IPAM, and LoadBalancer IP allocation by LB-IPAM\nare stopped. Therefore, the advertisement of new and withdrawal of old PodCIDR and\nService VIP routes will be stopped as well.\n\n\nMitigation\n''''''''''\n\nThere's no direct mitigation in terms of the BGP. However, running the Cilium\nOperator with a :ref:`high-availability setup <cilium_operator_internals>` will\nmake the Cilium Operator more resilient to failures.\n\nService Losing All Backends\n \n If all service backends are gone due to an outage or a configuration mistake, BGP\nControl Plane behaves differently depending on the Service's\n externalTrafficPolicy . When the  externalTrafficPolicy  is set to\n Cluster , the Service's VIP remains advertised from all nodes selected by the\n CiliumBGPPeeringPolicy  or  CiliumBGPClusterConfig . When the  externalTrafficPolicy \nis set to  Local , the advertisement stops entirely because the Service's VIP is only advertised\nfrom the node where the Service backends are running. \n Mitigation\n'''''''''' \n There's no direct mitigation in terms of the BGP. In general, you should\nprevent the Service backends from being all gone by Kubernetes features like\nPodDisruptionBudget.",
  "item_type": "unknown",
  "module_path": "/tmp/cilium-repo/Documentation/network/bgp-control-plane/bgp-control-plane-operation.rst",
  "extracted_at": "2025-09-03T01:13:29.189954Z"
}