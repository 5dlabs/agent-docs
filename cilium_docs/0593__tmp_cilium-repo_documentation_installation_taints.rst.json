{
  "url": "file:///tmp/cilium-repo/Documentation/installation/taints.rst",
  "content": ".. only:: not (epub or latex or html)\n\n    WARNING: You are looking at unreleased Cilium documentation.\n    Please use the official rendered version released here:\n    https://docs.cilium.io\n\n.. _taint_effects:\n\n#####################################################\nConsiderations on Node Pool Taints and Unmanaged Pods\n#####################################################\n\nDepending on the environment or cloud provider being used, a CNI plugin and/or\nconfiguration file may be pre-installed in nodes belonging to a given cluster\nwhere Cilium is being installed or already running. Upon starting on a given\nnode, and if it is intended as the exclusive CNI plugin for the cluster, Cilium\ndoes its best to take ownership of CNI on the node. However, a couple situations\ncan prevent this from happening:\n\n* Cilium can only take ownership of CNI on a node after starting. Pods starting\n  before Cilium runs on a given node may get IPs from the pre-configured CNI.\n\n* Some cloud providers may revert changes made to the CNI configuration by\n  Cilium during operations such as node reboots, updates or routine maintenance.\n\nThis is notably the case with GKE (non-Dataplane V2), in which node reboots and\nupgrades will undo changes made by Cilium and re-instate the default CNI\nconfiguration.\n\nTo help overcome this situation to the largest possible extent in environments\nand cloud providers where Cilium isn't supported as the single CNI, Cilium can\nmanipulate Kubernetes's `taints <https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/>`_\non a given node to help preventing pods from starting before Cilium runs on said\nnode. The mechanism works as follows:\n\n1. The cluster administrator places a specific taint (see below) on a given\n   uninitialized node. Depending on the taint's effect (see below), this prevents\n   pods that don't have a matching toleration from either being scheduled or\n   altogether running on the node until the taint is removed.\n\n2. Cilium runs on the node, initializes it and, once ready, removes the\n   aforementioned taint.\n\n3. From this point on, pods will start being scheduled and running on the node,\n   having their networking managed by Cilium.\n\n4. If Cilium is temporarily removed from the node, the Operator will re-apply\n   the taint (but only with NoSchedule).\n\nBy default, the taint key is ``node.cilium.io/agent-not-ready``, but in some\nscenarios (such as when Cluster Autoscaler is being used but its flags cannot be\nconfigured) this key may need to be tweaked. This can be done using the\n``agent-not-ready-taint-key`` option. In the aforementioned example, users should\nspecify a key starting with ``ignore-taint.cluster-autoscaler.kubernetes.io/``.\nWhen such a value is used, the Cluster Autoscaler will ignore it when simulating\nscheduling, allowing the cluster to scale up.\n\nThe taint's effect should be chosen taking into account the following\nconsiderations:\n\n* If ``NoSchedule`` is used, pods won't be *scheduled* to a node until Cilium\n  has the chance to remove the taint. However, one practical effect of this is\n  that if some external process (such as a reboot) resets the CNI configuration on\n  said node, pods that were already scheduled will be allowed to start\n  concurrently with Cilium when the node next reboots, and hence may become\n  unmanaged and have their networking being managed by another CNI plugin.\n\n* If ``NoExecute`` is used, pods won't be *executed* (nor *scheduled*) on a node\n  until Cilium has had the chance to remove the taint. One practical effect of\n  this is that whenever the taint is added back to the node by some external\n  process (such as during an upgrade or eventually a routine operation), pods\n  will be evicted from the node until Cilium has had the chance to remove the\n  taint.\n\nAnother important thing to consider is the concept of node itself, and the\ndifferent point of views over a node. For example, the instance/VM which backs a\nKubernetes node can be patched or reset filesystem-wise by a cloud provider, or\naltogether replaced with an entirely new instance/VM that comes back with the\nsame name as the already-existing Kubernetes ``Node`` resource. Even though in\nsaid scenarios the node-pool-level taint will be added back to the ``Node``\nresource, pods that were already scheduled to the node having this name will run\non the node at the same time as Cilium, potentially becoming unmanaged. This is\nwhy ``NoExecute`` is recommended, as assuming the taint is added back in this\nscenario, already-scheduled pods won't run.\n\nHowever, on some environments or cloud providers, and as mentioned above, it may\nhappen that a taint established at the node-pool level is added back to a node\nafter Cilium has removed it and for reasons other than a node upgrade/reset.\nThe exact circumstances in which this may happen may vary, but this may lead to\nunexpected/undesired pod evictions in the particular case when ``NoExecute`` is\nbeing used as the taint effect. It is, thus, recommended that in each deployment\nand depending on the environment or cloud provider, a careful decision is made\nregarding the taint effect (or even regarding whether to use the taint-based\napproach at all) based on the information above, on the environment or cloud\nprovider's documentation, and on the fact that one is essentially establishing\na trade-off between having unmanaged pods in the cluster (which can lead to\ndropped traffic and other issues) and having unexpected/undesired evictions\n(which can lead to application downtime).\n\nTaking into account all of the above, throughout the Cilium documentation we\nrecommend ``NoExecute`` to be used as we believe it to be the least disruptive\nmode that users can use to deploy Cilium on cloud providers.\n",
  "item_type": "unknown",
  "module_path": "/tmp/cilium-repo/Documentation/installation/taints.rst",
  "extracted_at": "2025-09-03T01:13:29.315390Z"
}