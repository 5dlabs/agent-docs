{
  "url": "file:///tmp/cilium-repo/Documentation/security/grpc.rst",
  "content": ".. only:: not (epub or latex or html)\n\n    WARNING: You are looking at unreleased Cilium documentation.\n    Please use the official rendered version released here:\n    https://docs.cilium.io\n\n*************\nSecuring gRPC\n*************\n\nThis document serves as an introduction to using Cilium to enforce gRPC-aware\nsecurity policies.  It is a detailed walk-through of getting a single-node\nCilium environment running on your machine. It is designed to take 15-30\nminutes.\n\n.. include:: gsg_requirements.rst\n\nIt is important for this demo that ``kube-dns`` is working correctly. To know the\nstatus of ``kube-dns`` you can run the following command:\n\n.. code-block:: shell-session\n\n    $ kubectl get deployment kube-dns -n kube-system\n    NAME       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\n    kube-dns   1         1         1            1           13h\n\nWhere at least one pod should be available.\n\nDeploy the Demo Application\n===========================\n\nNow that we have Cilium deployed and ``kube-dns`` operating correctly we can\ndeploy our demo gRPC application.  Since our first demo of Cilium + HTTP-aware security\npolicies was Star Wars-themed, we decided to do the same for gRPC. While the\n`HTTP-aware Cilium  Star Wars demo <https://cilium.io/blog/2017/5/4/demo-may-the-force-be-with-you/>`_\nshowed how the Galactic Empire used HTTP-aware security policies to protect the Death Star from the\nRebel Alliance, this gRPC demo shows how the lack of gRPC-aware security policies allowed Leia, Chewbacca, Lando, C-3PO, and R2-D2 to escape from Cloud City, which had been overtaken by\nempire forces.\n\n`gRPC <https://grpc.io/>`_ is a high-performance RPC framework built on top of the `protobuf <https://developers.google.com/protocol-buffers/>`_\nserialization/deserialization library popularized by Google.  There are gRPC bindings\nfor many programming languages, and the efficiency of the protobuf parsing as well as\nadvantages from leveraging HTTP 2 as a transport make it a popular RPC framework for\nthose building new microservices from scratch.\n\nFor those unfamiliar with the details of the movie, Leia and the other rebels are\nfleeing storm troopers and trying to reach the space port platform where the Millennium Falcon\nis parked, so they can fly out of Cloud City. However, the door to the platform is closed,\nand the access code has been changed. However, R2-D2 is able to access the Cloud City\ncomputer system via a public terminal, and disable this security, opening the door and\nletting the Rebels reach the Millennium Falcon just in time to escape.\n\n.. image:: images/cilium_grpc_gsg_r2d2_terminal.png\n\nIn our example, Cloud City's internal computer system is built as a set of gRPC-based\nmicroservices (who knew that gRPC was actually invented a long time ago, in a galaxy\nfar, far away?).\n\nWith gRPC, each service is defined using a language independent protocol buffer definition.\nHere is the definition for the system used to manage doors within Cloud City:\n\n.. code-block:: java\n\n  package cloudcity;\n\n  // The door manager service definition.\n  service DoorManager {\n\n    // Get human readable name of door.\n    rpc GetName(DoorRequest) returns (DoorNameReply) {}\n\n    // Find the location of this door.\n    rpc GetLocation (DoorRequest) returns (DoorLocationReply) {}\n\n    // Find out whether door is open or closed\n    rpc GetStatus(DoorRequest) returns (DoorStatusReply) {}\n\n    // Request maintenance on the door\n    rpc RequestMaintenance(DoorMaintRequest) returns (DoorActionReply) {}\n\n    // Set Access Code to Open / Lock the door\n    rpc SetAccessCode(DoorAccessCodeRequest) returns (DoorActionReply) {}\n\n  }\n\nTo keep the setup small, we will just launch two pods to represent this setup:\n\n- **cc-door-mgr**: A single pod running the gRPC door manager service with label ``app=cc-door-mgr``.\n- **terminal-87**: One of the public network access terminals scattered across Cloud City. R2-D2 plugs into terminal-87 as the rebels are desperately trying to escape. This terminal uses the gRPC client code to communicate with the door management services with label ``app=public-terminal``.\n\n\n.. image:: images/cilium_grpc_gsg_topology.png\n\nThe file ``cc-door-app.yaml`` contains a Kubernetes Deployment for the door manager\nservice, a Kubernetes Pod representing ``terminal-87``, and a Kubernetes Service for\nthe door manager services. To deploy this example app, run:\n\n.. parsed-literal::\n\n    $ kubectl create -f \\ |SCM_WEB|\\/examples/kubernetes-grpc/cc-door-app.yaml\n    deployment \"cc-door-mgr\" created\n    service \"cc-door-server\" created\n    pod \"terminal-87\" created\n\nKubernetes will deploy the pods and service in the background. Running\n``kubectl get svc,pods`` will inform you about the progress of the operation.\nEach pod will go through several states until it reaches ``Running`` at which\npoint the setup is ready.\n\n.. code-block:: shell-session\n\n    $ kubectl get pods,svc\n    NAME                                 READY     STATUS    RESTARTS   AGE\n    po/cc-door-mgr-3590146619-cv4jn      1/1       Running   0          1m\n    po/terminal-87                       1/1       Running   0          1m\n\n    NAME                 CLUSTER-IP   EXTERNAL-IP   PORT(S)     AGE\n    svc/cc-door-server   10.0.0.72    <none>        50051/TCP   1m\n    svc/kubernetes       10.0.0.1     <none>        443/TCP     6m\n\nTest Access Between gRPC Client and Server\n==========================================\n\nFirst, let's confirm that the public terminal can properly act as a client to the\ndoor service.  We can test this by running a Python gRPC client for the door service that\nexists in the *terminal-87* container.\n\nWe'll invoke the 'cc_door_client' with the name of the gRPC method to call, and any\nparameters (in this case, the door-id):\n\n.. code-block:: shell-session\n\n    $ kubectl exec terminal-87 -- python3 /cloudcity/cc_door_client.py GetName 1\n    Door name is: Spaceport Door #1\n\n    $ kubectl exec terminal-87 -- python3 /cloudcity/cc_door_client.py GetLocation 1\n    Door location is lat = 10.222200393676758 long = 68.87879943847656\n\nExposing this information to public terminals seems quite useful, as it helps travelers new\nto Cloud City identify and locate different doors. But recall that the door service also\nexposes several other methods, including ``SetAccessCode``. If access to the door manager\nservice is protected only using traditional IP and port-based firewalling, the TCP port of\nthe service (50051 in this example) will be wide open to allow legitimate calls like\n``GetName`` and ``GetLocation``, which also leave more sensitive calls like ``SetAccessCode`` exposed as\nwell. It is this mismatch between the course granularity of traditional firewalls and\nthe fine-grained nature of gRPC calls that R2-D2 exploited to override the security\nand help the rebels escape.\n\nTo see this, run:\n\n.. code-block:: shell-session\n\n    $ kubectl exec terminal-87 -- python3 /cloudcity/cc_door_client.py SetAccessCode 1 999\n    Successfully set AccessCode to 999\n\n\nSecuring Access to a gRPC Service with Cilium\n=============================================\n\nOnce the legitimate owners of Cloud City recover the city from the empire, how can they\nuse Cilium to plug this key security hole and block requests to ``SetAccessCode`` and ``GetStatus``\nwhile still allowing ``GetName``, ``GetLocation``, and ``RequestMaintenance``?\n\n.. image:: images/cilium_grpc_gsg_policy.png\n\nSince gRPC build on top of HTTP, this can be achieved easily by understanding how a\ngRPC call is mapped to an HTTP URL, and then applying a Cilium HTTP-aware filter to\nallow public terminals to only invoke a subset of all the total gRPC methods available\non the door service.\n\nEach gRPC method is mapped to an HTTP POST call to a URL of the form\n``/cloudcity.DoorManager/<method-name>``.\n\nAs a result, the following *CiliumNetworkPolicy* rule limits access of pods with label\n``app=public-terminal`` to only invoke ``GetName``, ``GetLocation``, and ``RequestMaintenance``\non the door service, identified by label ``app=cc-door-mgr``:\n\n.. literalinclude:: ../../examples/kubernetes-grpc/cc-door-ingress-security.yaml\n   :language: yaml\n   :emphasize-lines: 9,13,21\n\nA *CiliumNetworkPolicy* contains a list of rules that define allowed requests,\nmeaning that requests that do not match any rules (e.g., ``SetAccessCode``) are denied as invalid.\n\nThe above rule applies to inbound (i.e., \"ingress\") connections to ``cc-door-mgr pods`` (as\nindicated by ``app: cc-door-mgr``\nin the \"endpointSelector\" section). The rule will apply to connections from pods with label\n``app: public-terminal`` as indicated by the \"fromEndpoints\" section.\nThe rule explicitly matches\ngRPC connections destined to TCP 50051, and white-lists specifically the permitted URLs.\n\nApply this gRPC-aware network security policy using ``kubectl`` in the main window:\n\n.. parsed-literal::\n\n    $ kubectl create -f \\ |SCM_WEB|\\/examples/kubernetes-grpc/cc-door-ingress-security.yaml\n\nAfter this security policy is in place, access to the innocuous calls like ``GetLocation``\nstill works as intended:\n\n.. code-block:: shell-session\n\n    $ kubectl exec terminal-87 -- python3 /cloudcity/cc_door_client.py GetLocation 1\n    Door location is lat = 10.222200393676758 long = 68.87879943847656\n\n\nHowever, if we then again try to invoke ``SetAccessCode``, it is denied:\n\n.. code-block:: shell-session\n\n    $ kubectl exec terminal-87 -- python3 /cloudcity/cc_door_client.py SetAccessCode 1 999\n\n    Traceback (most recent call last):\n      File \"/cloudcity/cc_door_client.py\", line 71, in <module>\n        run()\n      File \"/cloudcity/cc_door_client.py\", line 52, in run\n        response = stub.SetAccessCode(cloudcity_pb2.DoorAccessCodeRequest(\n      File \"/usr/local/lib/python3.8/dist-packages/grpc/_channel.py\", line 826, in __call__\n        return _end_unary_response_blocking(state, call, False, None)\n      File \"/usr/local/lib/python3.8/dist-packages/grpc/_channel.py\", line 729, in _end_unary_response_blocking\n        raise _InactiveRpcError(state)\n    grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n        status = StatusCode.PERMISSION_DENIED\n        details = \"Access denied\"\n\t      debug_error_string = \"{\"created\":\"@1748342370.953859451\",\"description\":\"Error received from peer ipv4:10.96.86.105:50051\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":1055,\"grpc_message\":\"Access denied\\r\\n\",\"grpc_status\":7}\"\n\n\nThis is now blocked, thanks to the Cilium network policy. And notice that unlike\na traditional firewall which would just drop packets in a way indistinguishable\nfrom a network failure, because Cilium operates at the API-layer, it can\nexplicitly reply with a custom gRPC status code 7 PERMISSION_DENIED, indicating that the\nrequest was intentionally denied for security reasons.\n\nThank goodness that the empire IT staff hadn't had time to deploy Cilium on\nCloud City's internal network prior to the escape attempt, or things might have\nturned out quite differently for Leia and the other Rebels!\n\nClean-Up\n========\n\nYou have now installed Cilium, deployed a demo app, and tested\nL7 gRPC-aware network security policies. To clean-up, run:\n\n.. parsed-literal::\n\n   $ kubectl delete -f \\ |SCM_WEB|\\/examples/kubernetes-grpc/cc-door-app.yaml\n   $ kubectl delete cnp rule1\n\nAfter this, you can re-run the tutorial from Step 1.\n",
  "item_type": "unknown",
  "module_path": "/tmp/cilium-repo/Documentation/security/grpc.rst",
  "extracted_at": "2025-09-03T00:53:44.701635Z"
}