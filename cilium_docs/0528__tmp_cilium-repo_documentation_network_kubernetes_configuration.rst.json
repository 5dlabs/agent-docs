{
  "url": "file:///tmp/cilium-repo/Documentation/network/kubernetes/configuration.rst",
  "content": ".. only:: not (epub or latex or html) \n WARNING: You are looking at unreleased Cilium documentation.\nPlease use the official rendered version released here:\nhttps://docs.cilium.io\n \n .. _k8s_configuration: \n \n Configuration \n \n ConfigMap Options \n In the :term: ConfigMap  there are several options that can be configured according\nto your preferences: \n \n \n debug  - Sets to run Cilium in full debug mode, which enables verbose\nlogging and configures eBPF programs to emit more visibility events into the\noutput of  cilium-dbg monitor . \n \n \n enable-ipv4  - Enable IPv4 addressing support \n \n \n enable-ipv6  - Enable IPv6 addressing support \n \n \n clean-cilium-bpf-state  - Removes all eBPF state from the filesystem on\nstartup. Endpoints will be restored with the same IP addresses, but ongoing\nconnections may be briefly disrupted and loadbalancing decisions will be\nlost, so active connections via the loadbalancer will break. All eBPF state\nwill be reconstructed from their original sources (for example, from\nKubernetes or the kvstore). This may be used to mitigate serious issues\nregarding eBPF maps. This option should be turned off again after restarting\nthe daemon. \n \n \n clean-cilium-state  - Removes  all  Cilium state, including unrecoverable\ninformation such as all endpoint state, as well as recoverable state such as\neBPF state pinned to the filesystem, CNI configuration files, library code,\nlinks, routes, and other information.  This operation is irreversible .\nExisting endpoints currently managed by Cilium may continue to operate as\nbefore, but Cilium will no longer manage them and they may stop working\nwithout warning. After using this operation, endpoints must be deleted and\nreconnected to allow the new instance of Cilium to manage them. \n \n \n monitor-aggregation  - This option enables coalescing of tracing events in\n cilium-dbg monitor  to only include periodic updates from active flows, or any\npackets that involve an L4 connection state change. Valid options are\n none ,  low ,  medium ,  maximum . \n \n none  - Generate a tracing event on every receive and send packet. \n low  - Generate a tracing event on every send packet. \n medium  - Generate a tracing event for send packets only on every new\nconnection, any time a packet contains TCP flags that have not been previously\nseen for the packet direction, and on average once per  monitor-aggregation-interval \n(assuming that a packet is seen during the interval). Each direction tracks\nTCP flags and report interval separately. If Cilium drops a packet, it will\nemit one event per packet dropped. \n maximum  - An alias for the most aggressive aggregation level. Currently\nthis is equivalent to setting  monitor-aggregation  to  medium . \n \n \n \n monitor-aggregation-interval  - Defines the interval to report tracing\nevents. Only applicable for  monitor-aggregation  levels  medium  or higher.\nAssuming new packets are sent at least once per interval, this ensures that on\naverage one event is sent during the interval. \n \n \n preallocate-bpf-maps  - Pre-allocation of map entries allows per-packet\nlatency to be reduced, at the expense of up-front memory allocation for the\nentries in the maps. Set to  true  to optimize for latency. If this value\nis modified, then during the next Cilium startup connectivity may be\ntemporarily disrupted for endpoints with active connections. \n \n \n Any changes that you perform in the Cilium :term: ConfigMap  and in\n cilium-etcd-secrets   Secret  will require you to restart any existing\nCilium pods in order for them to pick the latest configuration. \n .. attention:: \n When updating keys or values in the ConfigMap, the changes might take up to\n2 minutes to be propagated to all nodes running in the cluster. For more\ninformation see the official Kubernetes docs:\n Mounted ConfigMaps are updated automatically <https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#mounted-configmaps-are-updated-automatically> __ \n The following :term: ConfigMap  is an example where the etcd cluster is running in 2\nnodes,  node-1  and  node-2  with TLS, and client to server authentication\nenabled. \n .. code-block:: yaml \n apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: cilium-config\n  namespace: kube-system\ndata:\n  # The kvstore configuration is used to enable use of a kvstore for state\n  # storage.\n  kvstore: etcd\n  kvstore-opt: '{\"etcd.config\": \"/var/lib/etcd-config/etcd.config\"}'\n\n  # This etcd-config contains the etcd endpoints of your cluster. If you use\n  # TLS please make sure you follow the tutorial in https://cilium.link/etcd-config\n  etcd-config: |-\n    ---\n    endpoints:\n      - https://node-1:31079\n      - https://node-2:31079\n    #\n    # In case you want to use TLS in etcd, uncomment the 'trusted-ca-file' line\n    # and create a kubernetes secret by following the tutorial in\n    # https://cilium.link/etcd-config\n    trusted-ca-file: '/var/lib/etcd-secrets/etcd-client-ca.crt'\n    #\n    # In case you want client to server authentication, uncomment the following\n    # lines and create a kubernetes secret by following the tutorial in\n    # https://cilium.link/etcd-config\n    key-file: '/var/lib/etcd-secrets/etcd-client.key'\n    cert-file: '/var/lib/etcd-secrets/etcd-client.crt'\n\n  # If you want to run cilium in debug mode change this value to true\n  debug: \"false\"\n  enable-ipv4: \"true\"\n  # If you want to clean cilium state; change this value to true\n  clean-cilium-state: \"false\"\n \n CNI \n :term: CNI  - Container Network Interface is the plugin layer used by Kubernetes to\ndelegate networking configuration. You can find additional information on the\n:term: CNI  project website. \n CNI configuration is automatically taken care of when deploying Cilium via the provided\n:term: DaemonSet . The  cilium  pod will generate an appropriate CNI configuration\nfile and write it to disk on startup. \n .. note:: In order for CNI installation to work properly, the\n kubelet  task must either be running on the host filesystem of the\nworker node, or the  /etc/cni/net.d  and  /opt/cni/bin \ndirectories must be mounted into the container where  kubelet  is\nrunning. This can be achieved with :term: Volumes  mounts. \n The CNI auto installation is performed as follows: \n \n \n The  /etc/cni/net.d  and  /opt/cni/bin  directories are mounted from the\nhost filesystem into the pod where Cilium is running. \n \n \n The binary  cilium-cni  is installed to  /opt/cni/bin . Any existing\nbinary with the name  cilium-cni  is overwritten. \n \n \n The file  /etc/cni/net.d/05-cilium.conflist  is written. \n \n \n Adjusting CNI configuration \n The CNI configuration file is automatically written and maintained by the\ncilium pod. It is written after the agent has finished initialization and\nis ready to handle pod sandbox creation. In addition, the agent will remove\nany other CNI configuration files by default. \n There are a number of Helm variables that adjust CNI configuration management.\nFor a full description, see the helm documentation. A brief summary: \n +--------------------+----------------------------------------+---------+\n| Helm variable      | Description                            | Default |\n+====================+========================================+=========+\n|  cni.customConf  | Disable CNI configuration management   | false   |\n+--------------------+----------------------------------------+---------+\n|  cni.exclusive   | Remove other CNI configuration files   | true    |\n+--------------------+----------------------------------------+---------+\n|  cni.install     | Install CNI configuration and binaries | true    |\n+--------------------+----------------------------------------+---------+ \n If you want to provide your own custom CNI configuration file, you can do\nso by passing a path to a cni template file, either on disk or provided\nvia a configMap. The Helm options that configure this are: \n +----------------------+----------------------------------------------------------------+\n| Helm variable        | Description                                                    |\n+======================+================================================================+\n|  cni.readCniConf   | Path (inside the agent) to a source CNI configuration file     |\n+----------------------+----------------------------------------------------------------+\n|  cni.configMap     | Name of a ConfigMap containing a source CNI configuration file |\n+----------------------+----------------------------------------------------------------+\n|  cni.configMapKey  | Install CNI configuration and binaries                         |\n+----------------------+----------------------------------------------------------------+ \n These Helm variables are converted to a smaller set of cilium ConfigMap keys: \n +-------------------------------+--------------------------------------------------------+\n| ConfigMap key                 | Description                                            |\n+===============================+========================================================+\n|  write-cni-conf-when-ready  | Path to write the CNI configuration file               |\n+-------------------------------+--------------------------------------------------------+\n|  read-cni-conf              | Path to read the source CNI configuration file         |\n+-------------------------------+--------------------------------------------------------+\n|  cni-exclusive              | Whether or not to remove other CNI configuration files |\n+-------------------------------+--------------------------------------------------------+ \n CRD Validation \n Custom Resource Validation was introduced in Kubernetes since version  1.8.0 .\nThis is still considered an alpha feature in Kubernetes  1.8.0  and beta in\nKubernetes  1.9.0 . \n Since Cilium  v1.0.0-rc3 , Cilium will create, or update in case it exists,\nthe Cilium Network Policy (CNP) Resource Definition with the embedded\nvalidation schema. This allows the validation of CiliumNetworkPolicy to be done\non the kube-apiserver when the policy is imported with an ability to provide\ndirect feedback when importing the resource. \n To enable this feature, the flag  --feature-gates=CustomResourceValidation=true \nmust be set when starting kube-apiserver. Cilium itself will automatically make\nuse of this feature and no additional flag is required. \n .. note:: In case there is an invalid CNP before updating to Cilium\n v1.0.0-rc3 , which contains the validator, the kube-apiserver\nvalidator will prevent Cilium from updating that invalid CNP with\nCilium node status. By checking Cilium logs for  unable to update           CNP, retrying... , it is possible to determine which Cilium Network\nPolicies are considered invalid after updating to Cilium\n v1.0.0-rc3 . \n To verify that the CNP resource definition contains the validation schema, run\nthe following command: \n .. code-block:: shell-session \n $ kubectl get crd ciliumnetworkpolicies.cilium.io -o json | grep -A 12 openAPIV3Schema\n        \"openAPIV3Schema\": {\n            \"oneOf\": [\n                {\n                    \"required\": [\n                        \"spec\"\n                    ]\n                },\n                {\n                    \"required\": [\n                        \"specs\"\n                    ]\n                }\n            ],\n \n In case the user writes a policy that does not conform to the schema, Kubernetes\nwill return an error, e.g.: \n .. code-block:: shell-session \n cat <<EOF > ./bad-cnp.yaml\napiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy\nmetadata:\n  name: my-new-cilium-object\nspec:\n  description: \"Policy to test multiple rules in a single file\"\n  endpointSelector:\n    matchLabels:\n      app: details\n      track: stable\n      version: v1\n  ingress:\n  - fromEndpoints:\n    - matchLabels:\n        app: reviews\n        track: stable\n        version: v1\n    toPorts:\n    - ports:\n      - port: '65536'\n        protocol: TCP\n      rules:\n        http:\n        - method: GET\n          path: \"/health\"\nEOF\n\nkubectl create -f ./bad-cnp.yaml\n...\nspec.ingress.toPorts.ports.port in body should match '^(6553[0-5]|655[0-2][0-9]|65[0-4][0-9]{2}|6[0-4][0-9]{3}|[1-5][0-9]{4}|[0-9]{1,4})$'\n \n In this case, the policy has a port out of the 0-65535 range. \n .. _bpffs_systemd: \n Mounting BPFFS with systemd \n Due to how systemd  mounts <https://unix.stackexchange.com/questions/283442/systemd-mount-fails-where-setting-doesnt-match-unit-name> __\nfilesystems, the mount point path must be reflected in the unit filename. \n .. code-block:: shell-session \n     cat <<EOF | sudo tee /etc/systemd/system/sys-fs-bpf.mount\n    [Unit]\n    Description=Cilium BPF mounts\n    Documentation=https://docs.cilium.io/\n    DefaultDependencies=no\n    Before=local-fs.target umount.target\n    After=swap.target\n\n    [Mount]\n    What=bpffs\n    Where=/sys/fs/bpf\n    Type=bpf\n    Options=rw,nosuid,nodev,noexec,relatime,mode=700\n\n    [Install]\n    WantedBy=multi-user.target\n    EOF\n \n Container Runtimes \n .. _crio-instructions: \n CRIO \n If you want to use CRIO, use the instructions below. \n .. include:: ../../installation/k8s-install-download-release.rst \n .. note:: \n The Helm flag  --set bpf.autoMount.enabled=false  might not be\nrequired for your setup. For more info see :ref: crio-known-issues . \n .. parsed-literal:: \n helm install cilium |CHART_RELEASE| \\\n--namespace kube-system \n Since CRI-O does not automatically detect that a new CNI plugin has been\ninstalled, you will need to restart the CRI-O daemon for it to pick up the\nCilium CNI configuration. \n First make sure Cilium is running: \n .. code-block:: shell-session \n $ kubectl get pods -n kube-system -o wide\nNAME               READY     STATUS    RESTARTS   AGE       IP          NODE\ncilium-mqtdz       1/1       Running   0          3m       10.0.2.15   minikube\n \n After that you can restart CRI-O: \n .. code-block:: shell-session \n minikube ssh -- sudo systemctl restart crio\n \n .. _crio-known-issues: \n Common CRIO issues \n Some CRI-O environments automatically mount the bpf filesystem in the pods,\nwhich is something that Cilium avoids doing when\n --set bpf.autoMount.enabled=false  is set. However, some\nCRI-O environments do not mount the bpf filesystem automatically which causes\nCilium to print the following message:: \n     level=warning msg=\"BPF system config check: NOT OK.\" error=\"CONFIG_BPF kernel parameter is required\" subsys=linux-datapath\n    level=warning msg=\"================================= WARNING ==========================================\" subsys=bpf\n    level=warning msg=\"BPF filesystem is not mounted. This will lead to network disruption when Cilium pods\" subsys=bpf\n    level=warning msg=\"are restarted. Ensure that the BPF filesystem is mounted in the host.\" subsys=bpf\n    level=warning msg=\"https://docs.cilium.io/en/stable/operations/system_requirements/#mounted-ebpf-filesystem\" subsys=bpf\n    level=warning msg=\"====================================================================================\" subsys=bpf\n    level=info msg=\"Mounting BPF filesystem at /sys/fs/bpf\" subsys=bpf\n \n If you see this warning in the Cilium pod logs with your CRI-O environment,\nplease remove the flag  --set bpf.autoMount.enabled=false  from\nyour Helm setup and redeploy Cilium.",
  "item_type": "unknown",
  "module_path": "/tmp/cilium-repo/Documentation/network/kubernetes/configuration.rst",
  "extracted_at": "2025-09-03T01:13:29.241005Z"
}