{
  "url": "file:///tmp/cilium-repo/Documentation/operations/upgrade.rst",
  "content": ".. only:: not (epub or latex or html) \n WARNING: You are looking at unreleased Cilium documentation.\nPlease use the official rendered version released here:\nhttps://docs.cilium.io\n \n .. _admin_upgrade: \n \n Upgrade Guide \n \n .. _upgrade_general: \n This upgrade guide is intended for Cilium running on Kubernetes. If you have\nquestions, feel free to ping us on  Cilium Slack _. \n .. include:: upgrade-warning.rst \n .. _pre_flight: \n Running pre-flight check (Required) \n When rolling out an upgrade with Kubernetes, Kubernetes will first terminate the\npod followed by pulling the new image version and then finally spin up the new\nimage. In order to reduce the downtime of the agent and to prevent  ErrImagePull \nerrors during upgrade, the pre-flight check pre-pulls the new image version.\nIf you are running in :ref: kubeproxy-free \nmode you must also pass on the Kubernetes API Server IP and /\nor the Kubernetes API Server Port when generating the  cilium-preflight.yaml \nfile. \n .. tabs::\n.. group-tab:: kubectl \n .. parsed-literal::\n\n  helm template |CHART_RELEASE| \\\\\n    --namespace=kube-system \\\\\n    --set preflight.enabled=true \\\\\n    --set agent=false \\\\\n    --set operator.enabled=false \\\\\n    > cilium-preflight.yaml\n  kubectl create -f cilium-preflight.yaml\n \n .. group-tab:: Helm \n .. parsed-literal::\n\n  helm install cilium-preflight |CHART_RELEASE| \\\\\n    --namespace=kube-system \\\\\n    --set preflight.enabled=true \\\\\n    --set agent=false \\\\\n    --set operator.enabled=false\n \n .. group-tab:: kubectl (kubeproxy-free) \n .. parsed-literal::\n\n  helm template |CHART_RELEASE| \\\\\n    --namespace=kube-system \\\\\n    --set preflight.enabled=true \\\\\n    --set agent=false \\\\\n    --set operator.enabled=false \\\\\n    --set k8sServiceHost=API_SERVER_IP \\\\\n    --set k8sServicePort=API_SERVER_PORT \\\\\n    > cilium-preflight.yaml\n  kubectl create -f cilium-preflight.yaml\n \n .. group-tab:: Helm (kubeproxy-free) \n .. parsed-literal::\n\n  helm install cilium-preflight |CHART_RELEASE| \\\\\n    --namespace=kube-system \\\\\n    --set preflight.enabled=true \\\\\n    --set agent=false \\\\\n    --set operator.enabled=false \\\\\n    --set k8sServiceHost=API_SERVER_IP \\\\\n    --set k8sServicePort=API_SERVER_PORT\n \n After applying the  cilium-preflight.yaml , ensure that the number of READY\npods is the same number of Cilium pods running. \n .. code-block:: shell-session \n $ kubectl get daemonset -n kube-system | sed -n '1p;/cilium/p'\nNAME                      DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\ncilium                    2         2         2       2            2           <none>          1h20m\ncilium-pre-flight-check   2         2         2       2            2           <none>          7m15s\n \n Once the number of READY pods are equal, make sure the Cilium pre-flight\ndeployment is also marked as READY 1/1. If it shows READY 0/1, consult the\n:ref: cnp_validation  section and resolve issues with the deployment before\ncontinuing with the upgrade. \n .. code-block:: shell-session \n $ kubectl get deployment -n kube-system cilium-pre-flight-check -w\nNAME                      READY   UP-TO-DATE   AVAILABLE   AGE\ncilium-pre-flight-check   1/1     1            0           12s\n \n .. _cleanup_preflight_check: \n Clean up pre-flight check \n Once the number of READY for the preflight :term: DaemonSet  is the same as the number\nof cilium pods running and the preflight  Deployment  is marked as READY  1/1 \nyou can delete the cilium-preflight and proceed with the upgrade. \n .. tabs::\n.. group-tab:: kubectl \n .. code-block:: shell-session\n\n  kubectl delete -f cilium-preflight.yaml\n \n .. group-tab:: Helm \n .. code-block:: shell-session\n\n  helm delete cilium-preflight --namespace=kube-system\n \n .. _upgrade_minor: \n Upgrading Cilium \n During normal cluster operations, all Cilium components should run the same\nversion. Upgrading just one of them (e.g., upgrading the agent without\nupgrading the operator) could result in unexpected cluster behavior.\nThe following steps will describe how to upgrade all of the components from\none stable release to a later stable release. \n .. include:: upgrade-warning.rst \n Step 1: Upgrade to latest patch version \n When upgrading from one minor release to another minor release, for example\n1.x to 1.y, it is recommended to upgrade to the  latest patch release <https://github.com/cilium/cilium#stable-releases> __ for a Cilium release series first.\nUpgrading to the latest patch release ensures the most seamless experience if a\nrollback is required following the minor release upgrade. The upgrade guides\nfor previous versions can be found for each minor version at the bottom left\ncorner. \n Step 2: Use Helm to Upgrade your Cilium deployment \n :term: Helm  can be used to either upgrade Cilium directly or to generate a new set of\nYAML files that can be used to upgrade an existing deployment via  kubectl .\nBy default, Helm will generate the new templates using the default values files\npackaged with each new release. You still need to ensure that you are\nspecifying the equivalent options as used for the initial deployment, either by\nspecifying a them at the command line or by committing the values to a YAML\nfile. \n .. include:: ../installation/k8s-install-download-release.rst \n To minimize datapath disruption during the upgrade, the\n upgradeCompatibility  option should be set to the initial Cilium\nversion which was installed in this cluster. \n .. tabs::\n.. group-tab:: kubectl \n Generate the required YAML file and deploy it:\n\n.. parsed-literal::\n\n  helm template |CHART_RELEASE| \\\\\n    --set upgradeCompatibility=1.X \\\\\n    --namespace kube-system \\\\\n    > cilium.yaml\n  kubectl apply -f cilium.yaml\n \n .. group-tab:: Helm \n Deploy Cilium release via Helm:\n\n.. parsed-literal::\n\n  helm upgrade cilium |CHART_RELEASE| \\\\\n    --namespace=kube-system \\\\\n    --set upgradeCompatibility=1.X\n \n .. note:: \n Instead of using  --set , you can also save the values relative to your\ndeployment in a YAML file and use it to regenerate the YAML for the latest\nCilium version. Running any of the previous commands will overwrite\nthe existing cluster's :term: ConfigMap  so it is critical to preserve any existing\noptions, either by setting them at the command line or storing them in a\nYAML file, similar to: \n .. code-block:: yaml \n   agent: true\n  upgradeCompatibility: \"1.8\"\n  ipam:\n    mode: \"kubernetes\"\n  k8sServiceHost: \"API_SERVER_IP\"\n  k8sServicePort: \"API_SERVER_PORT\"\n  kubeProxyReplacement: \"true\"\n \n You can then upgrade using this values file by running: \n .. parsed-literal:: \n   helm upgrade cilium |CHART_RELEASE| \\\\\n    --namespace=kube-system \\\\\n    -f my-values.yaml\n \n When upgrading from one minor release to another minor release using\n helm upgrade , do  not  use Helm's  --reuse-values  flag.\nThe  --reuse-values  flag ignores any newly introduced values present in\nthe new release and thus may cause the Helm template to render incorrectly.\nInstead, if you want to reuse the values from your existing installation,\nsave the old values in a values file, check the file for any renamed or\ndeprecated values, and then pass it to the  helm upgrade  command as\ndescribed above. You can retrieve and save the values from an existing\ninstallation with the following command: \n .. code-block:: shell-session \n helm get values cilium --namespace=kube-system -o yaml > old-values.yaml \n The  --reuse-values  flag may only be safely used if the Cilium chart version\nremains unchanged, for example when  helm upgrade  is used to apply\nconfiguration changes without upgrading Cilium. \n Step 3: Rolling Back \n Occasionally, it may be necessary to undo the rollout because a step was missed\nor something went wrong during upgrade. To undo the rollout run: \n .. tabs::\n.. group-tab:: kubectl \n .. code-block:: shell-session\n\n  kubectl rollout undo daemonset/cilium -n kube-system\n \n .. group-tab:: Helm \n .. code-block:: shell-session\n\n  helm history cilium --namespace=kube-system\n  helm rollback cilium [REVISION] --namespace=kube-system\n \n This will revert the latest changes to the Cilium  DaemonSet  and return\nCilium to the state it was in prior to the upgrade. \n .. note:: \n When rolling back after new features of the new minor version have already\nbeen consumed, consult the :ref:`version_notes` to check and prepare for\nincompatible feature use before downgrading/rolling back. This step is only\nrequired after new functionality introduced in the new minor version has\nalready been explicitly used by creating new resources or by opting into\nnew features via the :term:`ConfigMap`.\n \n .. _version_notes:\n.. _upgrade_version_specifics: \n Version Specific Notes \n This section details the upgrade notes specific to |CURRENT_RELEASE|. Read them\ncarefully and take the suggested actions before upgrading Cilium to |CURRENT_RELEASE|.\nFor upgrades to earlier releases, see the\n:prev-docs: upgrade notes to the previous version <operations/upgrade/#upgrade-notes> . \n The only tested upgrade and rollback path is between consecutive minor releases.\nAlways perform upgrades and rollbacks between one minor release at a time.\nAdditionally, always update to the latest patch release of your current version\nbefore attempting an upgrade. \n Tested upgrades are expected to have minimal to no impact on new and existing\nconnections matched by either no Network Policies, or L3/L4 Network Policies only.\nAny traffic flowing via user space proxies (for example, because an L7 policy is\nin place, or using Ingress/Gateway API) will be disrupted during upgrade. Endpoints\ncommunicating via the proxy must reconnect to re-establish connections. \n .. _current_release_required_changes: \n .. _1.19_upgrade_notes: \n 1.19 Upgrade Notes \n \n \n MCS-API CoreDNS configuration recommendation has been updated. See :ref: clustermesh_mcsapi_prereqs  for more details. \n \n \n The  v2alpha1  version of  CiliumLoadBalancerIPPool  CRD has been deprecated in favor of the  v2  version. Please change  apiVersion: cilium.io/v2alpha1 \nto  apiVersion: cilium.io/v2  in your manifests for all  CiliumLoadBalancerIPPool  resources. \n \n \n In a Cluster Mesh environment, network policy ingress and egress selectors currently select by default\nendpoints from all clusters unless one or more clusters are explicitly specified in the policy itself.\nThe  policy-default-local-cluster  flag allows to change this behavior, and only select endpoints\nfrom the local cluster, unless explicitly specified, to improve the default security posture.\nThis option is now enabled by default in Cilium v1.19. If you are using Cilium ClusterMesh and network policies,\nyou need to take action to update your network policies to avoid this change from breaking connectivity for applications\nacross different clusters. See :ref: change_policy_default_local_cluster  for more details and migration recommendations\nto update your network policies. \n \n \n Kafka Network Policy support is deprecated and will be removed in Cilium v1.20. \n \n \n Hubble field mask support was stabilized. In the Observer gRPC API,  GetFlowsRequest.Experimental.field_mask  was removed in favor of  GetFlowsRequest.field_mask . In the Hubble CLI, the  --experimental-field-mask  has been renamed to  --field-mask  and  --experimental-use-default-field-mask  renamed to  -use-default-field-mask  (now  true  by default). \n \n \n enable-remote-node-masquerade  config option is introduced.\nTo masquerade traffic to remote nodes in BPF masquerading mode,\nuse the option  enable-remote-node-masquerade: \"true\" .\nThis option requires  enable-bpf-masquerade: \"true\"  and also either\n enable-ipv4-masquerade: \"true\"  or  enable-ipv6-masquerade: \"true\" \nto SNAT traffic for IPv4 and IPv6, respectively.\nThis flag currently masquerades traffic to node  InternalIP  addresses.\nThis may change in future. See :gh-issue: 35823 \nand :gh-issue: 17177  for further discussion on this topic. \n \n \n Removed Options \n * The previously deprecated ``--bpf-lb-proto-diff`` flag has been removed.\n* The previously deprecated PCAP recorder feature and its accompanying flags (``--enable-recorder``,\n  ``--hubble-recorder-*``) have been removed.\n* The previously deprecated ``--enable-session-affinity``, ``--enable-internal-traffic-policy``, and\n  ``--enable-svc-source-range-check`` flags have been removed. Their corresponding features are\n  enabled by default.\n* The previously deprecated ``--enable-node-port``, ``--enable-host-port``, and ``--enable-external-ips``\n  flags have been removed. To enable the corresponding features, users must set ``--kube-proxy-replacement=true``.\n\nDeprecated Options\n \n Helm Options \n * The Helm option ``clustermesh.enableMCSAPISupport`` has been deprecated in favor of ``clustermesh.mcsapi.enabled``\n  and will be removed in Cilium 1.20.\n* The Helm option ``clustermesh.config.clusters`` now support a new format based on a dict\n  in addition to the previous list format. The new format is recommended for users installing\n  Cilium ClusterMesh without Cilium CLI and could allow you to organize your clusters definition\n  in multiple Helm value files. See the Cilium Helm chart documentation or value file for more details.\n\n\nAgent Options\n \n Cluster Mesh API Server Options \n \n\nBugtool Options\n~~~~~~~~~~~~~~~\n\n\nAdded Metrics\n~~~~~~~~~~~~~\n\nRemoved Metrics\n~~~~~~~~~~~~~~~\n\n* ``k8s_internal_traffic_policy_enabled`` has been removed, because the corresponding feature is enabled by default.\n\nChanged Metrics\n~~~~~~~~~~~~~~~\n\nThe following metrics previously had instances (i.e. for some watcher K8s resource type labels) under ``workqueue_``.\nIn this release any such metrics have been renamed and combined into the correct metric name prefixed with ``cilium_operator_``.\n\nAs well, any remaining Operator k8s workqueue metrics that use the label ``queue_name`` have had it renamed to \n``name`` to be consistent with agent k8s workqueue metrics.\n\n* The metric ``workqueue_adds_total`` has been renamed and combined into to ``cilium_operator_k8s_workqueue_adds_total``, the label ``queue_name`` has been renamed to ``name``.\n* The metric ``workqueue_depth`` has been renamed and combined into ``cilium_operator_k8s_workqueue_adds_total``, the label ``queue_name`` has been renamed to ``name``.\n* The metric ``workqueue_longest_running_processor_seconds`` has been renamed and combined into ``cilium_operator_k8s_workqueue_longest_running_processor_seconds``, the label ``queue_name`` has been renamed to ``name``.\n* The metric ``workqueue_queue_duration_seconds`` has been renamed and combined into ``cilium_operator_k8s_workqueue_queue_duration_seconds``, the label ``queue_name`` has been renamed to ``name``.\n* The metric ``workqueue_retries_total`` has been renamed and combined into ``cilium_operator_k8s_workqueue_retries_total`, the label ``queue_name`` has been renamed to ``name``.\n* The metric ``workqueue_unfinished_work_seconds`` has been renamed and combined into ``cilium_operator_k8s_workqueue_unfinished_work_seconds`, the label ``queue_name`` has been renamed to ``name``.\n* The metric ``workqueue_work_duration_seconds`` has been renamed and combined into ``cilium_operator_k8s_workqueue_work_duration_seconds``, the label ``queue_name`` has been renamed to ``name``.\n\n* ``k8s_client_rate_limiter_duration_seconds`` no longer has labels ``path`` and ``method``.\n\nDeprecated Metrics\n~~~~~~~~~~~~~~~~~~\n\n\nAdvanced\n========\n\nUpgrade Impact\n--------------\n\nUpgrades are designed to have minimal impact on your running deployment.\nNetworking connectivity, policy enforcement and load balancing will remain\nfunctional in general. The following is a list of operations that will not be\navailable during the upgrade:\n\n* API-aware policy rules are enforced in user space proxies and are\n  running as part of the Cilium pod. Upgrading Cilium causes the proxy to\n  restart, which results in a connectivity outage and causes the connection to reset.\n\n* Existing policy will remain effective but implementation of new policy rules\n  will be postponed to after the upgrade has been completed on a particular\n  node.\n\n* Monitoring components such as ``cilium-dbg monitor`` will experience a brief\n  outage while the Cilium pod is restarting. Events are queued up and read\n  after the upgrade. If the number of events exceeds the event buffer size,\n  events will be lost.\n\n\nMigrating from kvstore-backed identities to Kubernetes CRD-backed identities\n----------------------------------------------------------------------------\n\nBeginning with Cilium 1.6, Kubernetes CRD-backed security identities can be\nused for smaller clusters. Along with other changes in 1.6, this allows\nkvstore-free operation if desired. It is possible to migrate identities from an\nexisting kvstore deployment to CRD-backed identities. This minimizes\ndisruptions to traffic as the update rolls out through the cluster.\n\nMigration\n~~~~~~~~~\n\nWhen identities change, existing connections can be disrupted while Cilium\ninitializes and synchronizes with the shared identity store. The disruption\noccurs when new numeric identities are used for existing pods on some instances\nand others are used on others. When converting to CRD-backed identities, it is\npossible to pre-allocate CRD identities so that the numeric identities match\nthose in the kvstore. This allows new and old Cilium instances in the rollout\nto agree.\n\nThere are two ways to achieve this: you can either run a one-off ``cilium preflight migrate-identity`` script\nwhich will perform a point-in-time copy of all identities from the kvstore to CRDs (added in Cilium 1.6), or use the \"Double Write\" identity\nallocation mode which will have Cilium manage identities in both the kvstore and CRD at the same time for a seamless migration (added in Cilium 1.17).\n\nMigration with the ``cilium preflight migrate-identity`` script\n###############################################################\n\nThe ``cilium preflight migrate-identity`` script is a one-off tool that can be used to copy identities from the kvstore into CRDs.\nIt has a couple of limitations:\n\n* If an identity is created in the kvstore after the one-off migration has been completed, it will not be copied into a CRD.\n  This means that you need to perform the migration on a cluster with no identity churn.\n* There is no easy way to revert back to ``--identity-allocation-mode=kvstore`` if something goes wrong after\n  Cilium has been migrated to ``--identity-allocation-mode=crd``\n\nIf these limitations are not acceptable, it is recommended to use the \":ref:`Double Write <double_write_migration>`\" identity allocation mode instead.\n\nThe following steps show an example of performing the migration using the ``cilium preflight migrate-identity`` script.\nIt is safe to re-run the command if desired. It will identify already allocated identities or ones that\ncannot be migrated. Note that identity ``34815`` is migrated, ``17003`` is\nalready migrated, and ``11730`` has a conflict and a new ID allocated for those\nlabels.\n\nThe steps below assume a stable cluster with no new identities created during\nthe rollout. Once Cilium using CRD-backed identities is running, it may begin\nallocating identities in a way that conflicts with older ones in the kvstore.\n\nThe cilium preflight manifest requires etcd support and can be built with:\n\n.. code-block:: shell-session\n\n    helm template cilium \\\n      --namespace=kube-system \\\n      --set preflight.enabled=true \\\n      --set agent=false \\\n      --set config.enabled=false \\\n      --set operator.enabled=false \\\n      --set etcd.enabled=true \\\n      --set etcd.ssl=true \\\n      > cilium-preflight.yaml\n    kubectl create -f cilium-preflight.yaml\n\n\nExample migration\n~~~~~~~~~~~~~~~~~\n\n.. code-block:: shell-session\n\n      $ kubectl exec -n kube-system cilium-pre-flight-check-1234 -- cilium-dbg preflight migrate-identity\n      INFO[0000] Setting up kvstore client\n      INFO[0000] Connecting to etcd server...                  config=/var/lib/cilium/etcd-config.yml endpoints=\"[https://192.168.60.11:2379]\" subsys=kvstore\n      INFO[0000] Setting up kubernetes client\n      INFO[0000] Establishing connection to apiserver          host=\"https://192.168.60.11:6443\" subsys=k8s\n      INFO[0000] Connected to apiserver                        subsys=k8s\n      INFO[0000] Got lease ID 29c66c67db8870c8                 subsys=kvstore\n      INFO[0000] Got lock lease ID 29c66c67db8870ca            subsys=kvstore\n      INFO[0000] Successfully verified version of etcd endpoint  config=/var/lib/cilium/etcd-config.yml endpoints=\"[https://192.168.60.11:2379]\" etcdEndpoint=\"https://192.168.60.11:2379\" subsys=kvstore version=3.3.13\n      INFO[0000] CRD (CustomResourceDefinition) is installed and up-to-date  name=CiliumNetworkPolicy/v2 subsys=k8s\n      INFO[0000] Updating CRD (CustomResourceDefinition)...    name=v2.CiliumEndpoint subsys=k8s\n      INFO[0001] CRD (CustomResourceDefinition) is installed and up-to-date  name=v2.CiliumEndpoint subsys=k8s\n      INFO[0001] Updating CRD (CustomResourceDefinition)...    name=v2.CiliumNode subsys=k8s\n      INFO[0002] CRD (CustomResourceDefinition) is installed and up-to-date  name=v2.CiliumNode subsys=k8s\n      INFO[0002] Updating CRD (CustomResourceDefinition)...    name=v2.CiliumIdentity subsys=k8s\n      INFO[0003] CRD (CustomResourceDefinition) is installed and up-to-date  name=v2.CiliumIdentity subsys=k8s\n      INFO[0003] Listing identities in kvstore\n      INFO[0003] Migrating identities to CRD\n      INFO[0003] Skipped non-kubernetes labels when labelling ciliumidentity. All labels will still be used in identity determination  labels=\"map[]\" subsys=crd-allocator\n      INFO[0003] Skipped non-kubernetes labels when labelling ciliumidentity. All labels will still be used in identity determination  labels=\"map[]\" subsys=crd-allocator\n      INFO[0003] Skipped non-kubernetes labels when labelling ciliumidentity. All labels will still be used in identity determination  labels=\"map[]\" subsys=crd-allocator\n      INFO[0003] Migrated identity                             identity=34815 identityLabels=\"k8s:class=tiefighter;k8s:io.cilium.k8s.policy.cluster=default;k8s:io.cilium.k8s.policy.serviceaccount=default;k8s:io.kubernetes.pod.namespace=default;k8s:org=empire;\"\n      WARN[0003] ID is allocated to a different key in CRD. A new ID will be allocated for the this key  identityLabels=\"k8s:class=deathstar;k8s:io.cilium.k8s.policy.cluster=default;k8s:io.cilium.k8s.policy.serviceaccount=default;k8s:io.kubernetes.pod.namespace=default;k8s:org=empire;\" oldIdentity=11730\n      INFO[0003] Reusing existing global key                   key=\"k8s:class=deathstar;k8s:io.cilium.k8s.policy.cluster=default;k8s:io.cilium.k8s.policy.serviceaccount=default;k8s:io.kubernetes.pod.namespace=default;k8s:org=empire;\" subsys=allocator\n      INFO[0003] New ID allocated for key in CRD               identity=17281 identityLabels=\"k8s:class=deathstar;k8s:io.cilium.k8s.policy.cluster=default;k8s:io.cilium.k8s.policy.serviceaccount=default;k8s:io.kubernetes.pod.namespace=default;k8s:org=empire;\" oldIdentity=11730\n      INFO[0003] ID was already allocated to this key. It is already migrated  identity=17003 identityLabels=\"k8s:class=xwing;k8s:io.cilium.k8s.policy.cluster=default;k8s:io.cilium.k8s.policy.serviceaccount=default;k8s:io.kubernetes.pod.namespace=default;k8s:org=alliance;\"\n\n.. note::\n\n    It is also possible to use the ``--k8s-kubeconfig-path``  and ``--kvstore-opt``\n    ``cilium`` CLI options with the preflight command. The default is to derive the\n    configuration as cilium-agent does.\n\n  .. code-block:: shell-session\n\n        cilium preflight migrate-identity --k8s-kubeconfig-path /var/lib/cilium/cilium.kubeconfig --kvstore etcd --kvstore-opt etcd.config=/var/lib/cilium/etcd-config.yml\n\nOnce the migration is complete, confirm the endpoint identities match by listing the endpoints stored in CRDs and in etcd:\n\n.. code-block:: shell-session\n\n      $ kubectl get ciliumendpoints -A # new CRD-backed endpoints\n      $ kubectl exec -n kube-system cilium-1234 -- cilium-dbg endpoint list # existing etcd-backed endpoints\n\nClearing CRD identities\n~~~~~~~~~~~~~~~~~~~~~~~\n\nIf a migration has gone wrong, it possible to start with a clean slate. Ensure that no Cilium instances are running with ``--identity-allocation-mode=crd`` and execute:\n\n.. code-block:: shell-session\n\n      $ kubectl delete ciliumid --all\n\n.. _double_write_migration:\n\nMigration with the \"Double Write\" identity allocation mode\n##########################################################\n\n.. include:: ../beta.rst\n\nThe \"Double Write\" Identity Allocation Mode allows Cilium to allocate identities as KVStore values *and* as CRDs at the\nsame time. This mode also has two versions: one where the source of truth comes from the kvstore (``--identity-allocation-mode=doublewrite-readkvstore``),\nand one where the source of truth comes from CRDs (``--identity-allocation-mode=doublewrite-readcrd``).\n\nThe high-level migration plan looks as follows:\n\n#. Starting state: Cilium is running in KVStore mode.\n#. Switch Cilium to \"Double Write\" mode with all reads happening from the KVStore. This is almost the same as the\n   pure KVStore mode with the only difference being that all identities are duplicated as CRDs but are not used.\n#. Switch Cilium to \"Double Write\" mode with all reads happening from CRDs. This is equivalent to Cilium running in\n   pure CRD mode but identities will still be updated in the KVStore to allow for the possibility of a fast rollback.\n#. Switch Cilium to CRD mode. The KVStore will no longer be used and will be ready for decommission.\n\nThis will allow you to perform a gradual and seamless migration with the possibility of a fast rollback at steps two or three.\n\nFurthermore, when the \"Double Write\" mode is enabled, the Operator will emit additional metrics to help monitor the\nmigration progress. These metrics can be used for alerting about identity inconsistencies between the KVStore and CRDs.\n\nNote that you can also use this to migrate from CRD to KVStore mode. All operations simply need to be repeated in reverse order.\n\nRollout Instructions\n~~~~~~~~~~~~~~~~~~~~\n\n#. Re-deploy first the Operator and then the Agents with ``--identity-allocation-mode=doublewrite-readkvstore``.\n#. Monitor the Operator metrics and logs to ensure that all identities have converged between the KVStore and CRDs. The relevant metrics emitted by the Operator are:\n\n   * ``cilium_operator_identity_crd_total_count`` and ``cilium_operator_identity_kvstore_total_count`` report the total number of identities in CRDs and KVStore respectively.\n   * ``cilium_operator_identity_crd_only_count`` and ``cilium_operator_identity_kvstore_only_count`` report the number of\n     identities that are only in CRDs or only in the KVStore respectively, to help detect inconsistencies.\n\n   In case further investigation is needed, the Operator logs will contain detailed information about the discrepancies between KVStore and CRD identities.\n   Note that Garbage Collection for KVStore identities and CRD identities happens at slightly different times, so it is possible to see discrepancies in the metrics\n   for certain periods of time, depending on ``--identity-gc-interval`` and ``--identity-heartbeat-timeout`` settings.\n#. Once all identities have converged, re-deploy the Operator and the Agents with ``--identity-allocation-mode=doublewrite-readcrd``.\n   This will cause Cilium to read identities only from CRDs, but continue to write them to the KVStore.\n#. Once you are ready to decommission the KVStore, re-deploy first the Agents and then the Operator with ``--identity-allocation-mode=crd``.\n   This will make Cilium read and write identities only to CRDs.\n#. You can now decommission the KVStore.\n\n.. _change_policy_default_local_cluster:\n\nPreparing for a ``policy-default-local-cluster`` change\n#######################################################\n\nCilium network policies used to implicitly select endpoints from all the clusters.\nCilium 1.18 introduced a new option called ``policy-default-local-cluster`` which\nwill be set by default in Cilium 1.19. This option restricts endpoints selection to\nthe local cluster by default. If you are using ClusterMesh and network policies this\nwill be a **breaking change** and you **need to take action** before upgrading to\nCilium 1.19.\n\nThis new option can be set in the ConfigMap or via the Helm value ``clustermesh.policyDefaultLocalCluster``.\nYou can set ``policy-default-local-cluster`` to ``false`` in Cilium 1.19 to keep the existing behavior,\nhowever this option will be deprecated and eventually removed in a future release so you should plan your\nmigration to set ``policy-default-local-cluster`` to ``true``.\n\nMigrating network policies in practice\n \n The command  cilium clustermesh inspect-policy-default-local-cluster --all-namespaces  can help you\ndiscover all the policies that will change as a result of changing  policy-default-local-cluster .\nYou can also replace  --all-namespaces  with  -n my-namespace  if you want to only inspect\npolicies from a particular namespace. \n Below is an example where there is one network policy that needs to be updated: \n .. code-block:: shell-session \n $ cilium clustermesh inspect-policy-default-local-cluster --all-namespaces\n\n⚠️ CiliumNetworkPolicy 0/1\n        ⚠️ default/allow-from-bar\n\n✅ CiliumClusterWideNetworkPolicy 0/0\n\n✅ NetworkPolicy 0/0\n \n In this situation you have only one CiliumNetworkPolicy which is affected by a\n policy-default-local-cluster  change. Let's take a look at the policy: \n .. code-block:: yaml \n apiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy\nmetadata:\n  name: allow-from-bar\n  namespace: default\nspec:\n  description: \"Allow ingress traffic from bar\"\n  endpointSelector:\n    matchLabels:\n      name: foo\n  ingress:\n  - fromEndpoints:\n    - matchLabels:\n        name: bar\n \n This network policy does not explicitly select a cluster. This means that with  policy-default-local-cluster \nset to  false  it allows traffic coming from  bar  in any clusters connected in your ClusterMesh.\nWith  policy-default-local-cluster  set to  true , this policy allows traffic from  bar  from only\nthe local cluster instead. \n If  foo  and  bar  are always in the same cluster, no further action is necessary. \n In case you want to do this on this individual policy rather than at a global level or that\n bar  is located on a remote cluster you can update your policy like that: \n .. code-block:: yaml \n apiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy\nmetadata:\n  name: allow-from-bar\n  namespace: default\nspec:\n  description: \"Allow ingress traffic from bar\"\n  endpointSelector:\n    matchLabels:\n      name: foo\n  ingress:\n  - fromEndpoints:\n    - matchLabels:\n        name: bar\n        io.cilium.k8s.policy.cluster: fixme-cluster-name\n \n If  bar  is located in multiple cluster you can also use a  matchExpressions \nselecting multiple clusters like that: \n .. code-block:: yaml \n apiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy\nmetadata:\n  name: allow-from-bar\n  namespace: default\nspec:\n  description: \"Allow ingress traffic from bar\"\n  endpointSelector:\n    matchLabels:\n      name: foo\n  ingress:\n  - fromEndpoints:\n    - matchLabels:\n        name: bar\n      matchExpressions:\n        - key: io.cilium.k8s.policy.cluster\n          operator: In\n          values:\n            - fixme-cluster-name-1\n            - fixme-cluster-name-2\n \n Alternatively, you can also allow traffic from  bar  located in every cluster and restore\nthe same behavior as setting  policy-default-local-cluster  to  false  but on this\nindividual policy: \n .. code-block:: yaml \n apiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy\nmetadata:\n  name: allow-from-bar\n  namespace: default\nspec:\n  description: \"Allow ingress traffic from bar\"\n  endpointSelector:\n    matchLabels:\n      name: foo\n  ingress:\n  - fromEndpoints:\n    - matchLabels:\n        name: bar\n      matchExpressions:\n        - key: io.cilium.k8s.policy.cluster\n          operator: Exists\n \n .. _cnp_validation: \n CNP Validation \n Running the CNP Validator will make sure the policies deployed in the cluster\nare valid. It is important to run this validation before an upgrade so it will\nmake sure Cilium has a correct behavior after upgrade. Avoiding doing this\nvalidation might cause Cilium from updating its  NodeStatus  in those invalid\nNetwork Policies as well as in the worst case scenario it might give a false\nsense of security to the user if a policy is badly formatted and Cilium is not\nenforcing that policy due a bad validation schema. This CNP Validator is\nautomatically executed as part of the pre-flight check :ref: pre_flight . \n Start by deployment the  cilium-pre-flight-check  and check if the\n Deployment  shows READY 1/1, if it does not check the pod logs. \n .. code-block:: shell-session \n   $ kubectl get deployment -n kube-system cilium-pre-flight-check -w\n  NAME                      READY   UP-TO-DATE   AVAILABLE   AGE\n  cilium-pre-flight-check   0/1     1            0           12s\n\n  $ kubectl logs -n kube-system deployment/cilium-pre-flight-check -c cnp-validator --previous\n  level=info msg=\"Setting up kubernetes client\"\n  level=info msg=\"Establishing connection to apiserver\" host=\"https://172.20.0.1:443\" subsys=k8s\n  level=info msg=\"Connected to apiserver\" subsys=k8s\n  level=info msg=\"Validating CiliumNetworkPolicy 'default/cidr-rule': OK!\n  level=error msg=\"Validating CiliumNetworkPolicy 'default/cnp-update': unexpected validation error: spec.labels: Invalid value: \\\"string\\\": spec.labels in body must be of type object: \\\"string\\\"\"\n  level=error msg=\"Found invalid CiliumNetworkPolicy\"\n \n In this example, we can see the  CiliumNetworkPolicy  in the  default \nnamespace with the name  cnp-update  is not valid for the Cilium version we\nare trying to upgrade. In order to fix this policy we need to edit it, we can\ndo this by saving the policy locally and modify it. For this example it seems\nthe  .spec.labels  has set an array of strings which is not correct as per\nthe official schema. \n .. code-block:: shell-session \n   $ kubectl get cnp -n default cnp-update -o yaml > cnp-bad.yaml\n  $ cat cnp-bad.yaml\n    apiVersion: cilium.io/v2\n    kind: CiliumNetworkPolicy\n    [...]\n    spec:\n      endpointSelector:\n        matchLabels:\n          id: app1\n      ingress:\n      - fromEndpoints:\n        - matchLabels:\n            id: app2\n        toPorts:\n        - ports:\n          - port: \"80\"\n            protocol: TCP\n      labels:\n      - custom=true\n    [...]\n \n To fix this policy we need to set the  .spec.labels  with the right format and\ncommit these changes into Kubernetes. \n .. code-block:: shell-session \n   $ cat cnp-bad.yaml\n    apiVersion: cilium.io/v2\n    kind: CiliumNetworkPolicy\n    [...]\n    spec:\n      endpointSelector:\n        matchLabels:\n          id: app1\n      ingress:\n      - fromEndpoints:\n        - matchLabels:\n            id: app2\n        toPorts:\n        - ports:\n          - port: \"80\"\n            protocol: TCP\n      labels:\n      - key: \"custom\"\n        value: \"true\"\n    [...]\n  $\n  $ kubectl apply -f ./cnp-bad.yaml\n \n After applying the fixed policy we can delete the pod that was validating the\npolicies so that Kubernetes creates a new pod immediately to verify if the fixed\npolicies are now valid. \n .. code-block:: shell-session \n   $ kubectl delete pod -n kube-system -l k8s-app=cilium-pre-flight-check-deployment\n  pod \"cilium-pre-flight-check-86dfb69668-ngbql\" deleted\n  $ kubectl get deployment -n kube-system cilium-pre-flight-check\n  NAME                      READY   UP-TO-DATE   AVAILABLE   AGE\n  cilium-pre-flight-check   1/1     1            1           55m\n  $ kubectl logs -n kube-system deployment/cilium-pre-flight-check -c cnp-validator\n  level=info msg=\"Setting up kubernetes client\"\n  level=info msg=\"Establishing connection to apiserver\" host=\"https://172.20.0.1:443\" subsys=k8s\n  level=info msg=\"Connected to apiserver\" subsys=k8s\n  level=info msg=\"Validating CiliumNetworkPolicy 'default/cidr-rule': OK!\n  level=info msg=\"Validating CiliumNetworkPolicy 'default/cnp-update': OK!\n  level=info msg=\"All CCNPs and CNPs valid!\"\n \n Once they are valid you can continue with the upgrade process. :ref: cleanup_preflight_check",
  "item_type": "unknown",
  "module_path": "/tmp/cilium-repo/Documentation/operations/upgrade.rst",
  "extracted_at": "2025-09-03T01:13:29.342652Z"
}