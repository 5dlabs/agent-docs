{
  "url": "file:///tmp/cilium-repo/Documentation/operations/performance/tuning.rst",
  "content": ".. only:: not (epub or latex or html) \n WARNING: You are looking at unreleased Cilium documentation.\nPlease use the official rendered version released here:\nhttps://docs.cilium.io\n \n .. _performance_tuning: \n \n Tuning Guide \n \n This guide helps you optimize a Cilium installation for optimal performance. \n Recommendation \n The default out of the box deployment of Cilium is focused on maximum compatibility\nrather than most optimal performance. If you are a performance-conscious user, here\nare the recommended settings for operating Cilium to get the best out of your setup. \n .. note::\nIn-place upgrade by just enabling the config settings on an existing\ncluster is not possible since these tunings change the underlying datapath\nfundamentals and therefore require Pod or even node restarts. \n The best way to consume this for an existing cluster is to utilize per-node\nconfiguration for enabling the tunings only on newly spawned nodes which join\nthe cluster. See the :ref:`per-node-configuration` page for more details.\n \n Each of the settings for the recommended performance profile are described in more\ndetail on this page and in this  KubeCon talk <https://sched.co/1R2s5> __: \n \n netkit device mode \n eBPF host-routing \n BIG TCP for IPv4/IPv6 \n Bandwidth Manager (optional, for BBR congestion control) \n Per-CPU distributed LRU and increased map size ratio \n eBPF clock probe to use jiffies for CT map \n \n Requirements: \n \n Kernel >= 6.8 \n Supported NICs for BIG TCP: mlx4, mlx5, ice \n \n To enable the main settings: \n .. tabs:: \n .. group-tab:: Helm\n\n   .. parsed-literal::\n\n       helm install cilium |CHART_RELEASE| \\\\\n         --namespace kube-system \\\\\n         --set routingMode=native \\\\\n         --set bpf.datapathMode=netkit \\\\\n         --set bpf.masquerade=true \\\\\n         --set bpf.distributedLRU.enabled=true \\\\\n         --set bpf.mapDynamicSizeRatio=0.08 \\\\\n         --set ipv6.enabled=true \\\\\n         --set enableIPv6BIGTCP=true \\\\\n         --set ipv4.enabled=true \\\\\n         --set enableIPv4BIGTCP=true \\\\\n         --set kubeProxyReplacement=true \\\\\n         --set bpfClockProbe=true\n \n For enabling BBR congestion control in addition, consider adding the following\nsettings to the above Helm install: \n .. tabs:: \n .. group-tab:: Helm\n\n   .. parsed-literal::\n\n         --set bandwidthManager.enabled=true \\\\\n         --set bandwidthManager.bbr=true\n \n .. _netkit: \n netkit device mode \n netkit devices provide connectivity for Pods with the goal to improve throughput\nand latency for applications as if they would have resided directly in the host\nnamespace, meaning, it reduces the datapath overhead for network namespaces down\nto zero. The  netkit driver in the kernel <https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/drivers/net/netkit.c> __\nhas been specifically designed for Cilium's needs and replaces the old-style veth\ndevice type. See also the  KubeCon talk on netkit <https://sched.co/1R2s5> __ for\nmore details. \n Cilium utilizes netkit in L3 device mode with blackholing traffic from the Pods\nwhen there is no BPF program attached. The Pod specific BPF programs are attached\ninside the netkit peer device, and can only be managed from the host namespace\nthrough Cilium. netkit in combination with eBPF-based host-routing achieves a\nfast network namespace switch for off-node traffic ingressing into the Pod or\nleaving the Pod. When netkit is enabled, Cilium also utilizes tcx for all\nattachments to non-netkit devices. This is done for higher efficiency as well\nas utilizing BPF links for all Cilium attachments. netkit is available for kernel\n6.8 and onwards and it also supports BIG TCP. Once the base kernels become more\nubiquitous, the veth device mode of Cilium will be deprecated. \n To validate whether your installation is running with netkit, run  cilium status \nin any of the Cilium Pods and look for the line reporting the status for\n\"Device Mode\" which should state \"netkit\". Also, ensure to have eBPF host\nrouting enabled - the reporting status under \"Host Routing\" must state \"BPF\". \n .. warning::\nThis is a beta feature. Please provide feedback and file a GitHub issue if\nyou experience any problems. Known issues with this feature are tracked\n here <https://github.com/cilium/cilium/issues?q=is%3Aissue%20label%3Afeature%2Fnetkit%20> _. \n .. note::\nIn-place upgrade by just enabling netkit on an existing cluster is not\npossible since the CNI plugin cannot simply replace veth with netkit after\nPod creation. Also, running both flavors in parallel is currently not\nsupported. \n The best way to consume this for an existing cluster is to utilize per-node\nconfiguration for enabling netkit on newly spawned nodes which join the\ncluster. See the :ref:`per-node-configuration` page for more details.\n \n Requirements: \n \n Kernel >= 6.8 \n eBPF host-routing \n \n To enable netkit device mode with eBPF host-routing: \n .. tabs:: \n .. group-tab:: Helm\n\n   .. parsed-literal::\n\n       helm install cilium |CHART_RELEASE| \\\\\n         --namespace kube-system \\\\\n         --set routingMode=native \\\\\n         --set bpf.datapathMode=netkit \\\\\n         --set bpf.masquerade=true \\\\\n         --set kubeProxyReplacement=true\n \n .. _eBPF_Host_Routing: \n eBPF Host-Routing \n Even when network routing is performed by Cilium using eBPF, by default network\npackets still traverse some parts of the regular network stack of the node.\nThis ensures that all packets still traverse through all of the iptables hooks\nin case you depend on them. However, they add significant overhead. For exact\nnumbers from our test environment, see :ref: benchmark_throughput  and compare\nthe results for \"Cilium\" and \"Cilium (legacy host-routing)\". \n We introduced  eBPF-based host-routing <https://cilium.io/blog/2020/11/10/cilium-19#veth> _\nin Cilium 1.9 to fully bypass iptables and the upper host stack, and to achieve\na faster network namespace switch compared to regular veth device operation.\nThis option is automatically enabled if your kernel supports it. To validate\nwhether your installation is running with eBPF host-routing, run  cilium status \nin any of the Cilium pods and look for the line reporting the status for\n\"Host Routing\" which should state \"BPF\". \n .. note::\nBPF host routing is incompatible with Istio (see :gh-issue: 36022  for details). \n Requirements: \n \n Kernel >= 5.10 \n eBPF-based kube-proxy replacement \n eBPF-based masquerading \n \n To enable eBPF Host-Routing: \n .. tabs:: \n .. group-tab:: Helm\n\n   .. parsed-literal::\n\n       helm install cilium |CHART_RELEASE| \\\\\n         --namespace kube-system \\\\\n         --set bpf.masquerade=true \\\\\n         --set kubeProxyReplacement=true\n \n Known limitations: \n eBPF host routing optimizes the host-internal packet routing, and packets no\nlonger hit the netfilter tables in the host namespace. Therefore, it is incompatible\nwith features relying on netfilter hooks (for example,  GKE Workload Identities _).\nConfigure  bpf.hostLegacyRouting=true  or leverage :ref: local-redirect-policy \nto work around this limitation. \n .. _ GKE Workload Identities : https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity \n .. _ipv6_big_tcp: \n IPv6 BIG TCP \n IPv6 BIG TCP allows the network stack to prepare larger GSO (transmit) and GRO\n(receive) packets to reduce the number of times the stack is traversed which\nimproves performance and latency. It reduces the CPU load and helps achieve\nhigher speeds (i.e. 100Gbit/s and beyond). \n To pass such packets through the stack BIG TCP adds a temporary Hop-By-Hop header\nafter the IPv6 one which is stripped before transmitting the packet over the wire. \n BIG TCP can operate in a DualStack setup, IPv4 packets will use the old lower\nlimits (64k) if IPv4 BIG TCP is not enabled, and IPv6 packets will use the new\nlarger ones (192k). Both IPv4 BIG TCP and IPv6 BIG TCP can be enabled so that\nboth use the larger one (192k). \n Note that Cilium assumes the default kernel values for GSO and GRO maximum sizes\nare 64k and adjusts them only when necessary, i.e. if BIG TCP is enabled and the\ncurrent GSO/GRO maximum sizes are less than 192k it will try to increase them,\nrespectively when BIG TCP is disabled and the current maximum values are more\nthan 64k it will try to decrease them. \n BIG TCP doesn't require network interface MTU changes. \n .. note::\nIn-place upgrade by just enabling BIG TCP on an existing cluster is currently\nnot possible since Cilium does not have access into Pods after they have been\ncreated. \n The best way to consume this for an existing cluster is to either restart Pods\nor to utilize per-node configuration for enabling BIG TCP on newly spawned nodes\nwhich join the cluster. See the :ref:`per-node-configuration` page for more\ndetails.\n \n Requirements: \n \n Kernel >= 5.19 \n eBPF Host-Routing \n eBPF-based kube-proxy replacement \n eBPF-based masquerading \n Tunneling and encryption disabled \n Supported NICs: mlx4, mlx5, ice \n \n To enable IPv6 BIG TCP: \n .. tabs:: \n .. group-tab:: Helm\n\n   .. parsed-literal::\n\n       helm install cilium |CHART_RELEASE| \\\\\n         --namespace kube-system \\\\\n         --set routingMode=native \\\\\n         --set bpf.masquerade=true \\\\\n         --set ipv6.enabled=true \\\\\n         --set enableIPv6BIGTCP=true \\\\\n         --set kubeProxyReplacement=true\n \n Note that after toggling the IPv6 BIG TCP option the Kubernetes Pods must be\nrestarted for the changes to take effect. \n To validate whether your installation is running with IPv6 BIG TCP,\nrun  cilium status  in any of the Cilium pods and look for the line\nreporting the status for \"IPv6 BIG TCP\" which should state \"enabled\". \n IPv4 BIG TCP \n Similar to IPv6 BIG TCP, IPv4 BIG TCP allows the network stack to prepare larger\nGSO (transmit) and GRO (receive) packets to reduce the number of times the stack\nis traversed which improves performance and latency. It reduces the CPU load and\nhelps achieve higher speeds (i.e. 100Gbit/s and beyond). \n To pass such packets through the stack BIG TCP sets IPv4 tot_len to 0 and uses\nskb->len as the real IPv4 total length. The proper IPv4 tot_len is set before\ntransmitting the packet over the wire. \n BIG TCP can operate in a DualStack setup, IPv6 packets will use the old lower\nlimits (64k) if IPv6 BIG TCP is not enabled, and IPv4 packets will use the new\nlarger ones (192k). Both IPv4 BIG TCP and IPv6 BIG TCP can be enabled so that\nboth use the larger one (192k). \n Note that Cilium assumes the default kernel values for GSO and GRO maximum sizes\nare 64k and adjusts them only when necessary, i.e. if BIG TCP is enabled and the\ncurrent GSO/GRO maximum sizes are less than 192k it will try to increase them,\nrespectively when BIG TCP is disabled and the current maximum values are more\nthan 64k it will try to decrease them. \n BIG TCP doesn't require network interface MTU changes. \n .. note::\nIn-place upgrade by just enabling BIG TCP on an existing cluster is currently\nnot possible since Cilium does not have access into Pods after they have been\ncreated. \n The best way to consume this for an existing cluster is to either restart Pods\nor to utilize per-node configuration for enabling BIG TCP on newly spawned nodes\nwhich join the cluster. See the :ref:`per-node-configuration` page for more\ndetails.\n \n Requirements: \n \n Kernel >= 6.3 \n eBPF Host-Routing \n eBPF-based kube-proxy replacement \n eBPF-based masquerading \n Tunneling and encryption disabled \n Supported NICs: mlx4, mlx5, ice \n \n To enable IPv4 BIG TCP: \n .. tabs:: \n .. group-tab:: Helm\n\n   .. parsed-literal::\n\n       helm install cilium |CHART_RELEASE| \\\\\n         --namespace kube-system \\\\\n         --set routingMode=native \\\\\n         --set bpf.masquerade=true \\\\\n         --set ipv4.enabled=true \\\\\n         --set enableIPv4BIGTCP=true \\\\\n         --set kubeProxyReplacement=true\n \n Note that after toggling the IPv4 BIG TCP option the Kubernetes Pods\nmust be restarted for the changes to take effect. \n To validate whether your installation is running with IPv4 BIG TCP,\nrun  cilium status  in any of the Cilium pods and look for the line\nreporting the status for \"IPv4 BIG TCP\" which should state \"enabled\". \n Bypass iptables Connection Tracking \n For the case when eBPF Host-Routing cannot be used and thus network packets\nstill need to traverse the regular network stack in the host namespace,\niptables can add a significant cost. This traversal cost can be minimized\nby disabling the connection tracking requirement for all Pod traffic, thus\nbypassing the iptables connection tracker. \n Requirements: \n \n Kernel >= 4.19.57, >= 5.1.16, >= 5.2 \n Direct-routing configuration \n eBPF-based kube-proxy replacement \n eBPF-based masquerading or no masquerading \n \n To enable the iptables connection-tracking bypass: \n .. tabs:: \n .. group-tab:: Cilium CLI\n\n   .. parsed-literal::\n\n      cilium install |CHART_VERSION| \\\\\n        --set installNoConntrackIptablesRules=true \\\\\n        --set kubeProxyReplacement=true\n\n.. group-tab:: Helm\n\n   .. parsed-literal::\n\n       helm install cilium |CHART_RELEASE| \\\\\n         --namespace kube-system \\\\\n         --set installNoConntrackIptablesRules=true \\\\\n         --set kubeProxyReplacement=true\n \n Hubble \n Running with Hubble observability enabled can come at the expense of\nperformance. The overhead of Hubble is somewhere between 1-15% depending\non your network traffic patterns and Hubble aggregation settings. \n In clusters with a huge amount of network traffic, cilium-agent might spend\na significant portion of CPU time on processing monitored events and Hubble may\neven lose some events.\nThere are multiple ways to tune Hubble to avoid this. \n Increase Hubble Event Queue Size \n The Hubble Event Queue buffers events after they have been emitted from datapath and\nbefore they are processed by the Hubble subsystem. If this queue is full, because Hubble\ncan't keep up with the amount of emitted events, Cilium will start dropping events.\nThis does not impact traffic, but the events won't be processed by Hubble and won't show\nup in Hubble flows or metrics. \n When this happens you will see log lines similar to the following. \n :: \n level=info msg=\"hubble events queue is processing messages again: NN messages were lost\" subsys=hubble\nlevel=warning msg=\"hubble events queue is full: dropping messages; consider increasing the queue size (hubble-event-queue-size) or provisioning more CPU\" subsys=hubble \n By default the Hubble event queue size is  #CPU * 1024 , or  16384  if your nodes have\nmore than 16 CPU cores. If you encounter event bursts that result in dropped events,\nincreasing this queue size might help. We recommend gradually doubling the queue length\nuntil the drops disappear. If you don't see any improvements after increasing the queue\nlength to 128k, further increasing the event queue size is unlikely to help. \n Be aware that increasing the Hubble event queue size will result in increased memory\nusage. Depending on your traffic pattern, increasing the queue size by  10,000  may\nincrease the memory usage by up to five Megabytes. \n .. tabs:: \n .. group-tab:: Cilium CLI\n\n   .. parsed-literal::\n\n       cilium install |CHART_VERSION| \\\\\n         --set hubble.eventQueueSize=32768\n\n.. group-tab:: Helm\n\n   .. parsed-literal::\n\n       helm install cilium |CHART_RELEASE| \\\\\n         --namespace kube-system \\\\\n         --set hubble.eventQueueSize=32768\n\n.. group-tab:: Per-Node\n\n  If only certain nodes are effected you may also set the queue length on a per-node\n  basis using a :ref:`CiliumNodeConfig object <per-node-configuration>`.\n\n  ::\n\n      apiVersion: cilium.io/v2\n      kind: CiliumNodeConfig\n      metadata:\n        namespace: kube-system\n        name: set-hubble-event-queue\n      spec:\n        nodeSelector:\n          matchLabels:\n            # Update selector to match your nodes\n            io.cilium.update-hubble-event-queue: \"true\"\n        defaults:\n          hubble-event-queue-size: \"32768\"\n \n Increasing the Hubble event queue size can't mitigate a consistently high rate of events\nbeing emitted by Cilium datapath and it does not reduce CPU utilization. For this you\nshould consider increasing the aggregation interval or rate limiting events. \n Increase Aggregation Interval \n By default Cilium generates a tracing event for send packets only on every new\nconnection, any time a packet contains TCP flags that have not been previously\nseen for the packet direction, and on average once per  monitor-aggregation-interval ,\nwhich defaults to 5 seconds. \n Depending on your network traffic patterns, the re-emitting of trace events per\naggregation interval can make up a large part of the total events. Increasing the\naggregation interval may decrease CPU utilization and can prevent lost events. \n The following will set the aggregation interval to 10 seconds. \n .. tabs::\n.. group-tab:: Cilium CLI \n    .. parsed-literal::\n\n       cilium install |CHART_VERSION| \\\\\n         --set bpf.events.monitorInterval=\"10s\"\n\n.. group-tab:: Helm\n\n   .. parsed-literal::\n\n       helm install cilium |CHART_RELEASE| \\\\\n         --namespace kube-system \\\\\n         --set bpf.events.monitorInterval=\"10s\"\n \n Rate Limit Events \n To further prevent high CPU utilization caused by Hubble, you can also set limits on how\nmany events can be generated by datapath code. Two limits are possible to configure: \n \n Rate limit - limits how many events on average can be generated \n Burst limit - limits the number of events that can be generated in a span of 1 second \n \n When both limits are set to 0, no BPF events rate limiting is imposed. \n .. note:: \n Helm configuration for BPF events map rate limiting is experimental and might\nchange in upcoming releases.\n \n .. warning:: \n When BPF events map rate limiting is enabled, Cilium monitor,\nHubble observability, Hubble metrics reliability, and Hubble export functionalities\nmight be impacted due to dropped events.\n \n To enable eBPF Event Rate Limiting with a rate limit of 10,000 and a burst limit of 50,000: \n .. tabs:: \n .. group-tab:: Cilium CLI\n\n   .. parsed-literal::\n\n       cilium install |CHART_VERSION| \\\\\n         --set bpf.events.default.rateLimit=10000 \\\\\n         --set bpf.events.default.burstLimit=50000\n\n.. group-tab:: Helm\n\n   .. parsed-literal::\n\n       helm install cilium |CHART_RELEASE| \\\\\n         --namespace kube-system \\\\\n         --set bpf.events.default.rateLimit=10000 \\\\\n         --set bpf.events.default.burstLimit=50000\n \n You can also choose to stop exposing event types in which you\nare not interested. For instance if you are mainly interested in\ndropped traffic, you can disable \"trace\" events which will likely reduce\nthe overall CPU consumption of the agent. \n .. tabs:: \n .. group-tab:: Cilium CLI\n\n   .. code-block:: shell-session\n\n       cilium config set bpf-events-trace-enabled false\n\n.. group-tab:: Helm\n\n   .. parsed-literal::\n\n       helm install cilium |CHART_RELEASE| \\\\\n         --namespace kube-system \\\\\n         --set bpf.events.trace.enabled=false\n \n .. warning:: \n Suppressing one or more event types will impact ``cilium monitor`` as well as Hubble observability capabilities, metrics and exports.\n \n Disable Hubble \n If all this is not sufficient, in order to optimize for maximum performance,\nyou can disable Hubble: \n .. tabs:: \n .. group-tab:: Cilium CLI\n\n   .. code-block:: shell-session\n\n       cilium hubble disable\n\n.. group-tab:: Helm\n\n   .. parsed-literal::\n\n       helm install cilium |CHART_RELEASE| \\\\\n         --namespace kube-system \\\\\n         --set hubble.enabled=false\n \n MTU \n The maximum transfer unit (MTU) can have a significant impact on the network\nthroughput of a configuration. Cilium will automatically detect the MTU of the\nunderlying network devices. Therefore, if your system is configured to use\njumbo frames, Cilium will automatically make use of it. \n To benefit from this, make sure that your system is configured to use jumbo\nframes if your network allows for it. \n Bandwidth Manager \n Cilium's Bandwidth Manager is responsible for managing network traffic more\nefficiently with the goal of improving overall application latency and throughput. \n Aside from natively supporting Kubernetes Pod bandwidth annotations, the\n Bandwidth Manager <https://cilium.io/blog/2020/11/10/cilium-19#bwmanager> _,\nfirst introduced in Cilium 1.9, is also setting up Fair Queue (FQ)\nqueueing disciplines to support TCP stack pacing (e.g. from EDT/BBR) on all\nexternal-facing network devices as well as setting optimal server-grade sysctl\nsettings for the networking stack. \n Requirements: \n \n eBPF-based kube-proxy replacement \n \n To enable the Bandwidth Manager: \n .. tabs:: \n .. group-tab:: Helm\n\n   .. parsed-literal::\n\n       helm install cilium |CHART_RELEASE| \\\\\n         --namespace kube-system \\\\\n         --set bandwidthManager.enabled=true \\\\\n         --set kubeProxyReplacement=true\n \n To validate whether your installation is running with Bandwidth Manager,\nrun  cilium status  in any of the Cilium pods and look for the line\nreporting the status for \"BandwidthManager\" which should state \"EDT with BPF\". \n BBR congestion control for Pods \n The base infrastructure around MQ/FQ setup provided by Cilium's Bandwidth Manager\nalso allows for use of TCP  BBR congestion control <https://queue.acm.org/detail.cfm?id=3022184> _\nfor Pods. BBR is in particular suitable when Pods are exposed behind Kubernetes\nServices which face external clients from the Internet. BBR achieves higher\nbandwidths and lower latencies for Internet traffic, for example, it has been\n shown <https://cloud.google.com/blog/products/networking/tcp-bbr-congestion-control-comes-to-gcp-your-internet-just-got-faster> _\nthat BBR's throughput can reach as much as 2,700x higher than today's best\nloss-based congestion control and queueing delays can be 25x lower. \n In order for BBR to work reliably for Pods, it requires a 5.18 or higher kernel.\nAs outlined in our  Linux Plumbers 2021 talk <https://lpc.events/event/11/contributions/953/> _,\nthis is needed since older kernels do not retain timestamps of network packets\nwhen switching from Pod to host network namespace. Due to the latter, the kernel's\npacing infrastructure does not function properly in general (not specific to Cilium).\nWe helped fixing this issue for recent kernels to retain timestamps and therefore to\nget BBR for Pods working. \n BBR also needs eBPF Host-Routing in order to retain the network packet's socket\nassociation all the way until the packet hits the FQ queueing discipline on the\nphysical device in the host namespace. \n .. note::\nIn-place upgrade by just enabling BBR on an existing cluster is not possible\nsince Cilium cannot migrate existing sockets over to BBR congestion control. \n The best way to consume this is to either only enable it on newly built clusters,\nto restart Pods on existing clusters, or to utilize per-node configuration for\nenabling BBR on newly spawned nodes which join the cluster. See the\n:ref:`per-node-configuration` page for more details.\n\nNote that the use of BBR could lead to a higher amount of TCP retransmissions\nand more aggressive behavior towards TCP CUBIC connections.\n \n Requirements: \n \n Kernel >= 5.18 \n Bandwidth Manager \n eBPF Host-Routing \n \n To enable the Bandwidth Manager with BBR for Pods: \n .. tabs:: \n .. group-tab:: Helm\n\n   .. parsed-literal::\n\n       helm install cilium |CHART_RELEASE| \\\\\n         --namespace kube-system \\\\\n         --set bandwidthManager.enabled=true \\\\\n         --set bandwidthManager.bbr=true \\\\\n         --set kubeProxyReplacement=true\n \n To validate whether your installation is running with BBR for Pods,\nrun  cilium status  in any of the Cilium pods and look for the line\nreporting the status for \"BandwidthManager\" which should then state\n EDT with BPF  as well as  [BBR] . \n XDP Acceleration \n Cilium has built-in support for accelerating NodePort, LoadBalancer services\nand services with externalIPs for the case where the arriving request needs\nto be pushed back out of the node when the backend is located on a remote node. \n In that case, the network packets do not need to be pushed all the way to the\nupper networking stack, but with the help of XDP, Cilium is able to process\nthose requests right out of the network driver layer. This helps to reduce\nlatency and scale-out of services given a single node's forwarding capacity\nis dramatically increased. The kube-proxy replacement at the XDP layer is\n available from Cilium 1.8 <https://cilium.io/blog/2020/06/22/cilium-18#kubeproxy-removal> _. \n Requirements: \n \n Kernel >= 4.19.57, >= 5.1.16, >= 5.2 \n Native XDP supported driver, check :ref: our driver list <XDP acceleration> \n eBPF-based kube-proxy replacement \n \n To enable the XDP Acceleration, check out :ref: our getting started guide <XDP acceleration>  which also contains instructions for setting it\nup on public cloud providers. \n To validate whether your installation is running with XDP Acceleration,\nrun  cilium status  in any of the Cilium pods and look for the line\nreporting the status for \"XDP Acceleration\" which should say \"Native\". \n eBPF Map Backend Memory \n Changing Cilium's core BPF map memory configuration from a node-global\nLRU memory pool to a distributed per-CPU memory pool helps to avoid\nspinlock contention in the kernel under stress (many CT/NAT element\nallocation and free operations). \n The trade-off is higher memory usage given the per-CPU pools cannot be\nshared anymore, so if a given CPU pool depletes it needs to recycle\nelements via LRU mechanism. It is therefore recommended to not only\nenable  bpf.distributedLRU.enabled  but to also increase the map\nsizing which can be done via  bpf.mapDynamicSizeRatio : \n .. tabs:: \n .. group-tab:: Helm\n\n   .. parsed-literal::\n\n       helm install cilium |CHART_RELEASE| \\\\\n         --namespace kube-system \\\\\n         --set kubeProxyReplacement=true \\\\\n         --set bpf.distributedLRU.enabled=true \\\\\n         --set bpf.mapDynamicSizeRatio=0.08\n \n Note that  bpf.distributedLRU.enabled  is off by default in Cilium for\nlegacy reasons given enabling this setting on-the-fly is disruptive for\nin-flight traffic since the BPF maps have to be recreated. \n It is recommended to use the per-node configuration to gradually phase in\nthis setting for new nodes joining the cluster. Alternatively, upon initial\ncluster creation it is recommended to consider enablement. \n Also,  bpf.distributedLRU.enabled  is currently only supported in combination\nwith  bpf.mapDynamicSizeRatio  as opposed to statically sized map configuration. \n eBPF Map Sizing \n All eBPF maps are created with upper capacity limits. Insertion beyond the\nlimit would fail or constrain the scalability of the datapath. Cilium is\nusing auto-derived defaults based on the given ratio of the total system\nmemory. \n However, the upper capacity limits used by the Cilium agent can be overridden\nfor advanced users. Please refer to the :ref: bpf_map_limitations  guide. \n eBPF Clock Probe \n Cilium can probe the underlying kernel to determine whether BPF supports\nretrieving jiffies instead of ktime. Given Cilium's CT map does not require\nhigh resolution, jiffies is more efficient and the preferred clock source.\nTo enable probing and possibly using jiffies,  bpfClockProbe=true  can\nbe set: \n .. tabs:: \n .. group-tab:: Helm\n\n   .. parsed-literal::\n\n       helm install cilium |CHART_RELEASE| \\\\\n         --namespace kube-system \\\\\n         --set kubeProxyReplacement=true \\\\\n         --set bpfClockProbe=true\n \n Note that  bpfClockProbe  is off by default in Cilium for legacy reasons\ngiven enabling this setting on-the-fly means that previous stored CT map\nentries with ktime as clock source for timestamps would now be interpreted\nas jiffies. \n It is therefore recommended to use the per-node configuration to gradually\nphase in this setting for new nodes joining the cluster. Alternatively, upon\ninitial cluster creation it is recommended to consider enablement. \n To validate whether jiffies is now used run  cilium status --verbose  in\nany of the Cilium Pods and look for the line  Clock Source for BPF . \n Linux Kernel \n In general, we highly recommend using the most recent LTS stable kernel (such\nas >= 5.10) provided by the  kernel community <https://www.kernel.org/category/releases.html> _\nor by a downstream distribution of your choice. The newer the kernel, the more\nlikely it is that various datapath optimizations can be used. \n In our Cilium release blogs, we also regularly highlight some of the eBPF based\nkernel work we conduct which implicitly helps Cilium's datapath performance\nsuch as  replacing retpolines with direct jumps in the eBPF JIT <https://cilium.io/blog/2020/02/18/cilium-17#upstream-linux> _. \n Moreover, the kernel allows to configure several options which will help maximize\nnetwork performance. \n CONFIG_PREEMPT_NONE \n Run a kernel version with  CONFIG_PREEMPT_NONE=y  set. Some Linux\ndistributions offer kernel images with this option set or you can re-compile\nthe Linux kernel.  CONFIG_PREEMPT_NONE=y  is the recommended setting for\nserver workloads. \n Kubernetes \n Set scheduling mode \n By default, the cilium daemonset is configured with an  inter-pod anti-affinity _\nrule. Inter-pod anti-affinity is not recommended for  clusters larger than several hundred nodes _\nas it reduces scheduling throughput of  kube-scheduler _. \n If your cilium daemonset uses a host port (e.g. if prometheus metrics are enabled),\n kube-scheduler  guarantees that only a single pod with that port/protocol is\nscheduled to a node -- effectively offering the same guarantee provided by the\ninter-pod anti-affinity rule. \n To leverage this, consider using  --set scheduling.mode=kube-scheduler  when\ninstalling or upgrading cilium. \n .. note::\nUse caution when changing changing host port numbers. Changing the host port\nnumber removes the  kube-scheduler  guarantee. When a host port number\nmust change, ensure at least one host port number is shared across the upgrade,\nor consider using  --set scheduling.mode=anti-affinity . \n .. _inter-pod anti-affinity: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n.. _clusters larger than several hundred nodes:  https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#:~:text=We%20do%20not%20recommend%20using%20them%20in%20clusters%20larger%20than%20several%20hundred%20nodes.\n.. _kube-scheduler: https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/ \n Further Considerations \n Various additional settings that we recommend help to tune the system for\nspecific workloads and to reduce jitter: \n tuned network-* profiles \n The  tuned <https://tuned-project.org/> _ project offers various profiles to\noptimize for deterministic performance at the cost of increased power consumption,\nthat is,  network-latency  and  network-throughput , for example. To enable\nthe former, run: \n .. code-block:: shell-session \n tuned-adm profile network-latency \n Set CPU governor to performance \n The CPU scaling up and down can impact latency tests and lead to sub-optimal\nperformance. To achieve maximum consistent performance. Set the CPU governor\nto  performance : \n .. code-block:: bash \n for CPU in /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor; do\necho performance > $CPU\ndone \n Stop  irqbalance  and pin the NIC interrupts to specific CPUs \n In case you are running  irqbalance , consider disabling it as it might\nmigrate the NIC's IRQ handling among CPUs and can therefore cause non-deterministic\nperformance: \n .. code-block:: shell-session \n killall irqbalance \n We highly recommend to pin the NIC interrupts to specific CPUs in order to\nallow for maximum workload isolation! \n See  this script <https://github.com/borkmann/netperf_scripts/blob/master/set_irq_affinity> _\nfor details and initial pointers on how to achieve this. Note that pinning the\nqueues can potentially vary in setup between different drivers. \n We generally also recommend to check various documentation and performance tuning\nguides from NIC vendors on this matter such as from\n Mellanox <https://enterprise-support.nvidia.com/s/article/performance-tuning-for-mellanox-adapters> ,\n Intel <https://www.intel.com/content/www/us/en/support/articles/000005811/network-and-i-o/ethernet-products.html> \nor others for more information.",
  "item_type": "unknown",
  "module_path": "/tmp/cilium-repo/Documentation/operations/performance/tuning.rst",
  "extracted_at": "2025-09-03T01:13:29.358477Z"
}