{
  "url": "file:///tmp/cilium-repo/Documentation/operations/troubleshooting.rst",
  "content": ".. only:: not (epub or latex or html) \n WARNING: You are looking at unreleased Cilium documentation.\nPlease use the official rendered version released here:\nhttps://docs.cilium.io\n \n .. _admin_guide: \n ###############\nTroubleshooting\n############### \n This document describes how to troubleshoot Cilium in different deployment\nmodes. It focuses on a full deployment of Cilium within a datacenter or public\ncloud. If you are just looking for a simple way to experiment, we highly\nrecommend trying out the :ref: getting_started  guide instead. \n This guide assumes that you have read the :ref: network_root  and  security_root  which explain all\nthe components and concepts. \n We use GitHub issues to maintain a list of  Cilium Frequently Asked Questions (FAQ) _. You can also check there to see if your question(s) is already\naddressed. \n Component & Cluster Health \n Kubernetes \n An initial overview of Cilium can be retrieved by listing all pods to verify\nwhether all pods have the status  Running : \n .. code-block:: shell-session \n $ kubectl -n kube-system get pods -l k8s-app=cilium\nNAME           READY     STATUS    RESTARTS   AGE\ncilium-2hq5z   1/1       Running   0          4d\ncilium-6kbtz   1/1       Running   0          4d\ncilium-klj4b   1/1       Running   0          4d\ncilium-zmjj9   1/1       Running   0          4d \n If Cilium encounters a problem that it cannot recover from, it will\nautomatically report the failure state via  cilium-dbg status  which is regularly\nqueried by the Kubernetes liveness probe to automatically restart Cilium pods.\nIf a Cilium pod is in state  CrashLoopBackoff  then this indicates a\npermanent failure scenario. \n Detailed Status \n \nIf a particular Cilium pod is not in running state, the status and health of\nthe agent on that node can be retrieved by running ``cilium-dbg status`` in the\ncontext of that pod:\n\n.. code-block:: shell-session\n\n   $ kubectl -n kube-system exec cilium-2hq5z -- cilium-dbg status\n   KVStore:                Ok   etcd: 1/1 connected: http://demo-etcd-lab--a.etcd.tgraf.test1.lab.corp.isovalent.link:2379 - 3.2.5 (Leader)\n   ContainerRuntime:       Ok   docker daemon: OK\n   Kubernetes:             Ok   OK\n   Kubernetes APIs:        [\"cilium/v2::CiliumNetworkPolicy\", \"networking.k8s.io/v1::NetworkPolicy\", \"core/v1::Service\", \"core/v1::Endpoint\", \"core/v1::Node\", \"CustomResourceDefinition\"]\n   Cilium:                 Ok   OK\n   NodeMonitor:            Disabled\n   Cilium health daemon:   Ok\n   Controller Status:      14/14 healthy\n   Proxy Status:           OK, ip 10.2.0.172, port-range 10000-20000\n   Cluster health:   4/4 reachable   (2018-06-16T09:49:58Z)\n\nAlternatively, the ``k8s-cilium-exec.sh`` script can be used to run ``cilium-dbg\nstatus`` on all nodes. This will provide detailed status and health information\nof all nodes in the cluster:\n\n.. code-block:: shell-session\n\n   curl -sLO https://raw.githubusercontent.com/cilium/cilium/main/contrib/k8s/k8s-cilium-exec.sh\n   chmod +x ./k8s-cilium-exec.sh\n\n... and run ``cilium-dbg status`` on all nodes:\n\n.. code-block:: shell-session\n\n   $ ./k8s-cilium-exec.sh cilium-dbg status\n   KVStore:                Ok   Etcd: http://127.0.0.1:2379 - (Leader) 3.1.10\n   ContainerRuntime:       Ok\n   Kubernetes:             Ok   OK\n   Kubernetes APIs:        [\"networking.k8s.io/v1beta1::Ingress\", \"core/v1::Node\", \"CustomResourceDefinition\", \"cilium/v2::CiliumNetworkPolicy\", \"networking.k8s.io/v1::NetworkPolicy\", \"core/v1::Service\", \"core/v1::Endpoint\"]\n   Cilium:                 Ok   OK\n   NodeMonitor:            Listening for events on 2 CPUs with 64x4096 of shared memory\n   Cilium health daemon:   Ok\n   Controller Status:      7/7 healthy\n   Proxy Status:           OK, ip 10.15.28.238, 0 redirects, port-range 10000-20000\n   Cluster health:   1/1 reachable   (2018-02-27T00:24:34Z)\n\nDetailed information about the status of Cilium can be inspected with the\n``cilium-dbg status --verbose`` command. Verbose output includes detailed IPAM state\n(allocated addresses), Cilium controller status, and details of the Proxy\nstatus.\n\n.. _ts_agent_logs:\n\nLogs\n~~~~\n\nTo retrieve log files of a cilium pod, run (replace ``cilium-1234`` with a pod\nname returned by ``kubectl -n kube-system get pods -l k8s-app=cilium``)\n\n.. code-block:: shell-session\n\n   kubectl -n kube-system logs --timestamps cilium-1234\n\nIf the cilium pod was already restarted due to the liveness problem after\nencountering an issue, it can be useful to retrieve the logs of the pod before\nthe last restart:\n\n.. code-block:: shell-session\n\n   kubectl -n kube-system logs --timestamps -p cilium-1234\n\nGeneric\n-------\n\nWhen logged in a host running Cilium, the cilium CLI can be invoked directly,\ne.g.:\n\n.. code-block:: shell-session\n\n   $ cilium-dbg status\n   KVStore:                Ok   etcd: 1/1 connected: https://192.168.60.11:2379 - 3.2.7 (Leader)\n   ContainerRuntime:       Ok\n   Kubernetes:             Ok   OK\n   Kubernetes APIs:        [\"core/v1::Endpoint\", \"networking.k8s.io/v1beta1::Ingress\", \"core/v1::Node\", \"CustomResourceDefinition\", \"cilium/v2::CiliumNetworkPolicy\", \"networking.k8s.io/v1::NetworkPolicy\", \"core/v1::Service\"]\n   Cilium:                 Ok   OK\n   NodeMonitor:            Listening for events on 2 CPUs with 64x4096 of shared memory\n   Cilium health daemon:   Ok\n   IPv4 address pool:      261/65535 allocated\n   IPv6 address pool:      4/4294967295 allocated\n   Controller Status:      20/20 healthy\n   Proxy Status:           OK, ip 10.0.28.238, port-range 10000-20000\n   Hubble:                 Ok      Current/Max Flows: 2542/4096 (62.06%), Flows/s: 164.21      Metrics: Disabled\n   Cluster health:         2/2 reachable   (2018-04-11T15:41:01Z)\n\n.. _hubble_troubleshooting:\n\nObserving Flows with Hubble\n===========================\n\nHubble is a built-in observability tool which allows you to inspect recent flow\nevents on all endpoints managed by Cilium.\n\nEnsure Hubble is running correctly\n----------------------------------\n\nTo ensure the Hubble client can connect to the Hubble server running inside\nCilium, you may use the ``hubble status`` command from within a Cilium pod:\n\n.. code-block:: shell-session\n\n   $ hubble status\n   Healthcheck (via unix:///var/run/cilium/hubble.sock): Ok\n   Current/Max Flows: 4095/4095 (100.00%)\n   Flows/s: 164.21\n\n``cilium-agent`` must be running with the ``--enable-hubble`` option (default) in order\nfor the Hubble server to be enabled. When deploying Cilium with Helm, make sure\nto set the ``hubble.enabled=true`` value.\n\nTo check if Hubble is enabled in your deployment, you may look for the\nfollowing output in ``cilium-dbg status``:\n\n.. code-block:: shell-session\n\n   $ cilium status\n   ...\n   Hubble:   Ok   Current/Max Flows: 4095/4095 (100.00%), Flows/s: 164.21   Metrics: Disabled\n   ...\n\n.. note::\n   Pods need to be managed by Cilium in order to be observable by Hubble.\n   See how to :ref:`ensure a pod is managed by Cilium<ensure_managed_pod>`\n   for more details.\n\nObserving flows of a specific pod\n---------------------------------\n\nIn order to observe the traffic of a specific pod, you will first have to\n:ref:`retrieve the name of the cilium instance managing it<retrieve_cilium_pod>`.\nThe Hubble CLI is part of the Cilium container image and can be accessed via\n``kubectl exec``. The following query for example will show all events related\nto flows which either originated or terminated in the ``default/tiefighter`` pod\nin the last three minutes:\n\n.. code-block:: shell-session\n\n   $ kubectl exec -n kube-system cilium-77lk6 -- hubble observe --since 3m --pod default/tiefighter\n   May  4 12:47:08.811: default/tiefighter:53875 -> kube-system/coredns-74ff55c5b-66f4n:53 to-endpoint FORWARDED (UDP)\n   May  4 12:47:08.811: default/tiefighter:53875 -> kube-system/coredns-74ff55c5b-66f4n:53 to-endpoint FORWARDED (UDP)\n   May  4 12:47:08.811: default/tiefighter:53875 <- kube-system/coredns-74ff55c5b-66f4n:53 to-endpoint FORWARDED (UDP)\n   May  4 12:47:08.811: default/tiefighter:53875 <- kube-system/coredns-74ff55c5b-66f4n:53 to-endpoint FORWARDED (UDP)\n   May  4 12:47:08.811: default/tiefighter:50214 <> default/deathstar-c74d84667-cx5kp:80 to-overlay FORWARDED (TCP Flags: SYN)\n   May  4 12:47:08.812: default/tiefighter:50214 <- default/deathstar-c74d84667-cx5kp:80 to-endpoint FORWARDED (TCP Flags: SYN, ACK)\n   May  4 12:47:08.812: default/tiefighter:50214 <> default/deathstar-c74d84667-cx5kp:80 to-overlay FORWARDED (TCP Flags: ACK)\n   May  4 12:47:08.812: default/tiefighter:50214 <> default/deathstar-c74d84667-cx5kp:80 to-overlay FORWARDED (TCP Flags: ACK, PSH)\n   May  4 12:47:08.812: default/tiefighter:50214 <- default/deathstar-c74d84667-cx5kp:80 to-endpoint FORWARDED (TCP Flags: ACK, PSH)\n   May  4 12:47:08.812: default/tiefighter:50214 <> default/deathstar-c74d84667-cx5kp:80 to-overlay FORWARDED (TCP Flags: ACK, FIN)\n   May  4 12:47:08.812: default/tiefighter:50214 <- default/deathstar-c74d84667-cx5kp:80 to-endpoint FORWARDED (TCP Flags: ACK, FIN)\n   May  4 12:47:08.812: default/tiefighter:50214 <> default/deathstar-c74d84667-cx5kp:80 to-overlay FORWARDED (TCP Flags: ACK)\n\nYou may also use ``-o json`` to obtain more detailed information about each\nflow event.\n\n.. note::\n   **Hubble Relay**  allows you to query multiple Hubble instances\n   simultaneously without having to first manually target a specific node.  See\n   `Observing flows with Hubble Relay`_ for more information.\n\nObserving flows with Hubble Relay\n=================================\n\nHubble Relay is a service which allows to query multiple Hubble instances\nsimultaneously and aggregate the results. See :ref:`hubble_setup` to enable\nHubble Relay if it is not yet enabled and install the Hubble CLI on your local\nmachine.\n\n.. include:: /observability/hubble/port-forward.rst\n\nYou can verify that Hubble Relay can be reached by using the Hubble CLI and\nrunning the following command from your local machine:\n\n.. code-block:: shell-session\n\n   hubble status -P\n\nThis command should return an output similar to the following:\n\n::\n\n   Healthcheck (via 127.0.0.1:4245): Ok\n   Current/Max Flows: 16380/16380 (100.00%)\n   Flows/s: 46.19\n   Connected Nodes: 4/4\n\nYou may see details about nodes that Hubble Relay is connected to by running\nthe following command:\n\n.. code-block:: shell-session\n\n   $ hubble list nodes -P\n   NAME              STATUS      AGE     FLOWS/S   CURRENT/MAX-FLOWS\n   cluster/node-cp   Connected   2m30s   13.94     2227/4095 ( 54.38%)\n   cluster/node-w1   Connected   2m31s   51.37     5108/9840 ( 51.91%)\n\nAs Hubble Relay shares the same API as individual Hubble instances, you may\nfollow the `Observing flows with Hubble`_ section keeping in mind that\nlimitations with regards to what can be seen from individual Hubble instances no\nlonger apply.\n\nConnectivity Problems\n=====================\n\nCilium connectivity tests\n------------------------------------\n\nThe Cilium connectivity test deploys a series of services, deployments, and\nCiliumNetworkPolicy which will use various connectivity paths to connect to\neach other. Connectivity paths include with and without service load-balancing\nand various network policy combinations.\n\n.. note::\n   The connectivity tests this will only work in a namespace with no other pods\n   or network policies applied. If there is a Cilium Clusterwide Network Policy\n   enabled, that may also break this connectivity check.\n\nTo run the connectivity tests create an isolated test namespace called\n``cilium-test`` to deploy the tests with.\n\n.. parsed-literal::\n\n   kubectl create ns cilium-test\n   kubectl apply --namespace=cilium-test -f \\ |SCM_WEB|\\/examples/kubernetes/connectivity-check/connectivity-check.yaml\n\nThe tests cover various functionality of the system. Below we call out each test\ntype. If tests pass, it suggests functionality of the referenced subsystem.\n\n+----------------------------+-----------------------------+-------------------------------+-----------------------------+----------------------------------------+\n| Pod-to-pod (intra-host)    | Pod-to-pod (inter-host)     | Pod-to-service (intra-host)   | Pod-to-service (inter-host) | Pod-to-external resource               |\n+============================+=============================+===============================+=============================+========================================+\n| eBPF routing is functional | Data plane, routing, network| eBPF service map lookup       | VXLAN overlay port if used  | Egress, CiliumNetworkPolicy, masquerade|\n+----------------------------+-----------------------------+-------------------------------+-----------------------------+----------------------------------------+\n\nThe pod name indicates the connectivity\nvariant and the readiness and liveness gate indicates success or failure of the\ntest:\n\n.. code-block:: shell-session\n\n   $ kubectl get pods -n cilium-test\n   NAME                                                    READY   STATUS    RESTARTS   AGE\n   echo-a-6788c799fd-42qxx                                 1/1     Running   0          69s\n   echo-b-59757679d4-pjtdl                                 1/1     Running   0          69s\n   echo-b-host-f86bd784d-wnh4v                             1/1     Running   0          68s\n   host-to-b-multi-node-clusterip-585db65b4d-x74nz         1/1     Running   0          68s\n   host-to-b-multi-node-headless-77c64bc7d8-kgf8p          1/1     Running   0          67s\n   pod-to-a-allowed-cnp-87b5895c8-bfw4x                    1/1     Running   0          68s\n   pod-to-a-b76ddb6b4-2v4kb                                1/1     Running   0          68s\n   pod-to-a-denied-cnp-677d9f567b-kkjp4                    1/1     Running   0          68s\n   pod-to-b-intra-node-nodeport-8484fb6d89-bwj8q           1/1     Running   0          68s\n   pod-to-b-multi-node-clusterip-f7655dbc8-h5bwk           1/1     Running   0          68s\n   pod-to-b-multi-node-headless-5fd98b9648-5bjj8           1/1     Running   0          68s\n   pod-to-b-multi-node-nodeport-74bd8d7bd5-kmfmm           1/1     Running   0          68s\n   pod-to-external-1111-7489c7c46d-jhtkr                   1/1     Running   0          68s\n   pod-to-external-fqdn-allow-google-cnp-b7b6bcdcb-97p75   1/1     Running   0          68s\n\nInformation about test failures can be determined by describing a failed test\npod\n\n.. code-block:: shell-session\n\n   $ kubectl describe pod pod-to-b-intra-node-hostport\n     Warning  Unhealthy  6s (x6 over 56s)   kubelet, agent1    Readiness probe failed: curl: (7) Failed to connect to echo-b-host-headless port 40000: Connection refused\n     Warning  Unhealthy  2s (x3 over 52s)   kubelet, agent1    Liveness probe failed: curl: (7) Failed to connect to echo-b-host-headless port 40000: Connection refused\n\n.. _cluster_connectivity_health:\n\nChecking cluster connectivity health\n------------------------------------\n\nCilium can rule out network fabric related issues when troubleshooting\nconnectivity issues by providing reliable health and latency probes between all\ncluster nodes and a simulated workload running on each node.\n\nBy default when Cilium is run, it launches instances of ``cilium-health`` in\nthe background to determine the overall connectivity status of the cluster. This\ntool periodically runs bidirectional traffic across multiple paths through the\ncluster and through each node using different protocols to determine the health\nstatus of each path and protocol. At any point in time, cilium-health may be\nqueried for the connectivity status of the last probe.\n\n.. code-block:: shell-session\n\n   $ kubectl -n kube-system exec -ti cilium-2hq5z -- cilium-health status --verbose\n   Probe time:   2018-06-16T09:51:58Z\n   Nodes:\n     ip-172-0-52-116.us-west-2.compute.internal (localhost):\n       Host connectivity to 172.0.52.116:\n         ICMP to stack: OK, RTT=315.254µs\n         HTTP to agent: OK, RTT=368.579µs\n       Endpoint connectivity to 10.2.0.183:\n         ICMP to stack: OK, RTT=190.658µs\n         HTTP to agent: OK, RTT=536.665µs\n     ip-172-0-117-198.us-west-2.compute.internal:\n       Host connectivity to 172.0.117.198:\n         ICMP to stack: OK, RTT=1.009679ms\n         HTTP to agent: OK, RTT=1.808628ms\n       Endpoint connectivity to 10.2.1.234:\n         ICMP to stack: OK, RTT=1.016365ms\n         HTTP to agent: OK, RTT=2.29877ms\n\nFor each node, the connectivity will be displayed for each protocol and path,\nboth to the node itself and to an endpoint on that node. The latency specified\nis a snapshot at the last time a probe was run, which is typically once per\nminute. The ICMP connectivity row represents Layer 3 connectivity to the\nnetworking stack, while the HTTP connectivity row represents connection to an\ninstance of the ``cilium-health`` agent running on the host or as an endpoint.\n\n.. _monitor:\n\nMonitoring Datapath State\n-------------------------\n\nSometimes you may experience broken connectivity, which may be due to a\nnumber of different causes. A main cause can be unwanted packet drops on\nthe networking level. The tool\n``cilium-dbg monitor`` allows you to quickly inspect and see if and where packet\ndrops happen. Following is an example output (use ``kubectl exec`` as in\nprevious examples if running with Kubernetes):\n\n.. code-block:: shell-session\n\n   $ kubectl -n kube-system exec -ti cilium-2hq5z -- cilium-dbg monitor --type drop\n   Listening for events on 2 CPUs with 64x4096 of shared memory\n   Press Ctrl-C to quit\n   xx drop (Policy denied) to endpoint 25729, identity 261->264: fd02::c0a8:210b:0:bf00 -> fd02::c0a8:210b:0:6481 EchoRequest\n   xx drop (Policy denied) to endpoint 25729, identity 261->264: fd02::c0a8:210b:0:bf00 -> fd02::c0a8:210b:0:6481 EchoRequest\n   xx drop (Policy denied) to endpoint 25729, identity 261->264: 10.11.13.37 -> 10.11.101.61 EchoRequest\n   xx drop (Policy denied) to endpoint 25729, identity 261->264: 10.11.13.37 -> 10.11.101.61 EchoRequest\n   xx drop (Invalid destination mac) to endpoint 0, identity 0->0: fe80::5c25:ddff:fe8e:78d8 -> ff02::2 RouterSolicitation\n\nThe above indicates that a packet to endpoint ID ``25729`` has been dropped due\nto violation of the Layer 3 policy.\n\nHandling drop (CT: Map insertion failed)\n \n If connectivity fails and  cilium-dbg monitor --type drop  shows  xx drop (CT: Map insertion failed) , then it is likely that the connection tracking table\nis filling up and the automatic adjustment of the garbage collector interval is\ninsufficient. \n Setting  --conntrack-gc-interval  to an interval lower than the current value\nmay help. This controls the time interval between two garbage collection runs. \n By default  --conntrack-gc-interval  is set to 0 which translates to\nusing a dynamic interval. In that case, the interval is updated after each\ngarbage collection run depending on how many entries were garbage collected.\nIf very few or no entries were garbage collected, the interval will increase;\nif many entries were garbage collected, it will decrease. The current interval\nvalue is reported in the Cilium agent logs. \n Alternatively, the value for  bpf-ct-global-any-max  and\n bpf-ct-global-tcp-max  can be increased. Setting both of these options will\nbe a trade-off of CPU for  conntrack-gc-interval , and for\n bpf-ct-global-any-max  and  bpf-ct-global-tcp-max  the amount of memory\nconsumed. You can track conntrack garbage collection related metrics such as\n datapath_conntrack_gc_runs_total  and  datapath_conntrack_gc_entries  to\nget visibility into garbage collection runs. Refer to :ref: metrics  for more\ndetails. \n Enabling datapath debug messages \n \nBy default, datapath debug messages are disabled, and therefore not shown in\n``cilium-dbg monitor -v`` output. To enable them, add ``\"datapath\"`` to\nthe ``debug-verbose`` option.\n\nPolicy Troubleshooting\n======================\n\n.. _ensure_managed_pod:\n\nEnsure pod is managed by Cilium\n-------------------------------\n\nA potential cause for policy enforcement not functioning as expected is that\nthe networking of the pod selected by the policy is not being managed by\nCilium. The following situations result in unmanaged pods:\n\n* The pod is running in host networking and will use the host's IP address\n  directly. Such pods have full network connectivity but Cilium will not\n  provide security policy enforcement for such pods by default. To enforce\n  policy against these pods, either set ``hostNetwork`` to false or use\n  :ref:`HostPolicies`.\n\n* The pod was started before Cilium was deployed. Cilium only manages pods\n  that have been deployed after Cilium itself was started. Cilium will not\n  provide security policy enforcement for such pods. These pods should be\n  restarted in order to ensure that Cilium can provide security policy\n  enforcement.\n\nIf pod networking is not managed by Cilium. Ingress and egress policy rules\nselecting the respective pods will not be applied. See the section\n:ref:`network_policy` for more details.\n\nFor a quick assessment of whether any pods are not managed by Cilium, the\n`Cilium CLI <https://github.com/cilium/cilium-cli>`_ will print the number\nof managed pods. If this prints that all of the pods are managed by Cilium,\nthen there is no problem:\n\n.. code-block:: shell-session\n\n   $ cilium status\n       /¯¯\\\n    /¯¯\\__/¯¯\\    Cilium:         OK\n    \\__/¯¯\\__/    Operator:       OK\n    /¯¯\\__/¯¯\\    Hubble:         OK\n    \\__/¯¯\\__/    ClusterMesh:    disabled\n       \\__/\n\n   Deployment        cilium-operator    Desired: 2, Ready: 2/2, Available: 2/2\n   Deployment        hubble-relay       Desired: 1, Ready: 1/1, Available: 1/1\n   Deployment        hubble-ui          Desired: 1, Ready: 1/1, Available: 1/1\n   DaemonSet         cilium             Desired: 2, Ready: 2/2, Available: 2/2\n   Containers:       cilium-operator    Running: 2\n                     hubble-relay       Running: 1\n                     hubble-ui          Running: 1\n                     cilium             Running: 2\n   Cluster Pods:     5/5 managed by Cilium\n   ...\n\nYou can run the following script to list the pods which are *not* managed by\nCilium:\n\n.. code-block:: shell-session\n\n   $ curl -sLO https://raw.githubusercontent.com/cilium/cilium/main/contrib/k8s/k8s-unmanaged.sh\n   $ chmod +x k8s-unmanaged.sh\n   $ ./k8s-unmanaged.sh\n   kube-system/cilium-hqpk7\n   kube-system/kube-addon-manager-minikube\n   kube-system/kube-dns-54cccfbdf8-zmv2c\n   kube-system/kubernetes-dashboard-77d8b98585-g52k5\n   kube-system/storage-provisioner\n\nUnderstand the rendering of your policy\n---------------------------------------\n\nThere are always multiple ways to approach a problem. Cilium can provide the\nrendering of the aggregate policy provided to it, leaving you to simply compare\nwith what you expect the policy to actually be rather than search (and\npotentially overlook) every policy. At the expense of reading a very large dump\nof an endpoint, this is often a faster path to discovering errant policy\nrequests in the Kubernetes API.\n\nStart by finding the endpoint you are debugging from the following list. There\nare several cross references for you to use in this list, including the IP\naddress and pod labels:\n\n.. code-block:: shell-session\n\n    kubectl -n kube-system exec -ti cilium-q8wvt -- cilium-dbg endpoint list\n\nWhen you find the correct endpoint, the first column of every row is the\nendpoint ID. Use that to dump the full endpoint information:\n\n.. code-block:: shell-session\n\n    kubectl -n kube-system exec -ti cilium-q8wvt -- cilium-dbg endpoint get 59084\n\n.. image:: images/troubleshooting_policy.png\n    :align: center\n\nImporting this dump into a JSON-friendly editor can help browse and navigate the\ninformation here. At the top level of the dump, there are two nodes of note:\n\n* ``spec``: The desired state of the endpoint\n* ``status``: The current state of the endpoint\n\nThis is the standard Kubernetes control loop pattern. Cilium is the controller\nhere, and it is iteratively working to bring the ``status`` in line with the\n``spec``.\n\nOpening the ``status``, we can drill down through ``policy.realized.l4``. Do\nyour ``ingress`` and ``egress`` rules match what you expect? If not, the\nreference to the errant rules can be found in the ``derived-from-rules`` node.\n\nPolicymap pressure and overflow\n-------------------------------\n\nThe most important step in debugging policymap pressure is finding out which\nnode(s) are impacted.\n\nThe ``cilium_bpf_map_pressure{map_name=\"cilium_policy_v2_*\"}`` metric monitors the\nendpoint's BPF policymap pressure. This metric exposes the maximum BPF map\npressure on the node, meaning the policymap experiencing the most pressure on a\nparticular node.\n\nOnce the node is known, the troubleshooting steps are as follows:\n\n1. Find the Cilium pod on the node experiencing the problematic policymap\n   pressure and obtain a shell via ``kubectl exec``.\n2. Use ``cilium policy selectors`` to get an overview of which selectors are\n   selecting many identities.\n3. The type of selector tells you what sort of policy rule could be having an\n   impact. The three existing types of selectors are explained below, each with\n   specific steps depending on the selector. See the steps below corresponding\n   to the type of selector.\n4. Consider bumping the policymap size as a last resort. However, keep in mind\n   the following implications:\n\n   * Increased memory consumption for each policymap.\n   * Generally, as identities increase in the cluster, the more work Cilium\n     performs.\n   * At a broader level, if the policy posture is such that all or nearly all\n     identities are selected, this suggests that the posture is too permissive.\n\n+---------------+------------------------------------------------------------------------------------------------------------+\n| Selector type | Form in ``cilium policy selectors`` output                                                                 |\n+===============+============================================================================================================+\n| CIDR          | ``&LabelSelector{MatchLabels:map[string]string{cidr.1.1.1.1/32: ,}``                                       |\n+---------------+------------------------------------------------------------------------------------------------------------+\n| FQDN          | ``MatchName: , MatchPattern: *``                                                                           |\n+---------------+------------------------------------------------------------------------------------------------------------+\n| Label         | ``&LabelSelector{MatchLabels:map[string]string{any.name: curl,k8s.io.kubernetes.pod.namespace: default,}`` |\n+---------------+------------------------------------------------------------------------------------------------------------+\n\nAn example output of ``cilium policy selectors``:\n\n.. code-block:: shell-session\n\n    root@kind-worker:/home/cilium# cilium policy selectors\n    SELECTOR                                                                                                                                                            LABELS                          USERS   IDENTITIES\n    &LabelSelector{MatchLabels:map[string]string{k8s.io.kubernetes.pod.namespace: kube-system,k8s.k8s-app: kube-dns,},MatchExpressions:[]LabelSelectorRequirement{},}   default/tofqdn-dns-visibility   1       16500\n    &LabelSelector{MatchLabels:map[string]string{reserved.none: ,},MatchExpressions:[]LabelSelectorRequirement{},}                                                      default/tofqdn-dns-visibility   1\n    MatchName: , MatchPattern: *                                                                                                                                        default/tofqdn-dns-visibility   1       16777231\n                                                                                                                                                                                                                16777232\n                                                                                                                                                                                                                16777233\n                                                                                                                                                                                                                16860295\n                                                                                                                                                                                                                16860322\n                                                                                                                                                                                                                16860323\n                                                                                                                                                                                                                16860324\n                                                                                                                                                                                                                16860325\n                                                                                                                                                                                                                16860326\n                                                                                                                                                                                                                16860327\n                                                                                                                                                                                                                16860328\n    &LabelSelector{MatchLabels:map[string]string{any.name: netperf,k8s.io.kubernetes.pod.namespace: default,},MatchExpressions:[]LabelSelectorRequirement{},}           default/tofqdn-dns-visibility   1\n    &LabelSelector{MatchLabels:map[string]string{cidr.1.1.1.1/32: ,},MatchExpressions:[]LabelSelectorRequirement{},}                                                    default/tofqdn-dns-visibility   1       16860329\n    &LabelSelector{MatchLabels:map[string]string{cidr.1.1.1.2/32: ,},MatchExpressions:[]LabelSelectorRequirement{},}                                                    default/tofqdn-dns-visibility   1       16860330\n    &LabelSelector{MatchLabels:map[string]string{cidr.1.1.1.3/32: ,},MatchExpressions:[]LabelSelectorRequirement{},}                                                    default/tofqdn-dns-visibility   1       16860331\n\nFrom the output above, we see that all three selectors are in use. The\nsignificant action here is to determine which selector is selecting the most\nidentities, because the policy containing that selector is the likely cause for\nthe policymap pressure.\n\nLabel\n~~~~~\n\nSee section on :ref:`identity-relevant labels <identity-relevant-labels>`.\n\nAnother aspect to consider is the permissiveness of the policies and whether it\ncould be reduced.\n\nCIDR\n~~~~\n\nOne way to reduce the number of identities selected by a CIDR selector is to\nbroaden the range of the CIDR, if possible. For example, in the above example\noutput, the policy contains a ``/32`` rule for each CIDR, rather than using a\nwider range like ``/30`` instead. Updating the policy with this rule creates an\nidentity that represents all IPs within the ``/30`` and therefore, only\nrequires the selector to select 1 identity.\n\nFQDN\n~~~~\n\nSee section on :ref:`isolating the source of toFQDNs issues regarding\nidentities and policy <isolating-source-toFQDNs-issues-identities-policy>`.\n\netcd (kvstore)\n==============\n\nIntroduction\n------------\n\nCilium can be operated in CRD-mode and kvstore/etcd mode. When cilium is\nrunning in kvstore/etcd mode, the kvstore becomes a vital component of the\noverall cluster health as it is required to be available for several\noperations.\n\nOperations for which the kvstore is strictly required when running in etcd\nmode:\n\nScheduling of new workloads:\n  As part of scheduling workloads/endpoints, agents will perform security\n  identity allocation which requires interaction with the kvstore. If a\n  workload can be scheduled due to re-using a known security identity, then\n  state propagation of the endpoint details to other nodes will still depend on\n  the kvstore and thus packets drops due to policy enforcement may be observed\n  as other nodes in the cluster will not be aware of the new workload.\n\nMulti cluster:\n  All state propagation between clusters depends on the kvstore.\n\nNode discovery:\n  New nodes require to register themselves in the kvstore.\n\nAgent bootstrap:\n  The Cilium agent will eventually fail if it can't connect to the kvstore at\n  bootstrap time, however, the agent will still perform all possible operations\n  while waiting for the kvstore to appear.\n\nOperations which *do not* require kvstore availability:\n\nAll datapath operations:\n  All datapath forwarding, policy enforcement and visibility functions for\n  existing workloads/endpoints do not depend on the kvstore. Packets will\n  continue to be forwarded and network policy rules will continue to be\n  enforced.\n\n  However, if the agent requires to restart as part of the\n  :ref:`etcd_recovery_behavior`, there can be delays in:\n\n  * processing of flow events and metrics\n  * short unavailability of layer 7 proxies\n\nNetworkPolicy updates:\n  Network policy updates will continue to be processed and applied.\n\nServices updates:\n  All updates to services will be processed and applied.\n\nUnderstanding etcd status\n-------------------------\n\nThe etcd status is reported when running ``cilium-dbg status``. The following line\nrepresents the status of etcd::\n\n   KVStore:  Ok  etcd: 1/1 connected, lease-ID=29c6732d5d580cb5, lock lease-ID=29c6732d5d580cb7, has-quorum=true: https://192.168.60.11:2379 - 3.4.9 (Leader)\n\nOK:\n  The overall status. Either ``OK`` or ``Failure``.\n\n1/1 connected:\n  Number of total etcd endpoints and how many of them are reachable.\n\nlease-ID:\n  UUID of the lease used for all keys owned by this agent.\n\nlock lease-ID:\n  UUID of the lease used for locks acquired by this agent.\n\nhas-quorum:\n  Status of etcd quorum. Either ``true`` or set to an error.\n\nconsecutive-errors:\n  Number of consecutive quorum errors. Only printed if errors are present.\n\nhttps://192.168.60.11:2379 - 3.4.9 (Leader):\n  List of all etcd endpoints stating the etcd version and whether the\n  particular endpoint is currently the elected leader. If an etcd endpoint\n  cannot be reached, the error is shown.\n\n.. _etcd_recovery_behavior:\n\nRecovery behavior\n-----------------\n\nIn the event of an etcd endpoint becoming unhealthy, etcd should automatically\nresolve this by electing a new leader and by failing over to a healthy etcd\nendpoint. As long as quorum is preserved, the etcd cluster will remain\nfunctional.\n\nIn addition, Cilium performs a background check in an interval to determine\netcd health and potentially take action. The interval depends on the overall\ncluster size. The larger the cluster, the longer the `interval\n<https://pkg.go.dev/github.com/cilium/cilium/pkg/kvstore?tab=doc#ExtraOptions.StatusCheckInterval>`_:\n\n * If no etcd endpoints can be reached, Cilium will report failure in ``cilium-dbg\n   status``. This will cause the liveness and readiness probe of Kubernetes to\n   fail and Cilium will be restarted.\n\n * A lock is acquired and released to test a write operation which requires\n   quorum. If this operation fails, loss of quorum is reported. If quorum fails\n   for three or more intervals in a row, Cilium is declared unhealthy.\n\n * The Cilium operator will constantly write to a heartbeat key\n   (``cilium/.heartbeat``). All Cilium agents will watch for updates to this\n   heartbeat key. This validates the ability for an agent to receive key\n   updates from etcd. If the heartbeat key is not updated in time, the quorum\n   check is declared to have failed and Cilium is declared unhealthy after 3 or\n   more consecutive failures.\n\nExample of a status with a quorum failure which has not yet reached the\nthreshold::\n\n    KVStore: Ok   etcd: 1/1 connected, lease-ID=29c6732d5d580cb5, lock lease-ID=29c6732d5d580cb7, has-quorum=2m2.778966915s since last heartbeat update has been received, consecutive-errors=1: https://192.168.60.11:2379 - 3.4.9 (Leader)\n\nExample of a status with the number of quorum failures exceeding the threshold::\n\n    KVStore: Failure   Err: quorum check failed 8 times in a row: 4m28.446600949s since last heartbeat update has been received\n\n.. _troubleshooting_clustermesh:\n\n.. include:: ./troubleshooting_clustermesh.rst\n\n.. _troubleshooting_servicemesh:\n\n.. include:: troubleshooting_servicemesh.rst\n\nSymptom Library\n===============\n\nNode to node traffic is being dropped\n-------------------------------------\n\nSymptom\n~~~~~~~\n\nEndpoint to endpoint communication on a single node succeeds but communication\nfails between endpoints across multiple nodes.\n\nTroubleshooting steps:\n~~~~~~~~~~~~~~~~~~~~~~\n\n#. Run ``cilium-health status --verbose`` on the node of the source and\n   destination endpoint. It should describe the connectivity from that node to\n   other nodes in the cluster, and to a simulated endpoint on each other node.\n   Identify points in the cluster that cannot talk to each other. If the\n   command does not describe the status of the other node, there may be an\n   issue with the KV-Store.\n\n#. Run ``cilium-dbg monitor`` on the node of the source and destination endpoint.\n   Look for packet drops.\n\n   When running in :ref:`arch_overlay` mode:\n\n#. Run ``cilium-dbg bpf tunnel list`` and verify that each Cilium node is aware of\n   the other nodes in the cluster.  If not, check the logfile for errors.\n\n#. If nodes are being populated correctly, run ``tcpdump -n -i cilium_vxlan`` on\n   each node to verify whether cross node traffic is being forwarded correctly\n   between nodes.\n\n   If packets are being dropped,\n\n   * verify that the node IP listed in ``cilium-dbg bpf tunnel list`` can reach each\n     other.\n   * verify that the firewall on each node allows UDP port 8472.\n\n   When running in :ref:`arch_direct_routing` mode:\n\n#. Run ``ip route`` or check your cloud provider router and verify that you have\n   routes installed to route the endpoint prefix between all nodes.\n\n#. Verify that the firewall on each node permits to route the endpoint IPs.\n\n\nUseful Scripts\n==============\n\n.. _retrieve_cilium_pod:\n\nRetrieve Cilium pod managing a particular pod\n---------------------------------------------\n\nIdentifies the Cilium pod that is managing a particular pod in a namespace:\n\n.. code-block:: shell-session\n\n    k8s-get-cilium-pod.sh <pod> <namespace>\n\n**Example:**\n\n.. code-block:: shell-session\n\n    $ curl -sLO https://raw.githubusercontent.com/cilium/cilium/main/contrib/k8s/k8s-get-cilium-pod.sh\n    $ chmod +x k8s-get-cilium-pod.sh\n    $ ./k8s-get-cilium-pod.sh luke-pod default\n    cilium-zmjj9\n    cilium-node-init-v7r9p\n    cilium-operator-f576f7977-s5gpq\n\nExecute a command in all Kubernetes Cilium pods\n-----------------------------------------------\n\nRun a command within all Cilium pods of a cluster\n\n.. code-block:: shell-session\n\n    k8s-cilium-exec.sh <command>\n\n**Example:**\n\n.. code-block:: shell-session\n\n    $ curl -sLO https://raw.githubusercontent.com/cilium/cilium/main/contrib/k8s/k8s-cilium-exec.sh\n    $ chmod +x k8s-cilium-exec.sh\n    $ ./k8s-cilium-exec.sh uptime\n     10:15:16 up 6 days,  7:37,  0 users,  load average: 0.00, 0.02, 0.00\n     10:15:16 up 6 days,  7:32,  0 users,  load average: 0.00, 0.03, 0.04\n     10:15:16 up 6 days,  7:30,  0 users,  load average: 0.75, 0.27, 0.15\n     10:15:16 up 6 days,  7:28,  0 users,  load average: 0.14, 0.04, 0.01\n\nList unmanaged Kubernetes pods\n------------------------------\n\nLists all Kubernetes pods in the cluster for which Cilium does *not* provide\nnetworking. This includes pods running in host-networking mode and pods that\nwere started before Cilium was deployed.\n\n.. code-block:: shell-session\n\n   k8s-unmanaged.sh\n\n**Example:**\n\n.. code-block:: shell-session\n\n   $ curl -sLO https://raw.githubusercontent.com/cilium/cilium/main/contrib/k8s/k8s-unmanaged.sh\n   $ chmod +x k8s-unmanaged.sh\n   $ ./k8s-unmanaged.sh\n   kube-system/cilium-hqpk7\n   kube-system/kube-addon-manager-minikube\n   kube-system/kube-dns-54cccfbdf8-zmv2c\n   kube-system/kubernetes-dashboard-77d8b98585-g52k5\n   kube-system/storage-provisioner\n\nReporting a problem\n===================\n\nBefore you report a problem, make sure to retrieve the necessary information\nfrom your cluster before the failure state is lost.\n\n.. _sysdump:\n\nAutomatic log & state collection\n--------------------------------\n\n.. include:: ../installation/cli-download.rst\n\nThen, execute ``cilium sysdump`` command to collect troubleshooting information\nfrom your Kubernetes cluster:\n\n.. code-block:: shell-session\n\n   cilium sysdump\n\nNote that by default ``cilium sysdump`` will attempt to collect as much logs as\npossible and for all the nodes in the cluster. If your cluster size is above 20\nnodes, consider setting the following options to limit the size of the sysdump.\nThis is not required, but useful for those who have a constraint on bandwidth or\nupload size.\n\n* set the ``--node-list`` option to pick only a few nodes in case the cluster has\n  many of them.\n* set the ``--logs-since-time`` option to go back in time to when the issues started.\n* set the ``--logs-limit-bytes`` option to limit the size of the log files (note:\n  passed onto ``kubectl logs``; does not apply to entire collection archive).\n\nIdeally, a sysdump that has a full history of select nodes, rather than a brief\nhistory of all the nodes, would be preferred (by using ``--node-list``). The second\nrecommended way would be to use ``--logs-since-time`` if you are able to narrow down\nwhen the issues started. Lastly, if the Cilium agent and Operator logs are too\nlarge, consider ``--logs-limit-bytes``.\n\nUse ``--help`` to see more options:\n\n.. code-block:: shell-session\n\n   cilium sysdump --help\n\nSingle Node Bugtool\n~~~~~~~~~~~~~~~~~~~\n\nIf you are not running Kubernetes, it is also possible to run the bug\ncollection tool manually with the scope of a single node:\n\nThe ``cilium-bugtool`` captures potentially useful information about your\nenvironment for debugging. The tool is meant to be used for debugging a single\nCilium agent node. In the Kubernetes case, if you have multiple Cilium pods,\nthe tool can retrieve debugging information from all of them. The tool works by\narchiving a collection of command output and files from several places. By\ndefault, it writes to the ``tmp`` directory.\n\nNote that the command needs to be run from inside the Cilium pod/container.\n\n.. code-block:: shell-session\n\n   cilium-bugtool\n\nWhen running it with no option as shown above, it will try to copy various\nfiles and execute some commands. If ``kubectl`` is detected, it will search for\nCilium pods. The default label being ``k8s-app=cilium``, but this and the\nnamespace can be changed via ``k8s-namespace`` and ``k8s-label`` respectively.\n\nIf you want to capture the archive from a Kubernetes pod, then the process is a\nbit different\n\n.. code-block:: shell-session\n\n   $ # First we need to get the Cilium pod\n   $ kubectl get pods --namespace kube-system\n   NAME                          READY     STATUS    RESTARTS   AGE\n   cilium-kg8lv                  1/1       Running   0          13m\n   kube-addon-manager-minikube   1/1       Running   0          1h\n   kube-dns-6fc954457d-sf2nk     3/3       Running   0          1h\n   kubernetes-dashboard-6xvc7    1/1       Running   0          1h\n\n   $ # Run the bugtool from this pod\n   $ kubectl -n kube-system exec cilium-kg8lv -- cilium-bugtool\n   [...]\n\n   $ # Copy the archive from the pod\n   $ kubectl cp kube-system/cilium-kg8lv:/tmp/cilium-bugtool-20180411-155146.166+0000-UTC-266836983.tar /tmp/cilium-bugtool-20180411-155146.166+0000-UTC-266836983.tar\n   [...]\n\n.. note::\n\n   Please check the archive for sensitive information and strip it\n   away before sharing it with us.\n\nBelow is an approximate list of the kind of information in the archive.\n\n* Cilium status\n* Cilium version\n* Kernel configuration\n* Resolve configuration\n* Cilium endpoint state\n* Cilium logs\n* Docker logs\n* ``dmesg``\n* ``ethtool``\n* ``ip a``\n* ``ip link``\n* ``ip r``\n* ``iptables-save``\n* ``kubectl -n kube-system get pods``\n* ``kubectl get pods,svc for all namespaces``\n* ``uname``\n* ``uptime``\n* ``cilium-dbg bpf * list``\n* ``cilium-dbg endpoint get for each endpoint``\n* ``cilium-dbg endpoint list``\n* ``hostname``\n* ``cilium-dbg policy get``\n* ``cilium-dbg service list``\n\n\nDebugging information\n~~~~~~~~~~~~~~~~~~~~~\n\nIf you are not running Kubernetes, you can use the ``cilium-dbg debuginfo`` command\nto retrieve useful debugging information. If you are running Kubernetes, this\ncommand is automatically run as part of the system dump.\n\n``cilium-dbg debuginfo`` can print useful output from the Cilium API. The output\nformat is in Markdown format so this can be used when reporting a bug on the\n`issue tracker`_.  Running without arguments will print to standard output, but\nyou can also redirect to a file like\n\n.. code-block:: shell-session\n\n   cilium-dbg debuginfo -f debuginfo.md\n\n.. note::\n\n   Please check the debuginfo file for sensitive information and strip it\n   away before sharing it with us.\n\n\nSlack assistance\n----------------\n\nThe `Cilium Slack`_ community is a helpful first point of assistance to get\nhelp troubleshooting a problem or to discuss options on how to address a\nproblem. The community is open to anyone.\n\nReport an issue via GitHub\n--------------------------\n\nIf you believe to have found an issue in Cilium, please report a\n`GitHub issue`_ and make sure to attach a system dump as described above to\nensure that developers have the best chance to reproduce the issue.\n\n.. _NodeSelector: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector\n.. _RBAC: https://kubernetes.io/docs/reference/access-authn-authz/rbac/\n.. _CNI: https://github.com/containernetworking/cni\n.. _Volumes: https://kubernetes.io/docs/tasks/configure-pod-container/configure-volume-storage/\n\n.. _Cilium Frequently Asked Questions (FAQ): https://github.com/cilium/cilium/issues?utf8=%E2%9C%93&q=label%3Akind%2Fquestion%20\n\n.. _issue tracker: https://github.com/cilium/cilium/issues\n.. _GitHub issue: `issue tracker`_",
  "item_type": "unknown",
  "module_path": "/tmp/cilium-repo/Documentation/operations/troubleshooting.rst",
  "extracted_at": "2025-09-03T01:13:29.347850Z"
}