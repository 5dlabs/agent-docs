{
  "url": "file:///tmp/cilium-repo/Documentation/operations/troubleshooting_clustermesh.rst",
  "content": "Cluster Mesh Troubleshooting \n Install the Cilium CLI \n .. include:: /installation/cli-download.rst \n Automatic Verification \n #. Validate that Cilium pods are healthy and ready: \n .. code-block:: shell-session\n\n   cilium status\n \n #. Validate that Cluster Mesh is enabled and operational: \n .. code-block:: shell-session\n\n   cilium clustermesh status\n \n #. In case of errors, run the troubleshoot command to automatically investigate\nCilium agents connectivity issues towards the ClusterMesh control plane in\nremote clusters: \n .. code-block:: shell-session\n\n   kubectl exec -it -n kube-system ds/cilium -c cilium-agent -- cilium-dbg troubleshoot clustermesh\n\nThe troubleshoot command performs a set of automatic checks to validate\nDNS resolution, network connectivity, TLS authentication, etcd authorization\nand more, and reports the output in a user friendly format.\n\nWhen KVStoreMesh is enabled, the output of the troubleshoot command refers\nto the connections from the agents to the local cache, and it is expected to\nbe the same for all the clusters they are connected to. Run the troubleshoot\ncommand inside the clustermesh-apiserver to investigate KVStoreMesh connectivity\nissues towards the ClusterMesh control plane in remote clusters:\n\n.. code-block:: shell-session\n\n  kubectl exec -it -n kube-system deploy/clustermesh-apiserver -c kvstoremesh -- \\\n    clustermesh-apiserver kvstoremesh-dbg troubleshoot\n\n.. tip::\n\n  You can specify one or more cluster names as parameters of the troubleshoot\n  command to run the checks only towards a subset of remote clusters.\n \n Manual Verification \n As an alternative to leveraging the tools presented in the previous section,\nyou may perform the following steps to troubleshoot ClusterMesh issues. \n #. Validate that each cluster is assigned a  unique  human-readable name as well\nas a numeric cluster ID (1-255). \n #. Validate that the clustermesh-apiserver is initialized correctly for each cluster: \n .. code-block:: shell-session\n\n    $ kubectl logs -n kube-system deployment/clustermesh-apiserver -c apiserver\n    ...\n    level=info msg=\"Connecting to etcd server...\" config=/var/lib/cilium/etcd-config.yaml endpoints=\"[https://127.0.0.1:2379]\" subsys=kvstore\n    level=info msg=\"Got lock lease ID 7c0281854b945c07\" subsys=kvstore\n    level=info msg=\"Initial etcd session established\" config=/var/lib/cilium/etcd-config.yaml endpoints=\"[https://127.0.0.1:2379]\" subsys=kvstore\n    level=info msg=\"Successfully verified version of etcd endpoint\" config=/var/lib/cilium/etcd-config.yaml endpoints=\"[https://127.0.0.1:2379]\" etcdEndpoint=\"https://127.0.0.1:2379\" subsys=kvstore version=3.4.13\n \n #. Validate that ClusterMesh is healthy running  cilium-dbg status --all-clusters  inside each Cilium agent:: \n     ClusterMesh:   1/1 remote clusters ready, 10 global-services\n       k8s-c2: ready, 3 nodes, 25 endpoints, 8 identities, 10 services, 0 MCS-API service exports, 0 reconnections (last: never)\n       └  etcd: 1/1 connected, leases=0, lock lease-ID=7c028201b53de662, has-quorum=true: https://k8s-c2.mesh.cilium.io:2379 - 3.5.4 (Leader)\n       └  remote configuration: expected=true, retrieved=true, cluster-id=3, kvstoremesh=false, sync-canaries=true, service-exports=disabled\n       └  synchronization status: nodes=true, endpoints=true, identities=true, services=true\n\nWhen KVStoreMesh is enabled, additionally check its status and validate that\nit is correctly connected to all remote clusters:\n\n.. code-block:: shell-session\n\n  $ kubectl --context $CLUSTER1 exec -it -n kube-system deploy/clustermesh-apiserver \\\n      -c kvstoremesh -- clustermesh-apiserver kvstoremesh-dbg status --verbose\n \n #. Validate that the required TLS secrets are set up properly. By default, the\nfollowing TLS secrets must be available in the namespace in which Cilium is\ninstalled: \n * ``clustermesh-apiserver-server-cert``, which is used by the etcd container\n  in the clustermesh-apiserver deployment. Not applicable if an external etcd\n  cluster is used.\n\n* ``clustermesh-apiserver-admin-cert``, which is used by the apiserver/kvstoremesh\n  containers in the clustermesh-apiserver deployment, to authenticate against the\n  sidecar etcd instance. Not applicable if an external etcd cluster is used.\n\n* ``clustermesh-apiserver-remote-cert``, which is used by Cilium agents, or\n  the kvstoremesh container in the clustermesh-apiserver deployment when\n  KVStoreMesh is enabled, to authenticate against remote etcd instances.\n\n* ``clustermesh-apiserver-local-cert``, which is used by Cilium agents to\n  authenticate against the local etcd instance. Only applicable if KVStoreMesh\n  is enabled.\n \n #. Validate that the configuration for remote clusters is picked up correctly.\nFor each remote cluster, an info log message  New remote cluster     configuration  along with the remote cluster name must be logged in the\n cilium-agent  logs. \n If the configuration is not found, check the following:\n\n* The ``cilium-clustermesh`` Kubernetes secret is present and correctly\n  mounted by the Cilium agent pods.\n\n* The secret contains a file for each remote cluster with the filename matching\n  the name of the remote cluster as provided by the ``--cluster-name`` argument\n  or the ``cluster-name`` ConfigMap option.\n\n* Each file named after a remote cluster contains a valid etcd configuration\n  consisting of the endpoints to reach the remote etcd cluster, and the path\n  of the certificate and private key to authenticate against that etcd cluster.\n  Additional files may be included in the secret to provide the certificate\n  and private key themselves.\n\n* The ``/var/lib/cilium/clustermesh`` directory inside any of the Cilium agent\n  pods contains the files mounted from the ``cilium-clustermesh`` secret.\n  You can use\n  ``kubectl exec -ti -n kube-system ds/cilium -c cilium-agent -- ls /var/lib/cilium/clustermesh``\n  to list the files present.\n \n #. Validate that the connection to the remote cluster could be established.\nYou will see a log message like this in the  cilium-agent  logs for each\nremote cluster:: \n    level=info msg=\"Connection to remote cluster established\"\n\nIf the connection failed, you will see a warning like this::\n\n   level=warning msg=\"Unable to establish etcd connection to remote cluster\"\n\nIf the connection fails, check the following:\n\n* When KVStoreMesh is disabled, validate that the ``hostAliases`` section in the Cilium DaemonSet maps\n  each remote cluster to the IP of the LoadBalancer that makes the remote\n  control plane available; When KVStoreMesh is enabled,\n  validate the ``hostAliases`` section in the clustermesh-apiserver Deployment.\n\n* Validate that a local node in the source cluster can reach the IP\n  specified in the ``hostAliases`` section. When KVStoreMesh is disabled, the ``cilium-clustermesh``\n  secret contains a configuration file for each remote cluster, it will\n  point to a logical name representing the remote cluster;\n  When KVStoreMesh is enabled, it exists in the ``cilium-kvstoremesh`` secret.\n\n  .. code-block:: yaml\n\n     endpoints:\n     - https://cluster1.mesh.cilium.io:2379\n\n  The name will *NOT* be resolvable via DNS outside the Cilium agent pods.\n  The name is mapped to an IP using ``hostAliases``. Run ``kubectl -n\n  kube-system get daemonset cilium -o yaml`` when KVStoreMesh is disabled,\n  or run ``kubectl -n kube-system get deployment clustermesh-apiserver -o yaml`` when KVStoreMesh is enabled,\n  grep for the FQDN to retrieve the IP that is configured. Then use ``curl`` to validate that the port is\n  reachable.\n\n* A firewall between the local cluster and the remote cluster may drop the\n  control plane connection. Ensure that port 2379/TCP is allowed.\n \n State Propagation \n #. Run  cilium-dbg node list  in one of the Cilium pods and validate that it\nlists both local nodes and nodes from remote clusters. If remote nodes are\nnot present, validate that Cilium agents (or KVStoreMesh, if enabled)\nare correctly connected to the given remote cluster. Additionally, verify\nthat the initial nodes synchronization from all clusters has completed. \n #. Validate the connectivity health matrix across clusters by running\n cilium-health status  inside any Cilium pod. It will list the status of\nthe connectivity health check to each remote node. If this fails, make sure\nthat the network allows the health checking traffic as specified in the\n:ref: firewall_requirements  section. \n #. Validate that identities are synchronized correctly by running  cilium-dbg     identity list  in one of the Cilium pods. It must list identities from all\nclusters. You can determine what cluster an identity belongs to by looking\nat the label  io.cilium.k8s.policy.cluster . If remote identities are\nnot present, validate that Cilium agents (or KVStoreMesh, if enabled)\nare correctly connected to the given remote cluster. Additionally, verify\nthat the initial identities synchronization from all clusters has completed. \n #. Validate that the IP cache is synchronized correctly by running  cilium-dbg     bpf ipcache list  or  cilium-dbg map get cilium_ipcache . The output must\ncontain pod IPs from local and remote clusters. If remote IP addresses are\nnot present, validate that Cilium agents (or KVStoreMesh, if enabled)\nare correctly connected to the given remote cluster. Additionally, verify\nthat the initial IPs synchronization from all clusters has completed. \n #. When using global services, ensure that global services are configured with\nendpoints from all clusters. Run  cilium-dbg service list  in any Cilium pod\nand validate that the backend IPs consist of pod IPs from all clusters\nrunning relevant backends. You can further validate the correct datapath\nplumbing by running  cilium-dbg bpf lb list  to inspect the state of the eBPF\nmaps. \n If this fails:\n\n* Run ``cilium-dbg debuginfo`` and look for the section ``k8s-service-cache``. In\n  that section, you will find the contents of the service correlation\n  cache. It will list the Kubernetes services and endpoints of the local\n  cluster.  It will also have a section ``externalEndpoints`` which must\n  list all endpoints of remote clusters.\n\n  ::\n\n      #### k8s-service-cache\n\n      (*k8s.ServiceCache)(0xc00000c500)({\n      [...]\n       services: (map[k8s.ServiceID]*k8s.Service) (len=2) {\n         (k8s.ServiceID) default/kubernetes: (*k8s.Service)(0xc000cd11d0)(frontend:172.20.0.1/ports=[https]/selector=map[]),\n         (k8s.ServiceID) kube-system/kube-dns: (*k8s.Service)(0xc000cd1220)(frontend:172.20.0.10/ports=[metrics dns dns-tcp]/selector=map[k8s-app:kube-dns])\n       },\n       endpoints: (map[k8s.ServiceID]*k8s.Endpoints) (len=2) {\n         (k8s.ServiceID) kube-system/kube-dns: (*k8s.Endpoints)(0xc0000103c0)(10.16.127.105:53/TCP,10.16.127.105:53/UDP,10.16.127.105:9153/TCP),\n         (k8s.ServiceID) default/kubernetes: (*k8s.Endpoints)(0xc0000103f8)(192.168.60.11:6443/TCP)\n       },\n       externalEndpoints: (map[k8s.ServiceID]k8s.externalEndpoints) {\n       }\n      })\n\n  The sections ``services`` and ``endpoints`` represent the services of the\n  local cluster, the section ``externalEndpoints`` lists all remote\n  services and will be correlated with services matching the same\n  ``ServiceID``.",
  "item_type": "unknown",
  "module_path": "/tmp/cilium-repo/Documentation/operations/troubleshooting_clustermesh.rst",
  "extracted_at": "2025-09-03T01:13:29.333897Z"
}