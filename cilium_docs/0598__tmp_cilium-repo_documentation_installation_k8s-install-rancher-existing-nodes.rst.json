{
  "url": "file:///tmp/cilium-repo/Documentation/installation/k8s-install-rancher-existing-nodes.rst",
  "content": ".. only:: not (epub or latex or html)\n\n    WARNING: You are looking at unreleased Cilium documentation.\n    Please use the official rendered version released here:\n    https://docs.cilium.io\n\n.. _rancher_managed_rke_clusters:\n\n**************************\nInstallation using Rancher\n**************************\n\nIntroduction\n============\n\nIf you're not using the Rancher Management Console/UI to install your clusters, head\nover to the :ref:`installation guides for standalone RKE clusters <rke_install>`.\n\nRancher comes with `official support for Cilium <https://ranchermanager.docs.rancher.com/faq/container-network-interface-providers>`__.\nFor most Rancher users, that's the recommended way to use Cilium on Rancher-managed\nclusters.\n\nHowever, as Rancher is using a custom\n``rke2-cilium`` `Helm chart <https://github.com/rancher/rke2-charts/tree/main-source/packages/rke2-cilium>`__\nwith independent release cycles, Cilium power-users might want to use an\nout-of-band Cilium installation instead, based on the official\n`Cilium Helm chart <https://github.com/cilium/charts>`__,\non top of their Rancher-managed RKE1/RKE2 downstream clusters.\nThis guide explains how to achieve this.\n\n.. note::\n\n    This guide only shows a step-by-step guide for Rancher-managed (**non-standalone**)\n    **RKE2** clusters.\n\n    However, for a legacy RKE1 cluster, it's even easier. You also need to edit\n    the cluster YAML and change ``network.cni`` to ``none`` as described in the\n    :ref:`RKE 1 standalone guide<rke1_cni_none>`, but there's no need to copy over\n    a Control Plane node local KubeConfig manually. Luckily, Rancher allows access\n    to RKE1 clusters in ``Updating`` state, which are not ready yet. Hence, there's\n    no chicken-egg issue to resolve.\n\nPrerequisites\n=============\n\n* Fully functioning `Rancher Version 2.x <https://ranchermanager.docs.rancher.com/>`__ instance\n* At least one empty Linux VM, to be used as initial downstream \"Custom Cluster\" (Control Plane) node\n* DNS record pointing to the Kubernetes API of the downstream \"Custom Cluster\" Control Plane node(s) or L4 load-balancer\n\nCreate a New Cluster\n====================\n\nIn Rancher UI, navigate to the Cluster Management page. In the top right, click on the\n``Create`` button to create a new cluster.\n\n.. image:: images/rancher_add_cluster.png\n\nOn the Cluster creation page select to create a new ``Custom`` cluster:\n\n.. image:: images/rancher_existing_nodes.png\n\nWhen the ``Create Custom`` page opens, provide at least a name for the cluster.\nGo through the other configuration options and configure the ones that are\nrelevant for your setup.\n\nNext to the ``Cluster Options`` section click the box to ``Edit as YAML``.\nThe configuration for the cluster will open up in an editor in the window.\n\n.. image:: images/rancher_edit_as_yaml.png\n\nWithin the ``Cluster`` CustomResource (``provisioning.cattle.io/v1``), the relevant\nparts to change are ``spec.rkeConfig.machineGlobalConfig.cni``,\n``spec.rkeConfig.machineGlobalConfig.tls-san``, and optionally\n``spec.rkeConfig.chartValues.rke2-calico`` and\n``spec.rkeConfig.machineGlobalConfig.disable-kube-proxy``:\n\n.. image:: images/rancher_delete_network_plugin.png\n\nIt's required to add a DNS record, pointing to the Control Plane node IP(s)\nor an L4 load-balancer in front of them, under\n``spec.rkeConfig.machineGlobalConfig.tls-san``, as that's required to resolve\na chicken-egg issue further down the line.\n\nEnsure that ``spec.rkeConfig.machineGlobalConfig.cni`` is set to ``none`` and\n``spec.rkeConfig.machineGlobalConfig.tls-san`` lists the mentioned DNS record:\n\n.. image:: images/rancher_network_plugin_none.png\n\nOptionally, if ``spec.rkeConfig.chartValues.rke2-calico`` is not empty, remove the\nfull object as you won't deploy Rancher's default CNI. At the same time, change\n``spec.rkeConfig.machineGlobalConfig.disable-kube-proxy`` to ``true`` in case you\nwant to run :ref:`Cilium without Kube-Proxy<kubeproxy-free>`.\n\nMake any additional changes to the configuration that are appropriate for your\nenvironment. When you are ready, click ``Create`` and Rancher will create the\ncluster.\n\n.. image:: images/rancher_cluster_state_provisioning.png\n\nThe cluster will stay in ``Updating`` state until you add nodes. Click on the cluster.\nIn the ``Registration`` tab you should see the generated ``Registation command`` you\nneed to run on the downstream cluster nodes.\n\nDo not forget to select the correct node roles. Rancher comes with the default to\ndeploy all three roles (``etcd``, ``Control Plane``, and ``Worker``), which is often\nnot what you want for multi-node clusters.\n\n.. image:: images/rancher_add_nodes.png\n\nA few seconds after you added at least a single node, you should see the new node(s)\nin the ``Machines`` tab. The machine will be stuck in ``Reconciling`` state and\nwon't become ``Active``:\n\n.. image:: images/rancher_node_not_ready.png\n\nThat's expected as there's no CNI running on this cluster yet. Unfortunately, this also\nmeans critical pods like ``rke2-coredns-rke2-coredns-*`` and ``cattle-cluster-agent-*`` \nare stuck in ``PENDING`` state. Hence, the downstream cluster is not yet able\nto register itself on Rancher.\n\nAs a next step, you need to resolve this chicken-egg issue by directly accessing\nthe downstream cluster's Kubernetes API, without going via Rancher. Rancher will not allow\naccess to this downstream cluster, as it's still in ``Updating`` state. That's why you\ncan't use the downstream cluster's KubeConfig provided by the Rancher management console/UI.\n\nCopy ``/etc/rancher/rke2/rke2.yaml`` from the first downstream cluster Control Plane\nnode to your jump/bastion host where you have ``helm`` installed and can access the\nCilium Helm charts.\n\n.. code-block:: shell-session\n\n    scp root@<cp-node-1-ip>:/etc/rancher/rke2/rke2.yaml .\n\nSearch and replace ``127.0.0.1`` (``clusters[0].cluster.server``) with the\nalready mentioned DNS record pointing to the Control Plane / L4 load-balancer IP(s).\n\n.. code-block:: yaml\n\n    apiVersion: v1\n    clusters:\n    - cluster:\n        certificate-authority-data: LS0...S0K\n        server: https://127.0.0.1:6443\n    name: default\n    contexts: {}\n\nCheck if you can access the Kubernetes API:\n\n.. code-block:: shell-session\n\n    export KUBECONFIG=$(pwd)/my-cluster-kubeconfig.yaml\n    kubectl get nodes\n    NAME                    STATUS     ROLES                       AGE   VERSION\n    rancher-demo-node       NotReady   control-plane,etcd,master   44m   v1.27.8+rke2r1\n\nIf successful, you can now install Cilium via Helm CLI:\n\n.. parsed-literal::\n\n    helm install cilium |CHART_RELEASE| \\\\\n      --namespace kube-system \\\\\n      -f my-cluster-cilium-values.yaml\n\nAfter a few minutes, you should see that the node changed to the ``Ready`` status:\n\n.. code-block:: shell-session\n\n    kubectl get nodes\n    NAME                    STATUS   ROLES                       AGE   VERSION\n    rancher-demo-node       Ready    control-plane,etcd,master   48m   v1.27.8+rke2r1\n\nBack in the Rancher UI, you should see that the cluster changed to the healthy\n``Active`` status:\n\n.. image:: images/rancher_my_cluster_active.png\n\nThat's it. You can now normally work with this cluster as if you\ninstalled the CNI the default Rancher way. Additional nodes can now be added\nstraightaway and the \"local Control Plane RKE2 KubeConfig\" workaround\nis not required anymore.\n\nOptional: Add Cilium to Rancher Registries\n==========================================\n\nOne small, optional convenience item would be to add the Cilium Helm repository\nto Rancher so that, in the future, Cilium can easily be upgraded via Rancher UI.\n\nYou have two options available:\n\n**Option 1**: Navigate to ``Cluster Management`` -> ``Advanced`` -> ``Repositories`` and\nclick the ``Create`` button:\n\n.. image:: images/rancher_add_repository.png\n\n**Option 2**: Alternatively, you can also just add the Cilium Helm repository\non a single cluster by navigating to ``<your-cluster>`` -> ``Apps`` -> ``Repositories``:\n\n.. image:: images/rancher_add_repository_cluster.png\n\nFor either option, in the window that opens, add the official Cilium Helm chart\nrepository (``https://helm.cilium.io``) to the Rancher repository list:\n\n.. image:: images/rancher_add_cilium_repository.png\n\nOnce added, you should see the Cilium repository in the repositories list:\n\n.. image:: images/rancher_repositories_list_success.png\n\nIf you now head to ``<your-cluster>`` -> ``Apps`` -> ``Installed Apps``, you\nshould see the ``cilium`` app. Ensure ``All Namespaces`` or\n``Project: System -> kube-system`` is selected at the top of the page.\n\n.. image:: images/rancher_cluster_cilium_app.png\n\nSince you added the Cilium repository, you will now see a small hint on this app entry\nwhen there's a new Cilium version released. You can then upgrade directly via Rancher UI.\n\n.. image:: images/rancher_cluster_cilium_app_upgrade.png\n\n.. image:: images/rancher_cluster_cilium_app_upgrade_version.png",
  "item_type": "unknown",
  "module_path": "/tmp/cilium-repo/Documentation/installation/k8s-install-rancher-existing-nodes.rst",
  "extracted_at": "2025-09-03T01:13:29.320941Z"
}