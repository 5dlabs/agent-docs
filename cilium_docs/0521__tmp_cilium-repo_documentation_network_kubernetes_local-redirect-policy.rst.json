{
  "url": "file:///tmp/cilium-repo/Documentation/network/kubernetes/local-redirect-policy.rst",
  "content": ".. only:: not (epub or latex or html)\n\n    WARNING: You are looking at unreleased Cilium documentation.\n    Please use the official rendered version released here:\n    https://docs.cilium.io\n\n.. _local-redirect-policy:\n\n*********************\nLocal Redirect Policy\n*********************\n\nThis document explains how to configure Cilium's Local Redirect Policy, that\nenables pod traffic destined to an IP address and port/protocol tuple\nor Kubernetes service to be redirected locally to backend pod(s) within a node,\nusing eBPF. The namespace of backend pod(s) need to match with that of the policy.\nThe CiliumLocalRedirectPolicy is configured as a ``CustomResourceDefinition``.\n\n.. admonition:: Video\n  :class: attention\n\n  Aside from this document, you can watch a video explanation of Cilium's Local Redirect Policy on `eCHO episode 39: Local Redirect Policy <https://www.youtube.com/watch?v=BT_gdlhjiQc&t=176s>`__.\n\nThere are two types of Local Redirect Policies supported. When traffic for a\nKubernetes service needs to be redirected, use the `ServiceMatcher` type. The\nservice needs to be of type ``clusterIP``.\nWhen traffic matching IP address and port/protocol, that doesn't belong to\nany Kubernetes service, needs to be redirected, use the `AddressMatcher` type.\n\nThe policies can be gated by Kubernetes Role-based access control (RBAC)\nframework. See the official `RBAC documentation\n<https://kubernetes.io/docs/reference/access-authn-authz/rbac/>`_.\n\nWhen policies are applied, matched pod traffic is redirected. If desired, RBAC\nconfigurations can be used such that application developers can not escape\nthe redirection.\n\nPrerequisites\n=============\n\n.. include:: ../../installation/k8s-install-download-release.rst\n\nEnable the feature by setting the ``localRedirectPolicies.enabled`` value to ``true``.\n\n.. parsed-literal::\n\n   helm upgrade cilium |CHART_RELEASE| \\\\\n     --namespace kube-system \\\\\n     --reuse-values \\\\\n     --set localRedirectPolicies.enabled=true\n\n\nRollout the operator and agent pods to make the changes effective:\n\n.. code-block:: shell-session\n\n    $ kubectl rollout restart deploy cilium-operator -n kube-system\n    $ kubectl rollout restart ds cilium -n kube-system\n\n\nVerify that Cilium agent and operator pods are running.\n\n.. code-block:: shell-session\n\n    $ kubectl -n kube-system get pods -l k8s-app=cilium\n    NAME           READY   STATUS    RESTARTS   AGE\n    cilium-5ngzd   1/1     Running   0          3m19s\n\n    $ kubectl -n kube-system get pods -l name=cilium-operator\n    NAME                               READY   STATUS    RESTARTS   AGE\n    cilium-operator-544b4d5cdd-qxvpv   1/1     Running   0          3m19s\n\nValidate that the Cilium Local Redirect Policy CRD has been registered.\n\n.. code-block:: shell-session\n\n\t   $ kubectl get crds\n\t   NAME                              CREATED AT\n\t   [...]\n\t   ciliumlocalredirectpolicies.cilium.io              2020-08-24T05:31:47Z\n\n.. note::\n\n    Local Redirect Policy supports either the socket-level loadbalancer or the tc loadbalancer.\n    The configuration depends on your specific use case and the type of service handling required.\n    Below are the Helm setups to work with ``localRedirectPolicies.enabled=true``:\n\n    1. Enable full kube-proxy replacement:\n\n      This setup is for users who want to replace kube-proxy with Cilium's eBPF implementation\n      and leverage Local Redirect Policy.\n\n      .. code-block:: yaml\n\n        kubeProxyReplacement: true\n        localRedirectPolicies:\n          enabled: true\n\n    2. Bypass the socket-level loadbalancer in pod namespaces:\n\n      This setup is for users who want to disable the socket-level loadbalancer in pod namespaces.\n      For example, this might be needed if there are custom redirection rules in the pod namespace\n      that would conflict with the socket-level load balancer.\n\n      .. code-block:: yaml\n\n        kubeProxyReplacement: true\n        socketLB:\n          hostNamespaceOnly: true\n        localRedirectPolicies:\n          enabled: true\n\n    3. Enable the socket-level loadbalancer only:\n\n      This setup is for users who prefer to retain kube-proxy for overall service handling\n      but still want to leverage Cilium's Local Redirect Policy.\n\n      .. code-block:: yaml\n\n        kubeProxyReplacement: false\n        socketLB:\n          enabled: true\n        localRedirectPolicies:\n          enabled: true\n\n    4. Disable any service handling except for ClusterIP services accessed from pods:\n\n      If you want to fully rely on kube-proxy for the service handling, you can disable all\n      kube-proxy replacement functionality expect ClusterIP services accessed from pod namespace.\n      Note that the pod traffic from host namespace isn't handled by Local Redirect Policy\n      with this setup.\n\n      .. code-block:: yaml\n\n        kubeProxyReplacement: false\n        localRedirectPolicies:\n          enabled: true\n\nCreate backend and client pods\n==============================\n\nDeploy a backend pod where traffic needs to be redirected to based on the\nconfigurations specified in a CiliumLocalRedirectPolicy. The metadata\nlabels and container port and protocol respectively match with the labels,\nport and protocol fields specified in the CiliumLocalRedirectPolicy custom\nresources that will be created in the next step.\n\n.. literalinclude:: ../../../examples/kubernetes-local-redirect/backend-pod.yaml\n   :language: yaml\n\n.. parsed-literal::\n\n    $ kubectl apply -f \\ |SCM_WEB|\\/examples/kubernetes-local-redirect/backend-pod.yaml\n\nVerify that the pod is running.\n\n.. code-block:: shell-session\n\n    $ kubectl get pods | grep lrp-pod\n    lrp-pod                      1/1     Running   0          46s\n\nDeploy a client pod that will generate traffic which will be redirected based on\nthe configurations specified in the CiliumLocalRedirectPolicy.\n\n.. parsed-literal::\n\n   $ kubectl create -f \\ |SCM_WEB|\\/examples/kubernetes-dns/dns-sw-app.yaml\n   $ kubectl wait pod/mediabot --for=condition=Ready\n   $ kubectl get pods\n   NAME                             READY   STATUS    RESTARTS   AGE\n   pod/mediabot                     1/1     Running   0          14s\n\nCreate Cilium Local Redirect Policy Custom Resources\n=====================================================\nThere are two types of configurations supported in the CiliumLocalRedirectPolicy\nin order to match the traffic that needs to be redirected.\n\n.. _AddressMatcher:\n\nAddressMatcher\n---------------\n\nThis type of configuration is specified using an IP address and a Layer 4 port/protocol.\nWhen multiple ports are specified for frontend in ``toPorts``, the ports need\nto be named. The port names will be used to map frontend ports with backend ports.\n\nVerify that the ports specified in ``toPorts`` under ``redirectBackend``\nexist in the backend pod spec.\n\nThe example shows how to redirect from traffic matching, IP address ``169.254.169.254``\nand Layer 4 port ``8080`` with protocol ``TCP``, to a backend pod deployed with\nlabels ``app=proxy`` and Layer 4 port ``80`` with protocol ``TCP``. The\n``localEndpointSelector`` set to ``app=proxy`` in the policy is used to select\nthe backend pods where traffic is redirected to.\n\nCreate a custom resource of type CiliumLocalRedirectPolicy with ``addressMatcher``\nconfiguration.\n\n.. literalinclude:: ../../../examples/kubernetes-local-redirect/lrp-addrmatcher.yaml\n   :language: yaml\n\n.. parsed-literal::\n\n    $ kubectl apply -f \\ |SCM_WEB|\\/examples/kubernetes-local-redirect/lrp-addrmatcher.yaml\n\nVerify that the custom resource is created.\n\n.. code-block:: shell-session\n\n    $ kubectl get ciliumlocalredirectpolicies | grep lrp-addr\n    NAME           AGE\n    lrp-addr       20h\n\nVerify that Cilium's eBPF kube-proxy replacement created a ``LocalRedirect``\nservice entry with the backend IP address of that of the ``lrp-pod`` that was\nselected by the policy. Make sure that ``cilium-dbg service list`` is run\nin Cilium pod running on the same node as ``lrp-pod``.\n\n.. code-block:: shell-session\n\n    $ kubectl describe pod lrp-pod  | grep 'IP:'\n    IP:           10.16.70.187\n\n.. code-block:: shell-session\n\n    $ kubectl exec -it -n kube-system cilium-5ngzd -- cilium-dbg service list\n    ID   Frontend               Service Type       Backend\n    [...]\n    4    172.20.0.51:80         LocalRedirect      1 => 10.16.70.187:80\n\nInvoke a curl command from the client pod to the IP address and port\nconfiguration specified in the ``lrp-addr`` custom resource above.\n\n.. code-block:: shell-session\n\n    $ kubectl exec mediabot -- curl -I -s http://169.254.169.254:8080/index.html\n    HTTP/1.1 200 OK\n    Server: nginx/1.19.2\n    Date: Fri, 28 Aug 2020 01:33:34 GMT\n    Content-Type: text/html\n    Content-Length: 612\n    Last-Modified: Tue, 11 Aug 2020 14:50:35 GMT\n    Connection: keep-alive\n    ETag: \"5f32b03b-264\"\n    Accept-Ranges: bytes\n\nVerify that the traffic was redirected to the ``lrp-pod`` that was deployed.\n``tcpdump`` should be run on the same node that ``lrp-pod`` is running on.\n\n.. code-block:: shell-session\n\n    $ sudo tcpdump -i any -n port 80\n    tcpdump: verbose output suppressed, use -v or -vv for full protocol decode\n    listening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes\n    01:36:24.608566 IP 10.16.215.55.60876 > 10.16.70.187.80: Flags [S], seq 2119454273, win 28200, options [mss 1410,sackOK,TS val 2541637677 ecr 0,nop,wscale 7], length 0\n    01:36:24.608600 IP 10.16.70.187.80 > 10.16.215.55.60876: Flags [S.], seq 1315636594, ack 2119454274, win 27960, options [mss 1410,sackOK,TS val 2962246962 ecr 2541637677,nop,wscale 7], length 0\n    01:36:24.608638 IP 10.16.215.55.60876 > 10.16.70.187.80: Flags [.], ack 1, win 221, options [nop,nop,TS val 2541637677 ecr 2962246962], length 0\n    01:36:24.608867 IP 10.16.215.55.60876 > 10.16.70.187.80: Flags [P.], seq 1:96, ack 1, win 221, options [nop,nop,TS val 2541637677 ecr 2962246962], length 95: HTTP: HEAD /index.html HTTP/1.1\n    01:36:24.608876 IP 10.16.70.187.80 > 10.16.215.55.60876: Flags [.], ack 96, win 219, options [nop,nop,TS val 2962246962 ecr 2541637677], length 0\n    01:36:24.609007 IP 10.16.70.187.80 > 10.16.215.55.60876: Flags [P.], seq 1:239, ack 96, win 219, options [nop,nop,TS val 2962246962 ecr 2541637677], length 238: HTTP: HTTP/1.1 200 OK\n    01:36:24.609052 IP 10.16.215.55.60876 > 10.16.70.187.80: Flags [.], ack 239, win 229, options [nop,nop,TS val 2541637677 ecr 2962246962], length 0\n\nThe allowed addresses can be constrained clusterwide using the\n``localRedirectPolicies.addressMatcherCIDRs`` helm option:\n\n.. code-block:: yaml\n\n  localRedirectPolicies:\n    enabled: true\n    addressMatchCIDRs:\n\t- 169.254.169.254/32\n\nThe above would only allow traffic going to ``169.254.169.254`` to be redirected\nwith an AddressMatcher rule. A policy with a disallowed address will be rejected\nand a warning log message is emitted by cilium-agent.\n\n.. _ServiceMatcher:\n\nServiceMatcher\n---------------\n\nThis type of configuration is specified using Kubernetes service name and namespace\nfor which traffic needs to be redirected. The service must be of type ``clusterIP``.\nWhen ``toPorts`` under ``redirectFrontend`` are not specified, traffic for\nall the service ports will be redirected. However, if traffic destined to only\na subset of ports needs to be redirected, these ports need to be specified in the spec.\nAdditionally, when multiple service ports are specified in the spec, they must be\nnamed. The port names will be used to map frontend ports with backend ports.\nVerify that the ports specified in ``toPorts`` under ``redirectBackend``\nexist in the backend pod spec. The ``localEndpointSelector`` set to ``app=proxy``\nin the policy is used to select the backend pods where traffic is redirected to.\n\nWhen a policy of this type is applied, the existing service entry\ncreated by Cilium's eBPF kube-proxy replacement will be replaced with a new\nservice entry of type ``LocalRedirect``. This entry may only have node-local backend pods.\n\nThe example shows how to redirect from traffic matching ``my-service``, to a\nbackend pod deployed with labels ``app=proxy`` and Layer 4 port ``80``\nwith protocol ``TCP``. The ``localEndpointSelector`` set to ``app=proxy`` in the\npolicy is used to select the backend pods where traffic is redirected to.\n\nDeploy the Kubernetes service for which traffic needs to be redirected.\n\n.. literalinclude:: ../../../examples/kubernetes-local-redirect/k8s-svc.yaml\n   :language: yaml\n\n.. parsed-literal::\n\n    $ kubectl apply -f \\ |SCM_WEB|\\/examples/kubernetes-local-redirect/k8s-svc.yaml\n\nVerify that the service is created.\n\n.. code-block:: shell-session\n\n    $ kubectl get service | grep 'my-service'\n    my-service   ClusterIP   172.20.0.51   <none>        80/TCP     2d7h\n\nVerify that Cilium's eBPF kube-proxy replacement created a ``ClusterIP``\nservice entry.\n\n.. code-block:: shell-session\n\n    $ kubectl exec -it -n kube-system ds/cilium -- cilium-dbg service list\n    ID   Frontend               Service Type   Backend\n    [...]\n    4    172.20.0.51:80         ClusterIP\n\nCreate a custom resource of type CiliumLocalRedirectPolicy with ``serviceMatcher``\nconfiguration.\n\n.. literalinclude:: ../../../examples/kubernetes-local-redirect/lrp-svcmatcher.yaml\n   :language: yaml\n\n.. parsed-literal::\n\n    $ kubectl apply -f \\ |SCM_WEB|\\/examples/kubernetes-local-redirect/lrp-svcmatcher.yaml\n\nVerify that the custom resource is created.\n\n.. code-block:: shell-session\n\n    $ kubectl get ciliumlocalredirectpolicies | grep svc\n    NAME               AGE\n    lrp-svc   20h\n\nVerify that entry Cilium's eBPF kube-proxy replacement updated the\nservice entry with type ``LocalRedirect`` and the node-local backend\nselected by the policy. Make sure to run ``cilium-dbg service list`` in Cilium pod\nrunning on the same node as ``lrp-pod``.\n\n.. code-block:: shell-session\n\n    $ kubectl exec -it -n kube-system cilium-5ngzd -- cilium-dbg service list\n    ID   Frontend               Service Type       Backend\n    [...]\n    4    172.20.0.51:80         LocalRedirect      1 => 10.16.70.187:80\n\nInvoke a curl command from the client pod to the Cluster IP address and port of\n``my-service`` specified in the ``lrp-svc`` custom resource above.\n\n.. code-block:: shell-session\n\n    $ kubectl exec mediabot -- curl -I -s http://172.20.0.51/index.html\n    HTTP/1.1 200 OK\n    Server: nginx/1.19.2\n    Date: Fri, 28 Aug 2020 01:50:50 GMT\n    Content-Type: text/html\n    Content-Length: 612\n    Last-Modified: Tue, 11 Aug 2020 14:50:35 GMT\n    Connection: keep-alive\n    ETag: \"5f32b03b-264\"\n    Accept-Ranges: bytes\n\nVerify that the traffic was redirected to the ``lrp-pod`` that was deployed.\n``tcpdump`` should be run on the same node that ``lrp-pod`` is running on.\n\n.. code-block:: shell-session\n\n    $ kubectl describe pod lrp-pod  | grep 'IP:'\n    IP:           10.16.70.187\n\n.. code-block:: shell-session\n\n    $ sudo tcpdump -i any -n port 80\n    tcpdump: verbose output suppressed, use -v or -vv for full protocol decode\n    listening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes\n    01:36:24.608566 IP 10.16.215.55.60186 > 10.16.70.187.80: Flags [S], seq 2119454273, win 28200, options [mss 1410,sackOK,TS val 2541637677 ecr 0,nop,wscale 7], length 0\n    01:36:24.608600 IP 10.16.70.187.80 > 10.16.215.55.60876: Flags [S.], seq 1315636594, ack 2119454274, win 27960, options [mss 1410,sackOK,TS val 2962246962 ecr 2541637677,nop,wscale 7], length 0\n    01:36:24.608638 IP 10.16.215.55.60876 > 10.16.70.187.80: Flags [.], ack 1, win 221, options [nop,nop,TS val 2541637677 ecr 2962246962], length 0\n    01:36:24.608867 IP 10.16.215.55.60876 > 10.16.70.187.80: Flags [P.], seq 1:96, ack 1, win 221, options [nop,nop,TS val 2541637677 ecr 2962246962], length 95: HTTP: HEAD /index.html HTTP/1.1\n    01:36:24.608876 IP 10.16.70.187.80 > 10.16.215.55.60876: Flags [.], ack 96, win 219, options [nop,nop,TS val 2962246962 ecr 2541637677], length 0\n    01:36:24.609007 IP 10.16.70.187.80 > 10.16.215.55.60876: Flags [P.], seq 1:239, ack 96, win 219, options [nop,nop,TS val 2962246962 ecr 2541637677], length 238: HTTP: HTTP/1.1 200 OK\n    01:36:24.609052 IP 10.16.215.55.60876 > 10.16.70.187.80: Flags [.], ack 239, win 229, options [nop,nop,TS val 2541637677 ecr 2962246962], length 0\n\nLimitations\n===========\nWhen you create a Local Redirect Policy, traffic for all the new connections\nthat get established after the policy is enforced will be redirected. But if\nyou have existing active connections to remote pods that match the configurations\nspecified in the policy, then these might not get redirected. To ensure all\nsuch connections are redirected locally, restart the client pods after\nconfiguring the CiliumLocalRedirectPolicy.\n\nLocal Redirect Policy updates are currently not supported. If there are any\nchanges to be made, delete the existing policy, and re-create a new one.\n\nUse Cases\n=========\nLocal Redirect Policy allows Cilium to support the following use cases:\n\nNode-local DNS cache\n--------------------\n`DNS node-cache <https://github.com/kubernetes/dns>`_ listens on a static IP to intercept\ntraffic from application pods to the cluster's DNS service VIP by default, which will be\nbypassed when Cilium is handling service resolution at or before the veth interface of the\napplication pod. To enable the DNS node-cache in a Cilium cluster, the following example\nsteers traffic to a local DNS node-cache which runs as a normal pod.\n\n* Deploy DNS node-cache in pod namespace.\n\n  .. tabs::\n\n    .. group-tab:: Quick Deployment\n\n        Deploy DNS node-cache.\n\n        .. note::\n\n           * The example yaml is populated with default values for ``__PILLAR_LOCAL_DNS__`` and\n             ``__PILLAR_DNS_DOMAIN__``.\n           * If you have a different deployment, please follow the official `NodeLocal DNSCache Configuration\n             <https://kubernetes.io/docs/tasks/administer-cluster/nodelocaldns/#configuration>`_\n             to fill in the required template variables ``__PILLAR__LOCAL__DNS__``, ``__PILLAR__DNS__DOMAIN__``,\n             and ``__PILLAR__DNS__SERVER__`` before applying the yaml.\n\n        .. parsed-literal::\n\n            $ wget \\ |SCM_WEB|\\/examples/kubernetes-local-redirect/node-local-dns.yaml\n\n            $ kubedns=$(kubectl get svc kube-dns -n kube-system -o jsonpath={.spec.clusterIP}) && sed -i \"s/__PILLAR__DNS__SERVER__/$kubedns/g;\" node-local-dns.yaml\n\n            $ kubectl apply -f node-local-dns.yaml\n\n    .. group-tab:: Manual Configuration\n\n         * Follow the official `NodeLocal DNSCache Configuration\n           <https://kubernetes.io/docs/tasks/administer-cluster/nodelocaldns/#configuration>`_\n           to fill in the required template variables ``__PILLAR__LOCAL__DNS__``, ``__PILLAR__DNS__DOMAIN__``,\n           and ``__PILLAR__DNS__SERVER__`` before applying the yaml.\n\n         * Make sure to use a Node-local DNS image with a release version >= 1.15.16.\n           This is to ensure that we have a knob to disable dummy network interface creation/deletion in\n           Node-local DNS when we deploy it in non-host namespace.\n\n         * Modify Node-local DNS cache's deployment yaml to pass these additional arguments to node-cache:\n           ``-skipteardown=true``, ``-setupinterface=false``, and ``-setupiptables=false``.\n\n         * Modify Node-local DNS cache's deployment yaml to put it in non-host namespace by setting\n           ``hostNetwork: false`` for the daemonset.\n\n         * In the Corefile, bind to ``0.0.0.0`` instead of the static IP.\n\n         * In the Corefile, let CoreDNS serve health-check on its own IP instead of the static IP by\n           removing the host IP string after health plugin.\n\n         * Modify Node-local DNS cache's deployment yaml to point readiness probe to its own IP by\n           removing the ``host`` field under ``readinessProbe``.\n\n* Deploy Local Redirect Policy (LRP) to steer DNS traffic to the node local dns cache.\n\n  .. parsed-literal::\n\n      $ kubectl apply -f \\ |SCM_WEB|\\/examples/kubernetes-local-redirect/node-local-dns-lrp.yaml\n\n  .. note::\n\n      * The LRP above uses ``kube-dns`` for the cluster DNS service, however if your cluster DNS service is different,\n        you will need to modify this example LRP to specify it.\n      * The namespace specified in the LRP above is set to the same namespace as the cluster's dns service.\n      * The LRP above uses the same port names ``dns`` and ``dns-tcp`` as the example quick deployment yaml, you will\n        need to modify those to match your deployment if they are different.\n\nAfter all ``node-local-dns`` pods are in ready status, DNS traffic will now go to the local node-cache first.\nYou can verify by checking the DNS cache's metrics ``coredns_dns_request_count_total`` via curling\n``<node-local-dns pod IP>:9253/metrics``, the metric should increment as new DNS requests being issued from\napplication pods are now redirected to the ``node-local-dns`` pod.\n\nIn the absence of a node-local DNS cache, DNS queries from application pods\nwill get directed to cluster DNS pods backed by the ``kube-dns`` service.\n\n* Troubleshooting\n\n    If DNS requests are failing to resolve, check the following:\n\n        - Ensure that the node-local DNS cache pods are running and ready.\n\n         .. code-block:: shell-session\n\n            $ kubectl --namespace kube-system get pods --selector=k8s-app=node-local-dns\n            NAME                   READY   STATUS    RESTARTS   AGE\n            node-local-dns-72r7m   1/1     Running   0          2d2h\n            node-local-dns-gc5bx   1/1     Running   0          2d2h\n\n        - Check if the local redirect policy has been applied correctly on all the cilium agent pods.\n\n         .. code-block:: shell-session\n\n            $ kubectl exec -it cilium-mhnhz -n kube-system -- cilium-dbg lrp list\n            LRP namespace   LRP name       FrontendType                Matching Service\n            kube-system     nodelocaldns   clusterIP + all svc ports   kube-system/kube-dns\n                            |              10.96.0.10:53/UDP -> 10.244.1.49:53(kube-system/node-local-dns-72r7m),\n                            |              10.96.0.10:53/TCP -> 10.244.1.49:53(kube-system/node-local-dns-72r7m),\n\n        - Check if the corresponding local redirect service entry has been created. If the service entry is missing,\n          there might have been a race condition in applying the policy and the node-local DNS DaemonSet pod resources.\n          As a workaround, you can restart the node-local DNS DaemonSet pods. File a `GitHub issue <https://github.com/cilium/cilium/issues/new/choose>`_\n          with a :ref:`sysdump <sysdump>` if the issue persists.\n\n         .. code-block:: shell-session\n\n            $ kubectl exec -it cilium-mhnhz -n kube-system -- cilium-dbg service list | grep LocalRedirect\n            11   10.96.0.10:53      LocalRedirect   1 => 10.244.1.49:53 (active)\n\nkiam redirect on EKS\n--------------------\n`kiam <https://github.com/uswitch/kiam>`_ agent runs on each node in an EKS\ncluster, and intercepts requests going to the AWS metadata server to fetch\nsecurity credentials for pods.\n\n- In order to only redirect traffic from pods to the kiam agent, and pass\n  traffic from the kiam agent to the AWS metadata server without any redirection,\n  we need the socket lookup functionality in the datapath. This functionality\n  requires v5.1.16, v5.2.0 or more recent Linux kernel. Make sure the kernel\n  version installed on EKS cluster nodes satisfies these requirements.\n\n- Deploy `kiam <https://github.com/uswitch/kiam>`_ using helm charts.\n\n  .. code-block:: shell-session\n\n      $ helm repo add uswitch https://uswitch.github.io/kiam-helm-charts/charts/\n      $ helm repo update\n      $ helm install --set agent.host.iptables=false --set agent.whitelist-route-regexp=meta-data kiam uswitch/kiam\n\n  - The above command may provide instructions to prepare kiam in the cluster.\n    Follow the instructions before continuing.\n\n  - kiam must run in the ``hostNetwork`` mode and without the \"--iptables\" argument.\n    The install instructions above ensure this by default.\n\n- Deploy the Local Redirect Policy to redirect pod traffic to the deployed kiam agent.\n\n  .. parsed-literal::\n\n      $ kubectl apply -f \\ |SCM_WEB|\\/examples/kubernetes-local-redirect/kiam-lrp.yaml\n\n.. note::\n\n    - The ``addressMatcher`` ip address in the Local Redirect Policy is set to\n      the ip address of the AWS metadata server and the ``toPorts`` port\n      to the default HTTP server port. The ``toPorts`` field under\n      ``redirectBackend`` configuration in the policy is set to the port that\n      the kiam agent listens on. The port is passed as \"--port\" argument in\n      the ``kiam-agent DaemonSet``.\n    - The Local Redirect Policy namespace is set to the namespace\n      in which kiam-agent DaemonSet is deployed.\n\n- Once all the kiam agent pods are in ``Running`` state, the metadata requests\n  from application pods will get redirected to the node-local kiam agent pods.\n  You can verify this by running a curl command to the AWS metadata server from\n  one of the application pods, and tcpdump command on the same EKS cluster node as the\n  pod. Following is an example output, where ``192.169.98.118`` is the ip\n  address of an application pod, and ``192.168.60.99`` is the ip address of the\n  kiam agent running on the same node as the application pod.\n\n  .. code-block:: shell-session\n\n      $ kubectl exec app-pod -- curl -s -w \"\\n\" -X GET http://169.254.169.254/latest/meta-data/\n      ami-id\n      ami-launch-index\n      ami-manifest-path\n      block-device-mapping/\n      events/\n      hostname\n      iam/\n      identity-credentials/\n      (...)\n\n  .. code-block:: shell-session\n\n      $ sudo tcpdump -i any -enn \"(port 8181) and (host 192.168.60.99 and 192.168.98.118)\"\n      tcpdump: verbose output suppressed, use -v or -vv for full protocol decode\n      listening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes\n      05:16:05.229597  In de:e4:e9:94:b5:9f ethertype IPv4 (0x0800), length 76: 192.168.98.118.47934 > 192.168.60.99.8181: Flags [S], seq 669026791, win 62727, options [mss 8961,sackOK,TS val 2539579886 ecr 0,nop,wscale 7], length 0\n      05:16:05.229657 Out 56:8f:62:18:6f:85 ethertype IPv4 (0x0800), length 76: 192.168.60.99.8181 > 192.168.98.118.47934: Flags [S.], seq 2355192249, ack 669026792, win 62643, options [mss 8961,sackOK,TS val 4263010641 ecr 2539579886,nop,wscale 7], length 0\n\nAdvanced configurations\n=======================\nWhen a local redirect policy is applied, cilium BPF datapath redirects traffic going to the policy frontend\n(identified by ip/port/protocol tuple) address to a node-local backend pod selected by the policy.\nHowever, for traffic originating from a node-local backend pod destined to the policy frontend, users may want to\nskip redirecting the traffic back to the node-local backend pod, and instead forward the traffic to the original frontend.\nThis behavior can be enabled by setting the ``skipRedirectFromBackend`` flag to ``true`` in the local redirect policy spec.\nThis configuration requires the use of ``getsockopt()`` with the ``SO_NETNS_COOKIE`` option, which is available in\nLinux kernel version >= 5.12. Note that ``SO_NETNS_COOKIE`` was introduced in 5.7 (available to BPF programs),\nand exposed to user space in versions >= 5.12.\n\n.. note::\n\n    In order to enable this configuration starting Cilium version 1.16.0, previously applied local redirect policies\n    and policies selected backend pods need to be deleted, and re-created.\n",
  "item_type": "unknown",
  "module_path": "/tmp/cilium-repo/Documentation/network/kubernetes/local-redirect-policy.rst",
  "extracted_at": "2025-09-03T01:13:29.221907Z"
}