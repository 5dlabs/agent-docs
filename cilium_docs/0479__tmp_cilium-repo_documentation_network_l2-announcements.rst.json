{
  "url": "file:///tmp/cilium-repo/Documentation/network/l2-announcements.rst",
  "content": ".. only:: not (epub or latex or html) \n WARNING: You are looking at unreleased Cilium documentation.\nPlease use the official rendered version released here:\nhttps://docs.cilium.io\n \n .. _l2_announcements: \n \n L2 Announcements / L2 Aware LB (Beta) \n \n .. include:: ../beta.rst \n L2 Announcements is a feature which makes services visible and reachable on\nthe local area network. This feature is primarily intended for on-premises\ndeployments within networks without BGP based routing such as office or\ncampus networks. \n When used, this feature will respond to ARP/NDP queries for ExternalIPs and/or\nLoadBalancer IPs. These IPs are Virtual IPs (not installed on network\ndevices) on multiple nodes, so for each service one node at a time will respond\nto ARP/NDP queries and respond with its MAC address. This node will perform\nload balancing with the service load balancing feature, thus acting as a\nnorth/south load balancer. \n The advantage of this feature over NodePort services is that each service can\nuse a unique IP so multiple services can use the same port numbers. When using\nNodePorts, it is up to the client to decide to which host to send traffic, and if a node\ngoes down, the IP+Port combo becomes unusable. With L2 announcements the service\nVIP simply migrates to another node and will continue to work. \n .. _l2_announcements_settings: \n Configuration\n############# \n The L2 Announcements feature and all the requirements can be enabled as follows: \n .. tabs::\n.. group-tab:: Helm \n     .. parsed-literal::\n\n        $ helm upgrade cilium |CHART_RELEASE| \\\\\n           --namespace kube-system \\\\\n           --reuse-values \\\\\n           --set l2announcements.enabled=true \\\\\n           --set k8sClientRateLimit.qps={QPS} \\\\\n           --set k8sClientRateLimit.burst={BURST} \\\\\n           --set kubeProxyReplacement=true \\\\\n           --set k8sServiceHost=${API_SERVER_IP} \\\\\n           --set k8sServicePort=${API_SERVER_PORT}\n           \n\n.. group-tab:: ConfigMap\n\n    .. code-block:: yaml\n\n        enable-l2-announcements: true\n        kube-proxy-replacement: true\n        k8s-client-qps: {QPS}\n        k8s-client-burst: {BURST}\n \n .. warning::\nSizing the client rate limit ( k8sClientRateLimit.qps  and  k8sClientRateLimit.burst )\nis important when using this feature due to increased API usage. See :ref: sizing_client_rate_limit  for sizing guidelines. \n Prerequisites\n############# \n \n \n Kube Proxy replacement mode must be enabled. For more information, see\n:ref: kubeproxy-free . \n \n \n All devices on which L2 Aware LB will be announced should be enabled and included in the\n --devices  flag or  devices  Helm option if explicitly set, see :ref: NodePort Devices . \n \n \n Limitations\n########### \n \n \n Due to the way L3->L2 translation protocols work, one node receives all\nARP/NDP requests for a specific IP, so no load balancing can happen before traffic hits the cluster. \n \n \n The feature currently has no traffic balancing mechanism so nodes within the\nsame policy might be asymmetrically loaded. For details see :ref: l2_announcements_leader_election . \n \n \n The feature is incompatible with the  externalTrafficPolicy: Local  on services as it may cause\nservice IPs to be announced on nodes without pods causing traffic drops. \n \n \n Policies\n######## \n Policies provide fine-grained control over which services should be announced,\nwhere, and how. This is an example policy using all optional fields: \n .. code-block:: yaml \n apiVersion: \"cilium.io/v2alpha1\"\nkind: CiliumL2AnnouncementPolicy\nmetadata:\n  name: policy1\nspec:\n  serviceSelector:\n    matchLabels:\n      color: blue\n  nodeSelector:\n    matchExpressions:\n      - key: node-role.kubernetes.io/control-plane\n        operator: DoesNotExist\n  interfaces:\n  - ^eth[0-9]+\n  externalIPs: true\n  loadBalancerIPs: true  \n \n Service Selector \n The service selector is a  label selector <https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/> __\nthat determines which services are selected by this policy. If no service\nselector is provided, all services are selected by the policy. A service must have\n loadBalancerClass <https://kubernetes.io/docs/concepts/services-networking/service/#load-balancer-class> __\nunspecified or set to  io.cilium/l2-announcer  to be selected by a policy for announcement. \n There are a few special purpose selector fields which don't match on labels but\ninstead on other metadata like  .meta.name  or  .meta.namespace . \n =============================== ===================\nSelector                        Field \n \n io.kubernetes.service.namespace  .meta.namespace \nio.kubernetes.service.name       .meta.name \n=============================== =================== \n Node Selector \n The node selector field is a  label selector <https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/> __\nwhich determines which nodes are candidates to announce the services from. \n It might be desirable to pick a subset of nodes in you cluster, since the chosen\nnode (see :ref: l2_announcements_leader_election ) will act as the north/south\nload balancer for all of the traffic for a particular service. \n Interfaces \n The interfaces field is a list of regular expressions ( golang syntax <https://pkg.go.dev/regexp/syntax> __)\nthat determine over which network interfaces the selected services will be\nannounced. This field is optional, if not specified all interfaces will be used. \n The expressions are OR-ed together, so any network device matching any of the\nexpressions will be matched. \n L2 announcements only work if the selected devices are also part of the set of\ndevices specified in the  devices  Helm option, see :ref: NodePort Devices . \n .. note::\nThis selector is NOT a security feature, services will still be available\nvia interfaces when not advertised (for example by hard-coding ARP/NDP entries). \n IP Types \n The  externalIPs  and  loadBalancerIPs  fields determine what sort of IPs\nare announced. They are both set to  false  by default, so a functional policy should always\nhave one or both set to  true . \n If  externalIPs  is  true  all IPs in  .spec.externalIPs <https://kubernetes.io/docs/concepts/services-networking/service/#external-ips> __\nfield are announced. These IPs are managed by service authors. \n If  loadBalancerIPs  is  true  all IPs in the service's  .status.loadbalancer.ingress  field\nare announced. These can be assigned by :ref: lb_ipam  which can be configured\nby cluster admins for better control over which IPs can be allocated. \n .. note::\nIf a user intends to use  externalIPs , the  externalIPs.enable=true \nHelm option should be set to enable service load balancing for external IPs. \n Status \n If a policy is invalid for any number of reasons, the status of the policy will reflect that.\nFor example if an invalid match expression is provided: \n .. code-block:: shell-session \n $ kubectl describe l2announcement\nName:         policy1\nNamespace: \nLabels:        \nAnnotations:   \nAPI Version:  cilium.io/v2alpha1\nKind:         CiliumL2AnnouncementPolicy\nMetadata:\n#[...]\nSpec:\n#[...]\nService Selector:\nMatch Expressions:\nKey:       something\nOperator:  NotIn\nValues:\nStatus:\nConditions:\nLast Transition Time:  2023-05-12T15:39:01Z\nMessage:               values: Invalid value: []string(nil): for 'in', 'notin' operators, values set can't be empty\nObserved Generation:   1\nReason:                error\nStatus:                True\nType:                  io.cilium/bad-service-selector \n The status of these error conditions will go to  False  as soon as the user\nupdates the policy to resolve the error. \n .. _l2_announcements_leader_election: \n Leader Election\n############### \n Due to the way ARP/NDP works, hosts only store one MAC address per IP, that being\nthe latest reply they see. This means that only one node in the cluster is allowed\nto reply to requests for a given IP. \n To implement this behavior, every Cilium agent resolves which services are\nselected for its node and will start participating in leader election for every\nservice. We use Kubernetes  lease mechanism <https://kubernetes.io/docs/concepts/architecture/leases/> __\nto achieve this. Each service translates to a lease, the lease holder will start\nreplying to requests on the selected interfaces. \n The lease mechanism is a first come, first serve picking order. So the first\nnode to claim a lease gets it. This might cause asymmetric traffic distribution. \n Leases \n The leases are created in the same namespace where Cilium is deployed,\ntypically  kube-system . You can inspect the leases with the following command: \n .. code-block:: shell-session \n $ kubectl -n kube-system get lease\nNAME                                  HOLDER                                                    AGE\ncilium-l2announce-default-deathstar   worker-node                                               2d20h\ncilium-operator-resource-lock         worker-node2-tPDVulKoRK                                   2d20h\nkube-controller-manager               control-plane-node_9bd97f6c-cd0c-4565-8486-e718deb310e4   2d21h\nkube-scheduler                        control-plane-node_2c490643-dd95-4f73-8862-139afe771ffd   2d21h\n \n The leases starting with  cilium-l2announce-  are leases used by this feature.\nThe last part of the name is the namespace and service name. The holder indicates\nthe name of the node that currently holds the lease and thus announced the IPs\nof that given service. \n To inspect a lease: \n .. code-block:: shell-session \n $ kubectl -n kube-system get lease/cilium-l2announce-default-deathstar -o yaml\napiVersion: coordination.k8s.io/v1\nkind: Lease\nmetadata:\n  creationTimestamp: \"2023-05-09T15:13:32Z\"\n  name: cilium-l2announce-default-deathstar\n  namespace: kube-system\n  resourceVersion: \"449966\"\n  uid: e3c9c020-6e24-4c5c-9df9-d0c50f6c4cec\nspec:\n  acquireTime: \"2023-05-09T15:14:20.108431Z\"\n  holderIdentity: worker-node\n  leaseDurationSeconds: 3\n  leaseTransitions: 1\n  renewTime: \"2023-05-12T12:15:26.773020Z\"\n \n The  acquireTime  is the time at which the current leader acquired the lease.\nThe  holderIdentity  is the name of the current holder/leader node.\nIf the leader does not renew the lease for  leaseDurationSeconds  seconds a\nnew leader is chosen.  leaseTransitions  indicates how often the lease changed\nhands and  renewTime  the last time the leader renewed the lease. \n There are three Helm options that can be tuned with regards to leases: \n \n \n l2announcements.leaseDuration  determines the  leaseDurationSeconds  value\nof created leases and by extent how long a leader must be \"down\" before\nfailover occurs. Its default value is 15s, it must always be greater than 1s\nand be larger than  leaseRenewDeadline . \n \n \n l2announcements.leaseRenewDeadline  is the interval at which the leader\nshould renew the lease. Its default value is 5s, it must be greater than\n leaseRetryPeriod  by at least 20% and is not allowed to be below  1ns . \n \n \n l2announcements.leaseRetryPeriod  if renewing the lease fails, how long\nshould the agent wait before it tries again. Its default value is 2s, it\nmust be smaller than  leaseRenewDeadline  by at least 20% and above  1ns . \n \n \n .. note::\nThe theoretical shortest time between failure and failover is\n leaseDuration - leaseRenewDeadline  and the longest  leaseDuration + leaseRenewDeadline .\nSo with the default values, failover occurs between 10s and 20s.\nFor the example below, these times are between 2s and 4s. \n .. tabs::\n.. group-tab:: Helm \n     .. parsed-literal::\n\n        $ helm upgrade cilium |CHART_RELEASE| \\\\\n           --namespace kube-system \\\\\n           --reuse-values \\\\\n           --set l2announcements.enabled=true \\\\\n           --set kubeProxyReplacement=true \\\\\n           --set k8sServiceHost=${API_SERVER_IP} \\\\\n           --set k8sServicePort=${API_SERVER_PORT} \\\\\n           --set k8sClientRateLimit.qps={QPS} \\\\\n           --set k8sClientRateLimit.burst={BURST} \\\\\n           --set l2announcements.leaseDuration=3s \\\\\n           --set l2announcements.leaseRenewDeadline=1s \\\\\n           --set l2announcements.leaseRetryPeriod=200ms\n\n.. group-tab:: ConfigMap\n\n    .. code-block:: yaml\n\n        enable-l2-announcements: true\n        kube-proxy-replacement: true\n        l2-announcements-lease-duration: 3s\n        l2-announcements-renew-deadline: 1s\n        l2-announcements-retry-period: 200ms\n        k8s-client-qps: {QPS}\n        k8s-client-burst: {BURST}\n \n There is a trade-off between fast failure detection and CPU + network usage.\nEach service incurs a CPU and network overhead, so clusters with smaller amounts\nof services can more easily afford faster failover times. Larger clusters might\nneed to increase parameters if the overhead is too high. \n .. _sizing_client_rate_limit: \n Sizing client rate limit \n The leader election process continually generates API traffic, the exact amount\ndepends on the configured lease duration, configured renew deadline, and amount\nof services using the feature. \n The default client rate limit is 5 QPS with allowed bursts up to 10 QPS. this\ndefault limit is quickly reached when utilizing L2 announcements and thus users\nshould size the client rate limit accordingly. \n In a worst case scenario, services are distributed unevenly, so we will assume\na peak load based on the renew deadline. In complex scenarios with multiple\npolicies over disjointed sets of node, max QPS per node will be lower. \n .. code-block:: text \n QPS = #services * (1 / leaseRenewDeadline) \n // example\n#services = 65\nleaseRenewDeadline = 2s\nQPS = 65 * (1 / 2s) = 32.5 QPS \n Setting the base QPS to around the calculated value should be sufficient, given\nin multi-node scenarios leases are spread around nodes, and non-holders participating\nin the election have a lower QPS. \n The burst QPS should be slightly higher to allow for bursts of traffic caused\nby other features which also use the API server. \n Failover \n When nodes participating in leader election detect that the lease holder did not\nrenew the lease for  leaseDurationSeconds  amount of seconds, they will ask\nthe API server to make them the new holder. The first request to be processed\ngets through and the rest are denied. \n When a node becomes the leader/holder, it will send out a gratuitous ARP reply\nover all of the configured interfaces. Clients who accept these will update\ntheir ARP tables at once causing them to send traffic to the new leader/holder.\nNot all clients accept gratuitous ARP replies since they can be used for ARP spoofing.\nSuch clients might experience longer downtime then configured in the leases\nsince they will only re-query via ARP when TTL in their internal tables\nhas been reached. \n Troubleshooting\n############### \n This section is a step by step guide on how to troubleshoot L2 Announcements,\nhopefully solving your issue or narrowing it down to a specific area. \n The first thing we need to do is to check that the feature is enabled, kube proxy replacement\nis active and optionally that external IPs are enabled. \n .. code-block:: shell-session \n $ kubectl -n kube-system exec ds/cilium -- cilium-dbg config --all | grep EnableL2Announcements\nEnableL2Announcements             : true\n\n$ kubectl -n kube-system exec ds/cilium -- cilium-dbg config --all | grep KubeProxyReplacement\nKubeProxyReplacement              : true\n\n$ kubectl -n kube-system exec ds/cilium -- cilium-dbg config --all | grep EnableExternalIPs\nEnableExternalIPs                 : true\n \n If  EnableL2Announcements  or  KubeProxyReplacement  indicates  false , make sure to enable the\ncorrect settings and deploy the helm chart :ref: l2_announcements_settings .  EnableExternalIPs  should be set to  true  if you intend to use external IPs. \n Next, ensure you have at least one policy configured, L2 announcements will not work without a policy. \n .. code-block:: shell-session \n $ kubectl get CiliumL2AnnouncementPolicy\nNAME      AGE\npolicy1   6m16s\n \n L2 announcements should now create a lease for every service matched by the policy. We can check the leases like so: \n .. code-block:: shell-session \n $ kubectl -n kube-system get lease | grep \"cilium-l2announce\"\ncilium-l2announce-default-service-red   kind-worker                       34s\n \n If the output is empty, then the policy is not correctly configured or the agent is not running correctly.\nCheck the logs of the agent for error messages: \n .. code-block:: shell-session \n $ kubectl -n kube-system logs ds/cilium | grep \"l2\"\n \n A common error is that the agent is not able to create leases. \n .. code-block:: shell-session \n $ kubectl -n kube-system logs ds/cilium | grep \"error\"\ntime=\"2024-06-25T12:01:43Z\" level=error msg=\"error retrieving resource lock kube-system/cilium-l2announce-default-service-red: leases.coordination.k8s.io \\\"cilium-l2announce-default-service-red\\\" is forbidden: User \\\"system:serviceaccount:kube-system:cilium\\\" cannot get resource \\\"leases\\\" in API group \\\"coordination.k8s.io\\\" in the namespace \\\"kube-system\\\"\" subsys=klog\n \n This can happen if the cluster role of the agent is not correct. This tends to happen when L2 announcements is enabled\nwithout using the helm chart. Redeploy the helm chart or manually update the cluster role, by running\n kubectl edit clusterrole cilium  and adding the following block to the rules: \n .. code-block:: yaml \n - apiGroups:\n  - coordination.k8s.io\n  resources:\n  - leases\n  verbs:\n  - create\n  - get\n  - update\n  - list\n  - delete\n \n Another common error is that the configured client rate limit is too low.\nThis can be seen in the logs as well: \n .. code-block:: shell-session \n $ kubectl -n kube-system logs ds/cilium | grep \"l2\"\n2023-07-04T14:59:51.959400310Z level=info msg=\"Waited for 1.395439596s due to client-side throttling, not priority and fairness, request: GET:https://127.0.0.1:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/cilium-l2announce-default-example\" subsys=klog\n2023-07-04T15:00:12.159409007Z level=info msg=\"Waited for 1.398748976s due to client-side throttling, not priority and fairness, request: PUT:https://127.0.0.1:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/cilium-l2announce-default-example\" subsys=klog\n \n These logs are associated with intermittent failures to renew the lease, connection issues and/or frequent leader changes.\nSee :ref: sizing_client_rate_limit  for more information on how to size the client rate limit. \n If you find a different L2 related error, please open a GitHub issue with the error message and the\nsteps you took to get there. \n Assuming the leases are created, the next step is to check the agent internal state. Pick a service which isn't working\nand inspect its lease. Take the holder name and find the cilium agent pod for the holder node.\nFinally, take the name of the cilium agent pod and inspect the l2-announce state: \n .. code-block:: shell-session \n $ kubectl -n kube-system get lease cilium-l2announce-default-service-red\nNAME                                    HOLDER        AGE\ncilium-l2announce-default-service-red   <node-name>   20m\n\n$ kubectl -n kube-system get pod -l 'app.kubernetes.io/name=cilium-agent' -o wide | grep <node-name>\n<agent-pod>   1/1     Running   0          35m   172.19.0.3   kind-worker          <none>           <none>\n\n$ kubectl -n kube-system exec pod/<agent-pod> -- cilium-dbg shell -- db/show l2-announce\n# IP        NetworkInterface\n10.0.10.0   eth0\n \n The l2 announce state should contain the IP of the service and the network interface it is announced on.\nIf the lease is present but its IP is not in the l2-announce state, or you are missing an entry for a given network device.\nDouble check that the device selector in the policy matches the desired network device (values are regular expressions).\nIf the filter seems correct or isn't specified, inspect the known devices: \n .. code-block:: shell-session \n $ kubectl -n kube-system exec ds/cilium -- cilium-dbg shell -- db/show devices\nName              Index   Selected   Type     MTU     HWAddr              Flags                    Addresses\nlxc5d23398605f6   10      false      veth     1500    b6:ed:d8:d2:dd:ec   up|broadcast|multicast   fe80::b4ed:d8ff:fed2:ddec\nlxc3bf03c00d6e3   12      false      veth     1500    8a:d1:0c:91:8a:d3   up|broadcast|multicast   fe80::88d1:cff:fe91:8ad3\neth0              50      true       veth     1500    02:42:ac:13:00:03   up|broadcast|multicast   172.19.0.3, fc00:c111::3, fe80::42:acff:fe13:3\nlo                1       false      device   65536                       up|loopback              127.0.0.1, ::1\ncilium_net        2       false      veth     1500    1a:a9:2f:4d:d3:3d   up|broadcast|multicast   fe80::18a9:2fff:fe4d:d33d\ncilium_vxlan      4       false      vxlan    1500    2a:05:26:8d:79:9c   up|broadcast|multicast   fe80::2805:26ff:fe8d:799c\nlxc611291f1ecbb   8       false      veth     1500    7a:fb:ec:54:e2:5c   up|broadcast|multicast   fe80::78fb:ecff:fe54:e25c\nlxc_health        16      false      veth     1500    0a:94:bf:49:d5:50   up|broadcast|multicast   fe80::894:bfff:fe49:d550\ncilium_host       3       false      veth     1500    22:32:e2:80:21:34   up|broadcast|multicast   10.244.1.239, fd00:10:244:1::f58a\n \n Only devices with  Selected  set to  true  can be used for L2 announcements. Typically all physical devices with IPs\nassigned to them will be considered selected. The  --devices  flag or  devices  Helm option can be used to filter\nout devices. If your desired device is in the list but not selected, check the devices flag/option to see if it filters it out. \n Please open a Github issue if your desired device doesn't appear in the list or it isn't selected while you believe it should be. \n If the L2 state contains the IP and device combination but there are still connection issues, it's time to test ARP\nwithin the cluster. Pick a cilium agent pod other than the lease holder on the same L2 network.\nThen use the following command to send an ARP request to the service IP: \n .. code-block:: shell-session \n $ kubectl -n kube-system exec pod/cilium-z4ef7 -- sh -c 'apt update && apt install -y arping && arping -i <netdev-on-l2> <service-ip>'\n[omitting apt output...]\nARPING 10.0.10.0\n58 bytes from 02:42:ac:13:00:03 (10.0.10.0): index=0 time=11.772 usec\n58 bytes from 02:42:ac:13:00:03 (10.0.10.0): index=1 time=9.234 usec\n58 bytes from 02:42:ac:13:00:03 (10.0.10.0): index=2 time=10.568 usec\n \n If the output is as above yet the service is still unreachable, from clients within the same L2 network,\nthe issue might be client related. If you expect the service to be reachable from outside the L2 network,\nand it is not, check the ARP and routing tables of the gateway device. \n If the ARP request fails (the output shows  Timeout ), check the BPF map of the cilium-agent with the lease: \n .. code-block:: shell-session \n $ kubectl -n kube-system exec pod/cilium-vxz67 -- bpftool map dump pinned /sys/fs/bpf/tc/globals/cilium_l2_responder_v4\n[{\n        \"key\": {\n            \"ip4\": 655370,\n            \"ifindex\": 50\n        },\n        \"value\": {\n            \"responses_sent\": 20\n        }\n    }\n]\n \n The  responses_sent  field is incremented every time the datapath responds to an ARP request. If the field\nis 0, then the ARP request doesn't make it to the node. If the field is greater than 0, the issue is on the\nreturn path. In both cases, inspect the network and the client. \n It is still possible that the service is unreachable even though ARP requests are answered. This can happen\nfor a number of reasons, usually unrelated to L2 announcements, but rather other Cilium features. \n One common issue however is caused by the usage of  .Spec.ExternalTrafficPolicy: Local  on services. This setting\nnormally tells a load balancer to only forward traffic to nodes with at least 1 ready pod to avoid a second hop.\nUnfortunately, L2 announcements isn't currently aware of this setting and will announce the service IP on all nodes\nmatching policies. If a node without a pod receives traffic, it will drop it. To fix this, set the policy to\n .Spec.ExternalTrafficPolicy: Cluster . \n Please open a Github issue if none of the above steps helped you solve your issue. \n .. _l2_pod_announcements: \n L2 Pod Announcements\n#################### \n L2 Pod Announcements announce Pod IP addresses on the L2 network using\nGratuitous ARP replies / Neighbor Discovery Advertisements. When enabled, the\nnode transmits Gratuitous ARP replies / NDP Advertisements for every locally\ncreated pod, on the configured network interface(s). This feature is enabled\nseparately from the above L2 announcements feature. \n To enable L2 Pod Announcements, set the following: \n .. tabs::\n.. group-tab:: Helm \n     .. parsed-literal::\n\n        $ helm upgrade cilium |CHART_RELEASE| \\\\\n           --namespace kube-system \\\\\n           --reuse-values \\\\\n           --set l2podAnnouncements.enabled=true \\\\\n           --set l2podAnnouncements.interface=eth0\n\n\n.. group-tab:: ConfigMap\n\n    .. code-block:: yaml\n\n        enable-l2-pod-announcements: true\n        l2-pod-announcements-interface: eth0\n \n The  l2podAnnouncements.interface / l2-pod-announcements-interface  options allows you to specify\none interface use to send announcements.  If you would like to send announcements on multiple interfaces, you should use the\n l2podAnnouncements.interfacePattern / l2-pod-announcements-interface-pattern  option instead.\nThis option takes a regex, matching on multiple interfaces. \n .. tabs::\n.. group-tab:: Helm \n     .. parsed-literal::\n\n        $ helm upgrade cilium |CHART_RELEASE| \\\\\n           --namespace kube-system \\\\\n           --reuse-values \\\\\n           --set l2podAnnouncements.enabled=true \\\\\n           --set l2podAnnouncements.interfacePattern='^(eth0|ens1)$'\n\n\n.. group-tab:: ConfigMap\n\n    .. code-block:: yaml\n\n        enable-l2-pod-announcements: true\n        l2-pod-announcements-interface-pattern: \"^(eth0|ens1)$\"\n \n .. note::\nSince this feature has no IPv6 support yet, only ARP messages are\nsent, no Unsolicited Neighbor Advertisements are sent.",
  "item_type": "unknown",
  "module_path": "/tmp/cilium-repo/Documentation/network/l2-announcements.rst",
  "extracted_at": "2025-09-03T01:13:29.173084Z"
}