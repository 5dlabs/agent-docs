{
  "url": "file:///tmp/cilium-repo/Documentation/overview/component-overview.rst",
  "content": ".. only:: not (epub or latex or html) \n WARNING: You are looking at unreleased Cilium documentation.\nPlease use the official rendered version released here:\nhttps://docs.cilium.io\n \n .. _component_overview: \n \n Component Overview \n \n .. image:: ../images/cilium-arch.png\n:align: center \n A deployment of Cilium and Hubble consists of the following components running\nin a cluster: \n Cilium \n Agent\nThe Cilium agent ( cilium-agent ) runs on each node in the cluster. At a\nhigh-level, the agent accepts configuration via Kubernetes or APIs that\ndescribes networking, service load-balancing, network policies, and\nvisibility & monitoring requirements. \n The Cilium agent listens for events from orchestration systems such as\nKubernetes to learn when containers or workloads are started and stopped. It\nmanages the eBPF programs which the Linux kernel uses to control all network\naccess in / out of those containers. \n Debug Client (CLI)\nThe Cilium debug CLI client ( cilium-dbg ) is a command-line tool that is\ninstalled along with the Cilium agent. It interacts with the REST API of the\nCilium agent running on the same node. The debug CLI allows inspecting the\nstate and status of the local agent. It also provides tooling to directly\naccess the eBPF maps to validate their state. \n .. note:: \n  The in-agent Cilium debug CLI client described here should not be confused\n with the ```cilium`` command line tool for quick-installing, managing and\n troubleshooting Cilium on Kubernetes clusters\n <https://github.com/cilium/cilium-cli>`_. That tool is typically installed\n remote from the cluster, and uses ``kubeconfig`` information to access\n Cilium running on the cluster via the Kubernetes API.\n \n Operator\nThe Cilium Operator is responsible for managing duties in the cluster which\nshould logically be handled once for the entire cluster, rather than once for\neach node in the cluster. The Cilium operator is not in the critical path for\nany forwarding or network policy decision. A cluster will generally continue\nto function if the operator is temporarily unavailable. However, depending on\nthe configuration, failure in availability of the operator can lead to: \n \n Delays in :ref: address_management  and thus delay in scheduling of new\nworkloads if the operator is required to allocate new IP addresses \n Failure to update the kvstore heartbeat key which will lead agents to\ndeclare kvstore unhealthiness and restart. \n \n CNI Plugin\nThe CNI plugin ( cilium-cni ) is invoked by Kubernetes when a pod is\nscheduled or terminated on a node. It interacts with the Cilium API of the\nnode to trigger the necessary datapath configuration to provide networking,\nload-balancing and network policies for the pod. \n Hubble \n Server\nThe Hubble server runs on each node and retrieves the eBPF-based visibility\nfrom Cilium. It is embedded into the Cilium agent in order to achieve high\nperformance and low-overhead. It offers a gRPC service to retrieve flows and\nPrometheus metrics. \n Relay\nRelay ( hubble-relay ) is a standalone component which is aware of all\nrunning Hubble servers and offers cluster-wide visibility by connecting to\ntheir respective gRPC APIs and providing an API that represents all servers\nin the cluster. \n Client (CLI)\nThe Hubble CLI ( hubble ) is a command-line tool able to connect to either\nthe gRPC API of  hubble-relay  or the local server to retrieve flow events. \n Graphical UI (GUI)\nThe graphical user interface ( hubble-ui ) utilizes relay-based visibility\nto provide a graphical service dependency and connectivity map. \n eBPF \n eBPF is a Linux kernel bytecode interpreter originally introduced to filter\nnetwork packets, e.g. tcpdump and socket filters. It has since been extended\nwith additional data structures such as hashtable and arrays as well as\nadditional actions to support packet mangling, forwarding, encapsulation, etc.\nAn in-kernel verifier ensures that eBPF programs are safe to run and a JIT\ncompiler converts the bytecode to CPU architecture specific instructions for\nnative execution efficiency. eBPF programs can be run at various hooking points\nin the kernel such as for incoming and outgoing packets. \n Cilium is capable of probing the Linux kernel for available features and will\nautomatically make use of more recent features as they are detected. \n For more detail on kernel versions, see: :ref: admin_kernel_version . \n Data Store \n Cilium requires a data store to propagate state between agents. It supports the\nfollowing data stores: \n Kubernetes CRDs (Default)\nThe default choice to store any data and propagate state is to use Kubernetes\ncustom resource definitions (CRDs). CRDs are offered by Kubernetes for\ncluster components to represent configurations and state via Kubernetes\nresources. \n Key-Value Store\nAll requirements for state storage and propagation can be met with Kubernetes\nCRDs as configured in the default configuration of Cilium. A key-value store\ncan optionally be used as an optimization to improve the scalability of a\ncluster as change notifications and storage requirements are more efficient\nwith direct key-value store usage. \n The currently supported key-value stores are: \n * `etcd <https://github.com/etcd-io/etcd>`_\n \n .. note:: \n  It is possible to leverage the etcd cluster of Kubernetes directly or to\n maintain a dedicated etcd cluster.",
  "item_type": "unknown",
  "module_path": "/tmp/cilium-repo/Documentation/overview/component-overview.rst",
  "extracted_at": "2025-09-03T01:13:29.369913Z"
}