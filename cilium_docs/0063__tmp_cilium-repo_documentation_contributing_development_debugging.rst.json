{
  "url": "file:///tmp/cilium-repo/Documentation/contributing/development/debugging.rst",
  "content": ".. only:: not (epub or latex or html) \n WARNING: You are looking at unreleased Cilium documentation.\nPlease use the official rendered version released here:\nhttps://docs.cilium.io\n \n .. _gs_debugging: \n #########\nDebugging\n######### \n Attaching a Debugger \n Cilium comes with a set of Makefile targets for quickly deploying development\nbuilds to a local :ref: Kind <gs_kind>  cluster. One of these targets is\n kind-debug-agent , which generates a container image that wraps the Cilium\nagent with a  Delve (dlv) <https://github.com/go-delve/delve> _ invocation. This\ncauses the agent process to listen for connections from a debugger front-end on\nport 2345. \n To build and push a debug image to your local Kind cluster, run: \n .. code-block:: shell-session \n $ make kind-debug-agent\n \n .. note::\nThe image is automatically pushed to the Kind nodes, but running Cilium\nPods are not restarted. To do so, run: \n   .. code-block:: shell-session\n    \n    $ kubectl delete pods -n kube-system -l app.kubernetes.io/name=cilium-agent\n \n If your Kind cluster was set up using  make kind , it will automatically\nbe configured using with the following port mappings: \n \n 23401 :  kind-control-plane-1 \n 2340* : Subsequent  kind-control-plane-*  nodes, if defined \n 23411 :  kind-worker-1 \n 2341* : Subsequent  kind-worker-*  nodes, if defined \n \n The Delve listener supports multiple debugging protocols, so any IDEs or\ndebugger front-ends that understand either the  Debug Adapter Protocol <https://microsoft.github.io/debug-adapter-protocol> _ or Delve API v2 are\nsupported. \n Visual Studio Code\n \n The Cilium repository contains a VS Code launch configuration\n( .vscode/launch.json ) that includes debug targets for the Kind control\nplane, the first two  kind-worker  nodes and the :ref: Cilium Operator <cilium_operator_internals> . \n .. image:: _static/vscode-run-and-debug.png\n:align: center \n | \n The preceding screenshot is taken from the 'Run And Debug' section in VS Code.\nThe default shortcut to access this section is  Shift+Ctrl+D . Select a target\nto attach to, start the debug session and set a breakpoint to halt the agent or\noperator on a specific code statement. This only works for Go code, BPF C code\ncannot be debugged this way. \n See  the VS Code debugging guide <https://code.visualstudio.com/docs/editor/debugging> _\nfor more details. \n Neovim\n \n The Cilium repository contains a  .nvim directory <https://github.com/cilium/cilium/tree/main/.nvim> _ containing a DAP\nconfiguration as well as a README on how to configure  nvim-dap . \n toFQDNs and DNS Debugging \n The interactions of L3 toFQDNs and L7 DNS rules can be difficult to debug\naround. Unlike many other policy rules, these are resolved at runtime with\nunknown data. Pods may create large numbers of IPs in the cache or the IPs\nreturned may not be compatible with our datapath implementation. Sometimes\nwe also just have bugs. \n Isolating the source of toFQDNs issues\n \n While there is no common culprit when debugging, the DNS Proxy shares the least\ncode with other system and so is more likely the least audited in this chain.\nThe cascading caching scheme is also complex in its behaviour. Determining\nwhether an issue is caused by the DNS components, in the policy layer or in the\ndatapath is often the first step when debugging toFQDNs related issues.\nGenerally, working top-down is easiest as the information needed to verify\nlow-level correctness can be collected in the initial debug invocations. \n REFUSED vs NXDOMAIN responses \n \nThe proxy uses REFUSED DNS responses to indicate a denied request. Some libc\nimplementations, notably musl which is common in Alpine Linux images, terminate\nthe whole DNS search in these cases. This often manifests as a connect error in\napplications, as the libc lookup returns no data.\nTo work around this, denied responses can be configured to be NXDOMAIN by\nsetting ``--tofqdns-dns-reject-response-code=nameError`` on the command line.\n\n\nMonitor Events\n~~~~~~~~~~~~~~\n\nThe DNS Proxy emits multiple L7 DNS monitor events. One for the request and one\nfor the response (if allowed). Often the L7 DNS rules are paired with L3\ntoFQDNs rules and events relating to those rules are also relevant.\n\n.. Note::\n\n    Be sure to run cilium-dbg monitor on the same node as the pod being debugged!\n\n.. code-block:: shell-session\n\n    $ kubectl exec pod/cilium-sbp8v -n kube-system -- cilium-dbg monitor --related-to 3459\n    Listening for events on 4 CPUs with 64x4096 of shared memory\n    Press Ctrl-C to quit\n    level=info msg=\"Initializing dissection cache...\" subsys=monitor\n\n    -> Request dns from 3459 ([k8s:org=alliance k8s:io.kubernetes.pod.namespace=default k8s:io.cilium.k8s.policy.serviceaccount=default k8s:io.cilium.k8s.policy.cluster=default k8s:class=xwing]) to 0 ([k8s:io.cilium.k8s.policy.serviceaccount=kube-dns k8s:io.kubernetes.pod.namespace=kube-system k8s:k8s-app=kube-dns k8s:io.cilium.k8s.policy.cluster=default]), identity 323->15194, verdict Forwarded DNS Query: cilium.io. A\n    -> endpoint 3459 flow 0xe6866e21 identity 15194->323 state reply ifindex lxc84b58cbdabfe orig-ip 10.60.1.115: 10.63.240.10:53 -> 10.60.0.182:42132 udp\n    -> Response dns to 3459 ([k8s:org=alliance k8s:io.kubernetes.pod.namespace=default k8s:io.cilium.k8s.policy.serviceaccount=default k8s:io.cilium.k8s.policy.cluster=default k8s:class=xwing]) from 0 ([k8s:io.cilium.k8s.policy.cluster=default k8s:io.cilium.k8s.policy.serviceaccount=kube-dns k8s:io.kubernetes.pod.namespace=kube-system k8s:k8s-app=kube-dns]), identity 323->15194, verdict Forwarded DNS Query: cilium.io. A TTL: 486 Answer: '104.198.14.52'\n    -> endpoint 3459 flow 0xe6866e21 identity 15194->323 state reply ifindex lxc84b58cbdabfe orig-ip 10.60.1.115: 10.63.240.10:53 -> 10.60.0.182:42132 udp\n    Policy verdict log: flow 0x614e9723 local EP ID 3459, remote ID 16777217, proto 6, egress, action allow, match L3-Only, 10.60.0.182:41510 -> 104.198.14.52:80 tcp SYN\n\n    -> stack flow 0x614e9723 identity 323->16777217 state new ifindex 0 orig-ip 0.0.0.0: 10.60.0.182:41510 -> 104.198.14.52:80 tcp SYN\n    -> 0: 10.60.0.182:41510 -> 104.198.14.52:80 tcp SYN\n    -> endpoint 3459 flow 0x7388921 identity 16777217->323 state reply ifindex lxc84b58cbdabfe orig-ip 104.198.14.52: 104.198.14.52:80 -> 10.60.0.182:41510 tcp SYN, ACK\n    -> stack flow 0x614e9723 identity 323->16777217 state established ifindex 0 orig-ip 0.0.0.0: 10.60.0.182:41510 -> 104.198.14.52:80 tcp ACK\n    -> 0: 10.60.0.182:41510 -> 104.198.14.52:80 tcp ACK\n    -> stack flow 0x614e9723 identity 323->16777217 state established ifindex 0 orig-ip 0.0.0.0: 10.60.0.182:41510 -> 104.198.14.52:80 tcp ACK\n    -> 0: 10.60.0.182:41510 -> 104.198.14.52:80 tcp ACK\n    -> endpoint 3459 flow 0x7388921 identity 16777217->323 state reply ifindex lxc84b58cbdabfe orig-ip 104.198.14.52: 104.198.14.52:80 -> 10.60.0.182:41510 tcp ACK\n    -> 0: 10.60.0.182:41510 -> 104.198.14.52:80 tcp ACK\n    -> stack flow 0x614e9723 identity 323->16777217 state established ifindex 0 orig-ip 0.0.0.0: 10.60.0.182:41510 -> 104.198.14.52:80 tcp ACK, FIN\n    -> 0: 10.60.0.182:41510 -> 104.198.14.52:80 tcp ACK, FIN\n    -> endpoint 3459 flow 0x7388921 identity 16777217->323 state reply ifindex lxc84b58cbdabfe orig-ip 104.198.14.52: 104.198.14.52:80 -> 10.60.0.182:41510 tcp ACK, FIN\n    -> stack flow 0x614e9723 identity 323->16777217 state established ifindex 0 orig-ip 0.0.0.0: 10.60.0.182:41510 -> 104.198.14.52:80 tcp ACK\n\nThe above is for a simple ``curl cilium.io`` in a pod. The L7 DNS request is\nthe first set of message and the subsequent L3 connection is the HTTP\ncomponent. AAAA DNS lookups commonly happen but were removed to simplify the\nexample.\n\n- If no L7 DNS requests appear, the proxy redirect is not in place. This may\n  mean that the policy does not select this endpoint or there is an issue with\n  the proxy redirection. Whether any redirects exist can be checked with\n  ``cilium-dbg status --all-redirects``.\n  In the past, a bug occurred with more permissive L3 rules overriding the\n  proxy redirect, causing the proxy to never see the requests.\n- If the L7 DNS request is blocked, with an explicit denied message, then the\n  requests are not allowed by the proxy. This may be due to a typo in the\n  network policy, or the matchPattern rule not allowing this domain. It may\n  also be due to a bug in policy propagation to the DNS Proxy.\n- If the DNS request is allowed, with an explicit message, and it should not\n  be, this may be because a more general policy is in place that allows the\n  request. ``matchPattern: \"*\"`` visibility policies are commonly in place and\n  would supersede all other, more restrictive, policies.\n  If no other policies are in place, incorrect allows may indicate a bug when\n  passing policy information to the proxy. There is no way to dump the rules in\n  the proxy, but a debug log is printed when a rule is added. Look for \n  ``DNS Proxy updating matchNames in allowed list during UpdateRules``.\n  The pkg/proxy/dns.go file contains the DNS proxy implementation.\n\nIf L7 DNS behaviour seems correct, see the sections below to further isolate\nthe issue. This can be verified with ``cilium-dbg fqdn cache list``. The IPs in the\nresponse should appear in the cache for the appropriate endpoint. The lookup\ntime is included in the json output of the command.\n\n.. code-block:: shell-session\n\n    $ kubectl exec pod/cilium-sbp8v -n kube-system -- cilium-dbg fqdn cache list\n    Endpoint   Source   FQDN         TTL    ExpirationTime             IPs\n    3459       lookup   cilium.io.   3600   2020-04-21T15:04:27.146Z   104.198.14.52\n\nAs of Cilium 1.16, the ``ExpirationTime`` represents the next time that\nthe entry will be evaluated for staleness. If the entry ``Source`` is\n``lookup``, then the entry will expire at that time. An equivalent entry with\nsource ``connection`` may be established when a ``lookup`` entry expires. If\nthe corresponding Endpoint continues to communicate to this domain via one of\nthe related IP addresses, then Cilium will continue to keep the ``connection``\nentry alive. When the expiration time for a ``connection`` entry is reached,\nthe entry will be re-evaluated to determine whether it is still used by active\nconnections, and at that time may expire or be renewed with a new target\nexpiration time.\n\nDNS Proxy Errors\n~~~~~~~~~~~~~~~~\n\nREFUSED responses are returned when the proxy encounters an error during\nprocessing. This can be confusing to debug as that is also the response when a\nDNS request is denied.  An error log is always printed in these cases. Some are\ncallbacks provided by other packages via daemon in cilium-agent.\n\n- ``Rejecting DNS query from endpoint due to error``: This is the \"normal\"\n  policy-reject message. It is a debug log.\n- ``cannot extract endpoint IP from DNS request``: The proxy cannot read the\n  socket information to read the source endpoint IP. This could mean an\n  issue with the datapath routing and information passing.\n- ``cannot extract endpoint ID from DNS request``: The proxy cannot use the\n  source endpoint IP to get the cilium-internal ID for that endpoint. This is\n  different from the Security Identity. This could mean that cilium is not\n  managing this endpoint and that something has gone awry. It could also mean a\n  routing problem where a packet has arrived at the proxy incorrectly.\n- ``cannot extract destination IP:port from DNS request``: The proxy cannot\n  read the socket information of the original request to obtain the intended\n  target IP:Port. This could mean an issue with the datapath routing and\n  information passing.\n- ``cannot find server ip in ipcache``: The proxy cannot resolve a Security\n  Identity for the target IP of the DNS request. This should always succeed, as\n  world catches all IPs not set by more specific entries. This can mean a\n  broken ipcache BPF table.\n- ``Rejecting DNS query from endpoint due to error``: While checking if the DNS\n  request was allowed (based on Endpoint ID, destination IP:Port and the DNS\n  query) an error occurred. These errors would come from the internal rule\n  lookup in the proxy, the ``allowed`` field.\n- ``Timeout waiting for response to forwarded proxied DNS lookup``: The proxy\n  forwards requests 1:1 and does not cache. It applies a 10s timeout on\n  responses to those requests, as the client will retry within this period\n  (usually). Bursts of these errors can happen if the DNS target server\n  misbehaves and many pods see DNS timeouts. This isn't an actual problem with\n  cilium or the proxy although it can be caused by policy blocking the DNS\n  target server if it is in-cluster.\n- ``Timed out waiting for datapath updates of FQDN IP information; returning\n  response``: When the proxy updates the DNS caches with response data, it\n  needs to allow some time for that information to get into the datapath.\n  Otherwise, pods would attempt to make the outbound connection (the thing that\n  caused the DNS lookup) before the datapath is ready. Many stacks retry the\n  SYN in such cases but some return an error and some apps further crash as a\n  response. This delay is configurable by setting the\n  ``--tofqdns-proxy-response-max-delay`` command line argument but defaults to\n  100ms. It can be exceeded if the system is under load.\n\n.. _isolating-source-toFQDNs-issues-identities-policy:\n\nIdentities and Policy\n~~~~~~~~~~~~~~~~~~~~~\n\nOnce a DNS response has been passed back through the proxy and is placed in the\nDNS cache ``toFQDNs`` rules can begin using the IPs in the cache. There are\nmultiple layers of cache:\n\n- A per-Endpoint ``DNSCache`` stores the lookups for this endpoint. It is\n  restored on cilium startup with the endpoint. Limits are applied here for\n  ``--tofqdns-endpoint-max-ip-per-hostname`` and TTLs are tracked. The\n  ``--tofqdns-min-ttl`` is not used here.\n- A per-Endpoint ``DNSZombieMapping`` list of IPs that have expired from the\n  per-Endpoint cache but are waiting for the Connection Tracking GC to mark\n  them in-use or not. This can take up to 12 hours to occur. This list is\n  size-limited by ``--tofqdns-max-deferred-connection-deletes``. \n- A global ``DNSCache`` where all endpoint and poller DNS data is collected. It\n  does apply the ``--tofqdns-min-ttl`` value but not the\n  ``--tofqdns-endpoint-max-ip-per-hostname`` value.\n\nIf an IP exists in the FQDN cache (check with ``cilium-dbg fqdn cache list``) then\n``toFQDNs`` rules that select a domain name, either explicitly via\n``matchName`` or via ``matchPattern``, should cause IPs for that domain to have\nallocated Security Identities. These can be listed with:\n\n.. code-block:: shell-session\n\n    $ kubectl exec pod/cilium-sbp8v -n kube-system -- cilium-dbg identity list\n    ID         LABELS\n    1          reserved:host\n    2          reserved:world\n    3          reserved:unmanaged\n    4          reserved:health\n    5          reserved:init\n    6          reserved:remote-node\n    323        k8s:class=xwing\n               k8s:io.cilium.k8s.policy.cluster=default\n               k8s:io.cilium.k8s.policy.serviceaccount=default\n               k8s:io.kubernetes.pod.namespace=default\n               k8s:org=alliance\n    ...\n    16777217   fqdn:*\n               reserved:world\n\nNote that FQDN identities are allocated locally on the node and have a high-bit set so they are often in the 16-million range.\nNote that this is the identity in the monitor output for the HTTP connection.\n\nIn cases where there is no matching identity for an IP in the fqdn cache it may\nsimply be because no policy selects an associated domain. The policy system\nrepresents each ``toFQDNs:`` rule with a ``FQDNSelector`` instance. These\nreceive updates from a global ``NameManager`` in the daemon.\nThey can be listed along with other selectors (roughly corresponding to any L3 rule):\n\n.. code-block:: shell-session\n\n    $ kubectl exec pod/cilium-sbp8v -n kube-system -- cilium-dbg policy selectors\n    SELECTOR                                                                                                         USERS   IDENTITIES\n    MatchName: , MatchPattern: *                                                                                     1       16777217\n    &LabelSelector{MatchLabels:map[string]string{},MatchExpressions:[]LabelSelectorRequirement{},}                   2       1\n                                                                                                                             2\n                                                                                                                             3\n                                                                                                                             4\n                                                                                                                             5\n                                                                                                                             6\n                                                                                                                             323\n                                                                                                                             6188\n                                                                                                                             15194\n                                                                                                                             18892\n                                                                                                                             25379\n                                                                                                                             29200\n                                                                                                                             32255\n                                                                                                                             33831\n                                                                                                                             16777217\n    &LabelSelector{MatchLabels:map[string]string{reserved.none: ,},MatchExpressions:[]LabelSelectorRequirement{},}   1\n\nIn this example 16777217 is used by two selectors, one with ``matchPattern: \"*\"``\nand another empty one. This is because of the policy in use:\n\n.. code-block:: yaml\n\n    apiVersion: cilium.io/v2\n    kind: CiliumNetworkPolicy\n    metadata:\n      name: \"tofqdn-dns-visibility\"\n    spec:\n      endpointSelector:\n        matchLabels:\n          any:org: alliance\n      egress:\n      - toPorts:\n          - ports:\n             - port: \"53\"\n               protocol: ANY\n            rules:\n              dns:\n                - matchPattern: \"*\"\n      - toFQDNs:\n          - matchPattern: \"*\"\n\nThe L7 DNS rule has an implicit L3 allow-all because it defines only L4 and L7\nsections. This is the second selector in the list, and includes all possible L3\nidentities known in the system. In contrast, the first selector, which\ncorresponds to the ``toFQDNS: matchName: \"*\"`` rule would list all identities\nfor IPs that came from the DNS Proxy.\n\nUnintended DNS Policy Drops\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``toFQDNSs`` policy enforcement relies on the source pod performing a DNS query\nbefore using an IP address returned in the DNS response. Sometimes pods may hold\non to a DNS response and start new connections to the same IP address at a later\ntime. This may trigger policy drops if the DNS response has expired as requested\nby the DNS server in the time-to-live (TTL) value in the response. When DNS is\nused for service load balancing the advertised TTL value may be short (e.g., 60\nseconds).\n\nCilium honors the TTL values returned by the DNS server by default, but you can\noverride them by setting a minimum TTL using ``--tofqdns-min-ttl`` flag. This\nsetting overrides short TTLs and allows the pod to use the IP address in the DNS\nresponse for a longer duration. Existing connections also keep the IP address as\nallowed in the policy.\n\nAny new connections opened by the pod using the same IP address without\nperforming a new DNS query after the (possibly extended) DNS TTL has expired are\ndropped by Cilium policy enforcement. To allow pods to use the DNS response\nafter TTL expiry for new connections, a command line option\n``--tofqdns-idle-connection-grace-period`` may be used to keep the IP address /\nname mapping valid in the policy for an extended time after DNS TTL expiry. This\noption takes effect only if the pod has opened at least one connection during\nthe DNS TTL period.\n\nDatapath Plumbing\n~~~~~~~~~~~~~~~~~\n\nFor a policy to be fully realized the datapath for an Endpoint must be updated.\nIn the case of a new DNS-source IP, the FQDN identity associated with it must\npropagate from the selectors to the Endpoint specific policy. Unless a new\npolicy is being added, this often only involves updating the Policy Map of the\nEndpoint with the new FQDN Identity of the IP. This can be verified:\n\n.. code-block:: shell-session\n\n    $ kubectl exec pod/cilium-sbp8v -n kube-system -- cilium-dbg bpf policy get 3459\n    DIRECTION   LABELS (source:key[=value])   PORT/PROTO   PROXY PORT   BYTES   PACKETS\n    Ingress     reserved:unknown              ANY          NONE         1367    7\n    Ingress     reserved:host                 ANY          NONE         0       0\n    Egress      reserved:unknown              53/TCP       36447        0       0\n    Egress      reserved:unknown              53/UDP       36447        138     2\n    Egress      fqdn:*                        ANY          NONE         477     6\n                reserved:world \n\nNote that the labels for identities are resolved here. This can be skipped, or\nthere may be cases where this doesn't occur:\n\n.. code-block:: shell-session\n\n    $ kubectl exec pod/cilium-sbp8v -n kube-system -- cilium-dbg bpf policy get -n 3459\n    DIRECTION   IDENTITY   PORT/PROTO   PROXY PORT   BYTES   PACKETS\n    Ingress     0          ANY          NONE         1367    7\n    Ingress     1          ANY          NONE         0       0\n    Egress      0          53/TCP       36447        0       0\n    Egress      0          53/UDP       36447        138     2\n    Egress      16777217   ANY          NONE         477     6\n\n\nL3 ``toFQDNs`` rules are egress only, so we would expect to see an ``Egress``\nentry with Security Identity ``16777217``. The L7 rule, used to redirect to the\nDNS Proxy is also present with a populated ``PROXY PORT``. It has a 0\n``IDENTITY`` as it is an L3 wildcard, i.e. the policy allows any peer on the\nspecified port.\n\nAn identity missing here can be an error in various places:\n\n- Policy doesn't actually allow this Endpoint to connect. A sanity check is to\n  use ``cilium-dbg endpoint list`` to see if cilium thinks it should have policy\n  enforcement.\n- Endpoint regeneration is slow and the Policy Map has not been updated yet.\n  This can occur in cases where we have leaked IPs from the DNS cache (i.e.\n  they were never deleted correctly) or when there are legitimately many IPs.\n  It can also simply mean an overloaded node or even a deadlock within cilium.\n- A more permissive policy has removed the need to include this identity. This\n  is likely a bug, however, as the IP would still have an identity allocated\n  and it would be included in the Policy Map.  In the past, a similar bug\n  occurred with the L7 redirect and that would stop this whole process at the\n  beginning.\n\nMutexes / Locks and Data Races\n------------------------------\n\n.. Note::\n\n    This section only applies to Golang code.\n\nThere are a few options available to debug Cilium data races and deadlocks.\n\nTo debug data races, Golang allows ``-race`` to be passed to the compiler to\ncompile Cilium with race detection. Additionally, the flag can be provided to\n``go test`` to detect data races in a testing context.\n\n.. _compile-cilium-with-race-detection:\n\n~~~~~~~~~~~~~~\nRace detection\n~~~~~~~~~~~~~~\n\nTo compile a Cilium binary with race detection, you can do:\n\n.. code-block:: shell-session\n\n    $ make RACE=1\n\n.. Note::\n\n    For building the Operator with race detection, you must also provide\n    ``BASE_IMAGE`` which can be the ``cilium/cilium-runtime`` image from the\n    root Dockerfile found in the Cilium repository.\n\nTo run integration tests with race detection, you can do:\n\n.. code-block:: shell-session\n\n    $ make RACE=1 integration-tests\n\n~~~~~~~~~~~~~~~~~~\nDeadlock detection\n~~~~~~~~~~~~~~~~~~\n\nCilium can be compiled with a build tag ``lockdebug`` which will provide a\nseamless wrapper over the standard mutex types in Golang, via\n`sasha-s/go-deadlock library <https://github.com/sasha-s/go-deadlock>`_. No\naction is required, besides building the binary with this tag.\n\nFor example:\n\n.. code-block:: shell-session\n\n    $ make LOCKDEBUG=1\n    $ # Deadlock detection during integration tests:\n    $ make LOCKDEBUG=1 integration-tests\n\nMoreover, you can enable mutex contention and blocked goroutine profiling with ``pprof`` (see below for more ``pprof`` examples).\nThese features can be enabled with the ``--pprof-block-profile-rate`` and ``--pprof-mutex-profile-fraction`` flags. Note that the block profiler\nis `not recommended <https://github.com/DataDog/go-profiler-notes/blob/65dd611ec7b225a8c843a284f755e3cfe0593176/guide/README.md#block-profiler-limitations>`_\nfor production due to performance overhead.\n\nCPU Profiling and Memory Leaks\n------------------------------\n\nCilium bundles ``gops``, a standard tool for Golang applications, which\nprovides the ability to collect CPU and memory profiles using ``pprof``.\nInspecting profiles can help identify CPU bottlenecks and memory leaks.\n\nTo capture a profile, take a :ref:`sysdump <sysdump>` of the cluster with the\nCilium CLI or more directly, use the ``cilium-bugtool`` command that is\nincluded in the Cilium image after enabling ``pprof`` in the Cilium ConfigMap:\n\n.. code-block:: shell-session\n\n    $ kubectl exec -ti -n kube-system <cilium-pod-name> -- cilium-bugtool --get-pprof --pprof-trace-seconds N\n    $ kubectl cp -n kube-system <cilium-pod-name>:/tmp/cilium-bugtool-<time-generated-name>.tar ./cilium-pprof.tar\n    $ tar xf ./cilium-pprof.tar\n\nBe mindful that the profile window is the number of seconds passed to\n``--pprof-trace-seconds``. Ensure that the number of seconds are enough to\ncapture Cilium while it is exhibiting the problematic behavior to debug.\n\nThere are 6 files that encompass the tar archive:\n\n.. code-block:: shell-session\n\n    Permissions Size User  Date Modified Name\n    .rw-r--r--   940 chris  6 Jul 14:04  gops-memstats-$(pidof-cilium-agent).md\n    .rw-r--r--  211k chris  6 Jul 14:04  gops-stack-$(pidof-cilium-agent).md\n    .rw-r--r--    58 chris  6 Jul 14:04  gops-stats-$(pidof-cilium-agent).md\n    .rw-r--r--   212 chris  6 Jul 14:04  pprof-cpu\n    .rw-r--r--  2.3M chris  6 Jul 14:04  pprof-heap\n    .rw-r--r--   25k chris  6 Jul 14:04  pprof-trace\n\nThe files prefixed with ``pprof-`` are profiles. For more information on each\none, see `Julia Evan's blog`_ on ``pprof``.\n\nTo view the CPU or memory profile, simply execute the following command:\n\n.. code-block:: shell-session\n\n    $ go tool pprof -http localhost:9090 pprof-cpu  # for CPU\n    $ go tool pprof -http localhost:9090 pprof-heap # for memory\n\nThis opens a browser window for profile inspection.\n\n.. _Julia Evan's blog: https://jvns.ca/blog/2017/09/24/profiling-go-with-pprof/",
  "item_type": "unknown",
  "module_path": "/tmp/cilium-repo/Documentation/contributing/development/debugging.rst",
  "extracted_at": "2025-09-03T00:53:44.757818Z"
}