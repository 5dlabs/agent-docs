{
  "url": "file:///tmp/cilium-repo/Documentation/network/kubernetes/bandwidth-manager.rst",
  "content": ".. only:: not (epub or latex or html) \n WARNING: You are looking at unreleased Cilium documentation.\nPlease use the official rendered version released here:\nhttps://docs.cilium.io\n \n .. _bandwidth-manager: \n \n Bandwidth Manager \n \n This guide explains how to configure Cilium's bandwidth manager to\noptimize TCP and UDP workloads and efficiently rate limit individual Pods\nif needed through the help of EDT (Earliest Departure Time) and eBPF.\nCilium's bandwidth manager is also prerequisite for enabling BBR congestion\ncontrol for Pods as outlined :ref: below<BBR Pods> . \n The bandwidth manager does not rely on CNI chaining and is natively integrated\ninto Cilium instead. Hence, it does not make use of the  bandwidth CNI <https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#support-traffic-shaping> _\nplugin. Due to scalability concerns in particular for multi-queue network\ninterfaces, it is not recommended to use the bandwidth CNI plugin which is\nbased on TBF (Token Bucket Filter) instead of EDT. \n .. note:: \n It is strongly recommended to use Bandwidth Manager in combination with\n:ref: BPF Host Routing<eBPF_Host_Routing>  as otherwise legacy routing\nthrough the upper stack could potentially result in undesired high latency\n(see  this comparison <https://github.com/cilium/cilium/issues/29083#issuecomment-1831867718> _\nfor more details). \n Cilium's bandwidth manager supports both  kubernetes.io/egress-bandwidth  and\n kubernetes.io/ingress-bandwidth  Pod annotations. The  egress-bandwidth  is enforced on egress at\nthe native host networking devices using EDT (Earliest Departure Time), while the  ingress-bandwidth \nis enforced using an eBPF-based token bucket implementation.\nThe bandwidth enforcement is supported for direct routing as well as tunneling\nmode in Cilium. \n .. include:: ../../installation/k8s-install-download-release.rst \n Cilium's bandwidth manager is disabled by default on new installations.\nTo install Cilium with the bandwidth manager enabled, run \n .. parsed-literal:: \n helm install cilium |CHART_RELEASE| \\\n--namespace kube-system \\\n--set bandwidthManager.enabled=true \n To enable the bandwidth manager on an existing installation, run \n .. parsed-literal:: \n helm upgrade cilium |CHART_RELEASE| \\\n--namespace kube-system \\\n--reuse-values \\\n--set bandwidthManager.enabled=true\nkubectl -n kube-system rollout restart ds/cilium \n The native host networking devices are auto detected as native devices which have\nthe default route on the host or have Kubernetes  InternalIP  or  ExternalIP  assigned.\n InternalIP  is preferred over  ExternalIP  if both exist. To change and manually specify\nthe devices, set their names in the  devices  helm option (e.g.\n devices='{eth0,eth1,eth2}' ). Each listed device has to be named the same\non all Cilium-managed nodes. \n Verify that the Cilium Pods have come up correctly: \n .. code-block:: shell-session \n $ kubectl -n kube-system get pods -l k8s-app=cilium\nNAME                READY     STATUS    RESTARTS   AGE\ncilium-crf7f        1/1       Running   0          10m\ncilium-db21a        1/1       Running   0          10m\n \n In order to verify whether the bandwidth manager feature has been enabled in Cilium,\nthe  cilium status  CLI command provides visibility through the  BandwidthManager \ninfo line. It also dumps a list of devices on which the egress bandwidth limitation\nis enforced: \n .. code-block:: shell-session \n $ kubectl -n kube-system exec ds/cilium -- cilium-dbg status | grep BandwidthManager\nBandwidthManager:       EDT with BPF [BBR] [eth0]\n \n To verify that bandwidth limits are indeed being enforced, one can deploy two\n netperf  Pods in different nodes: \n .. code-block:: yaml \n ---\napiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    # Limits egress bandwidth to 10Mbit/s and ingress bandwidth to 20Mbit/s.\n    kubernetes.io/egress-bandwidth: \"10M\"\n    kubernetes.io/ingress-bandwidth: \"20M\"\n  labels:\n    # This pod will act as server.\n    app.kubernetes.io/name: netperf-server\n  name: netperf-server\nspec:\n  containers:\n  - name: netperf\n    image: cilium/netperf\n    args:\n    - iperf3\n    - \"-s\"\n    ports:\n    - containerPort: 5201\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  # This Pod will act as client.\n  name: netperf-client\nspec:\n  affinity:\n    # Prevents the client from being scheduled to the\n    # same node as the server.\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n          matchExpressions:\n          - key: app.kubernetes.io/name\n            operator: In\n            values:\n            - netperf-server\n        topologyKey: kubernetes.io/hostname\n  containers:\n  - name: netperf\n    args:\n    - sleep\n    - infinity\n    image: cilium/netperf\n \n Once up and running, the  netperf-client  Pod can be used to test bandwidth enforcement\non the  netperf-server  Pod.\nFirst test the egress bandwidth: \n .. code-block:: shell-session \n $ NETPERF_SERVER_IP=$(kubectl get pod netperf-server -o jsonpath='{.status.podIP}')\n$ kubectl exec netperf-client --  \niperf3 -R -c \"${NETPERF_SERVER_IP}\"\nConnecting to host 10.42.0.52, port 5201\nReverse mode, remote host 10.42.0.52 is sending\n[  5] local 10.42.1.23 port 49422 connected to 10.42.0.52 port 5201\n[ ID] Interval           Transfer     Bitrate\n[  5]   0.00-1.00   sec  1.19 MBytes  9.99 Mbits/sec\n[  5]   1.00-2.00   sec  1.17 MBytes  9.77 Mbits/sec\n[  5]   2.00-3.00   sec  1.10 MBytes  9.26 Mbits/sec\n[  5]   3.00-4.00   sec  1.17 MBytes  9.77 Mbits/sec\n[  5]   4.00-5.00   sec  1.17 MBytes  9.77 Mbits/sec\n[  5]   5.00-6.00   sec  1.10 MBytes  9.26 Mbits/sec\n[  5]   6.00-7.00   sec  1.17 MBytes  9.77 Mbits/sec\n[  5]   7.00-8.00   sec  1.10 MBytes  9.26 Mbits/sec\n[  5]   8.00-9.00   sec  1.17 MBytes  9.77 Mbits/sec\n[  5]   9.00-10.00  sec  1.10 MBytes  9.26 Mbits/sec\n- - - - - - - - - - - - - - - - - - - - - - - - -\n[ ID] Interval           Transfer     Bitrate         Retr\n[  5]   0.00-10.09  sec  14.1 MBytes  11.7 Mbits/sec    0             sender\n[  5]   0.00-10.00  sec  11.4 MBytes  9.59 Mbits/sec                  receiver \n As can be seen, egress traffic of the  netperf-server  Pod has been limited to 10Mbit per second.\nThen test the ingress bandwidth. \n .. code-block:: shell-session \n $ NETPERF_SERVER_IP=$(kubectl get pod netperf-server -o jsonpath='{.status.podIP}')\n$ kubectl exec netperf-client --  \niperf3 -c \"${NETPERF_SERVER_IP}\"\nConnecting to host 10.42.0.52, port 5201\n[  5] local 10.42.1.23 port 40058 connected to 10.42.0.52 port 5201\n[ ID] Interval           Transfer     Bitrate         Retr  Cwnd\n[  5]   0.00-1.00   sec  6.73 MBytes  56.4 Mbits/sec  551   25.9 KBytes\n[  5]   1.00-2.00   sec  3.56 MBytes  29.9 Mbits/sec  159   8.19 KBytes\n[  5]   2.00-3.00   sec  2.45 MBytes  20.6 Mbits/sec  191   2.73 KBytes\n[  5]   3.00-4.00   sec  1.17 MBytes  9.77 Mbits/sec  170   34.1 KBytes\n[  5]   4.00-5.00   sec  2.39 MBytes  20.1 Mbits/sec  224   8.19 KBytes\n[  5]   5.00-6.00   sec  2.45 MBytes  20.6 Mbits/sec  274   6.83 KBytes\n[  5]   6.00-7.00   sec  2.39 MBytes  20.1 Mbits/sec  170   2.73 KBytes\n[  5]   7.00-8.00   sec  2.45 MBytes  20.6 Mbits/sec  262   5.46 KBytes\n[  5]   8.00-9.00   sec  2.45 MBytes  20.6 Mbits/sec  260   5.46 KBytes\n[  5]   9.00-10.00  sec  2.42 MBytes  20.3 Mbits/sec  210   32.8 KBytes\n- - - - - - - - - - - - - - - - - - - - - - - - -\n[ ID] Interval           Transfer     Bitrate         Retr\n[  5]   0.00-10.00  sec  28.5 MBytes  23.9 Mbits/sec  2471             sender\n[  5]   0.00-10.04  sec  25.6 MBytes  21.4 Mbits/sec                  receiver \n As can be seen, ingress traffic of the  netperf-server  Pod has been limited to 20Mbit per second. \n In order to introspect current endpoint bandwidth settings from BPF side, the following\ncommand can be run (replace  cilium-xxxxx  with the name of the Cilium Pod that is co-located with\nthe  netperf-server  Pod): \n .. code-block:: shell-session \n $ kubectl exec -it -n kube-system cilium-xxxxxx -- cilium-dbg bpf bandwidth list\nIDENTITY   DIRECTION   PRIO   BANDWIDTH (BitsPerSec)\n724        Egress      0      10M\n724        Ingress     0      50M\n \n Each Pod is represented in Cilium as an :ref: endpoint  which has an identity. The above\nidentity can then be correlated with the  cilium-dbg endpoint list  command. \n .. note:: \n Bandwidth limits apply on a per-Pod scope. In our example, if multiple\nreplicas of the Pod are created, then each of the Pod instances receives\na 10M bandwidth limit. \n .. _BBR Pods: \n BBR for Pods\n############ \n The base infrastructure around MQ/FQ setup provided by Cilium's bandwidth manager\nalso allows for use of TCP  BBR congestion control <https://queue.acm.org/detail.cfm?id=3022184> _\nfor Pods. \n BBR is in particular suitable when Pods are exposed behind Kubernetes Services which\nface external clients from the Internet. BBR achieves higher bandwidths and lower\nlatencies for Internet traffic, for example, it has been  shown <https://cloud.google.com/blog/products/networking/tcp-bbr-congestion-control-comes-to-gcp-your-internet-just-got-faster> _ that BBR's throughput can reach as much\nas 2,700x higher than today's best loss-based congestion control and queueing delays\ncan be 25x lower. \n .. note:: \n BBR for Pods requires a v5.18.x or more recent Linux kernel. \n To enable the bandwidth manager with BBR congestion control, deploy with the following: \n .. parsed-literal:: \n helm upgrade cilium |CHART_RELEASE| \\\n--namespace kube-system \\\n--reuse-values \\\n--set bandwidthManager.enabled=true \\\n--set bandwidthManager.bbr=true\nkubectl -n kube-system rollout restart ds/cilium \n In order for BBR to work reliably for Pods, it requires a 5.18 or higher kernel.\nAs outlined in our  Linux Plumbers 2021 talk <https://lpc.events/event/11/contributions/953/> _,\nthis is needed since older kernels do not retain timestamps of network packets\nwhen switching from Pod to host network namespace. Due to the latter, the kernel's\npacing infrastructure does not function properly in general (not specific to Cilium). \n We helped with fixing this issue for recent kernels to retain timestamps and therefore\nto get BBR for Pods working. Prior to that kernel, BBR was only working for sockets\nwhich are in the initial network namespace (hostns). BBR also needs eBPF Host-Routing\nin order to retain the network packet's socket association all the way until the\npacket hits the FQ queueing discipline on the physical device in the host namespace.\n(Without eBPF Host-Routing the packet's socket association would otherwise be orphaned\ninside the host stacks forwarding/routing layer.). \n In order to verify whether the bandwidth manager with BBR has been enabled in Cilium,\nthe  cilium status  CLI command provides visibility again through the  BandwidthManager \ninfo line: \n .. code-block:: shell-session \n $ kubectl -n kube-system exec ds/cilium -- cilium-dbg status | grep BandwidthManager\nBandwidthManager:       EDT with BPF [BBR] [eth0]\n \n Once this setting is enabled, it will use BBR as a default for all newly spawned Pods.\nIdeally, BBR is selected upon initial Cilium installation when the cluster is created\nsuch that all nodes and Pods in the cluster homogeneously use BBR as otherwise there\ncould be  potential unfairness issues <https://blog.apnic.net/2020/01/10/when-to-use-and-not-use-bbr/> _\nfor other connections still using CUBIC. Also note that due to the nature of BBR's\nprobing you might observe a higher rate of TCP retransmissions compared to CUBIC. \n We recommend to use BBR in particular for clusters where Pods are exposed as Services\nwhich serve external clients connecting from the Internet. \n BBR for The Host\n################\nIn legacy routing mode, it is not possible to enable BBR for Cilium-managed pods\n( hostNetwork: false ) for the reasons mentioned above; however, it is\npossible to enable BBR for  only  the host network namespace by adding the\n bandwidthManager.bbrHostNamespaceOnly=true  flag. \n .. parsed-literal:: \n helm upgrade cilium |CHART_RELEASE| \\\n--namespace kube-system \\\n--reuse-values \\\n--set bandwidthManager.enabled=true \\\n--set bandwidthManager.bbr=true\n--set bandwidthManager.bbrHostNamespaceOnly=true\nkubectl -n kube-system rollout restart ds/cilium \n With  bandwidthManager.bbrHostNamespaceOnly , processes in the host network\nnamespace, including pods that set  hostNetwork  to  true , will use BBR. \n Limitations\n########### \n * Bandwidth enforcement currently does not work in combination with L7 Cilium Network Policies.\n  In case they select the Pod at egress, then the bandwidth enforcement will be disabled for\n  those Pods.\n* Bandwidth enforcement doesn't work with nested network namespace environments like Kind. This is because\n  they typically don't have access to the global sysctl under ``/proc/sys/net/core`` and the\n  bandwidth enforcement depends on them.\n \n .. admonition:: Video\n:class: attention \n For more insights on Cilium's bandwidth manager, check out this  KubeCon talk on Better Bandwidth Management with eBPF <https://www.youtube.com/watch?v=QTSS6ktK8hY> __ and  eCHO episode 98: Exploring the bandwidth manager with Cilium <https://www.youtube.com/watch?v=-JnXe8vAUKQ> __.",
  "item_type": "unknown",
  "module_path": "/tmp/cilium-repo/Documentation/network/kubernetes/bandwidth-manager.rst",
  "extracted_at": "2025-09-03T01:13:29.247067Z"
}