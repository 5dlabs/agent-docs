{
  "url": "file:///tmp/cilium-repo/Documentation/installation/k8s-install-kubespray.rst",
  "content": ".. only:: not (epub or latex or html) \n WARNING: You are looking at unreleased Cilium documentation.\nPlease use the official rendered version released here:\nhttps://docs.cilium.io\n \n .. _k8s_install_kubespray: \n \n Installation using Kubespray \n \n The guide is to use Kubespray for creating an AWS Kubernetes cluster running\nCilium as the CNI. The guide uses: \n \n Kubespray v2.6.0 \n Latest  Cilium released version _ (instructions for using the version are mentioned below) \n \n Please consult  Kubespray Prerequisites <https://github.com/kubernetes-sigs/kubespray#requirements> __ and Cilium :ref: admin_system_reqs . \n ..  Cilium released version:  latest released Cilium version \n Installing Kubespray \n .. code-block:: shell-session \n $ git clone --branch v2.6.0 https://github.com/kubernetes-sigs/kubespray \n Install dependencies from  requirements.txt \n .. code-block:: shell-session \n $ cd kubespray\n$ sudo pip install -r requirements.txt \n Infrastructure Provisioning \n We will use Terraform for provisioning AWS infrastructure. \n Configure AWS credentials \n Export the variables for your AWS credentials \n .. code-block:: shell-session \n export AWS_ACCESS_KEY_ID=\"www\"\nexport AWS_SECRET_ACCESS_KEY =\"xxx\"\nexport AWS_SSH_KEY_NAME=\"yyy\"\nexport AWS_DEFAULT_REGION=\"zzz\" \n Configure Terraform Variables \n We will start by specifying the infrastructure needed for the Kubernetes cluster. \n .. code-block:: shell-session \n $ cd contrib/terraform/aws\n$ cp contrib/terraform/aws/terraform.tfvars.example terraform.tfvars \n Open the file and change any defaults particularly, the number of master, etcd, and worker nodes.\nYou can change the master and etcd number to 1 for deployments that don't need high availability.\nBy default, this tutorial will create: \n \n VPC with 2 public and private subnets \n Bastion Hosts and NAT Gateways in the Public Subnet \n Three of each (masters, etcd, and worker nodes) in the Private Subnet \n AWS ELB in the Public Subnet for accessing the Kubernetes API from\nthe internet \n Terraform scripts using  CoreOS  as base image. \n \n Example  terraform.tfvars  file: \n .. code-block:: bash \n #Global Vars\naws_cluster_name = \"kubespray\" \n #VPC Vars\naws_vpc_cidr_block = \"XXX.XXX.192.0/18\"\naws_cidr_subnets_private = [\"XXX.XXX.192.0/20\",\"XXX.XXX.208.0/20\"]\naws_cidr_subnets_public = [\"XXX.XXX.224.0/20\",\"XXX.XXX.240.0/20\"] \n #Bastion Host\naws_bastion_size = \"t2.medium\" \n #Kubernetes Cluster \n aws_kube_master_num = 3\naws_kube_master_size = \"t2.medium\" \n aws_etcd_num = 3\naws_etcd_size = \"t2.medium\" \n aws_kube_worker_num = 3\naws_kube_worker_size = \"t2.medium\" \n #Settings AWS ELB \n aws_elb_api_port = 6443\nk8s_secure_api_port = 6443\nkube_insecure_apiserver_address = \"0.0.0.0\" \n Apply the configuration \n terraform init  to initialize the following modules \n \n module.aws-vpc \n module.aws-elb \n module.aws-iam \n \n .. code-block:: shell-session \n $ terraform init \n Once initialized , execute: \n .. code-block:: shell-session \n $ terraform plan -out=aws_kubespray_plan \n This will generate a file,  aws_kubespray_plan , depicting an execution\nplan of the infrastructure that will be created on AWS. To apply, execute: \n .. code-block:: shell-session \n $ terraform init\n$ terraform apply \"aws_kubespray_plan\" \n Terraform automatically creates an Ansible Inventory file at  inventory/hosts . \n Installing Kubernetes cluster with Cilium as CNI \n Kubespray uses Ansible as its substrate for provisioning and orchestration. Once the infrastructure is created, you can run the Ansible playbook to install Kubernetes and all the required dependencies. Execute the below command in the kubespray clone repo, providing the correct path of the AWS EC2 ssh private key in  ansible_ssh_private_key_file=<path to EC2 SSH private key file> \n We recommend using the  latest released Cilium version _ by passing the variable when running the  ansible-playbook  command.\nFor example, you could add the following flag to the command below:  -e cilium_version=v1.11.0 . \n .. code-block:: shell-session \n $ ansible-playbook -i ./inventory/hosts ./cluster.yml -e ansible_user=core -e bootstrap_os=coreos -e kube_network_plugin=cilium -b --become-user=root --flush-cache  -e ansible_ssh_private_key_file= \n .. _latest released Cilium version: https://github.com/cilium/cilium/releases \n If you are interested in configuring your Kubernetes cluster setup, you should consider copying the sample inventory. Then, you can edit the variables in the relevant file in the  group_vars  directory. \n .. code-block:: shell-session \n $ cp -r inventory/sample inventory/my-inventory\n$ cp ./inventory/hosts ./inventory/my-inventory/hosts\n$ echo 'cilium_version: \"v1.11.0\"' >> ./inventory/my-inventory/group_vars/k8s_cluster/k8s-net-cilium.yml\n$ ansible-playbook -i ./inventory/my-inventory/hosts ./cluster.yml -e ansible_user=core -e bootstrap_os=coreos -e kube_network_plugin=cilium -b --become-user=root --flush-cache -e ansible_ssh_private_key_file= \n Validate Cluster \n To check if cluster is created successfully, ssh into the bastion host with the user  core . \n .. code-block:: shell-session \n $ # Get information about the basiton host\n$ cat ssh-bastion.conf\n$ ssh -i ~/path/to/ec2-key-file.pem core@public_ip_of_bastion_host \n Execute the commands below from the bastion host. If  kubectl  isn't installed on the bastion host, you can login to the master node to test the below commands. You may need to copy the private key to the bastion host to access the master node. \n .. include:: k8s-install-validate.rst \n Delete Cluster \n .. code-block:: shell-session \n $ cd contrib/terraform/aws\n$ terraform destroy",
  "item_type": "unknown",
  "module_path": "/tmp/cilium-repo/Documentation/installation/k8s-install-kubespray.rst",
  "extracted_at": "2025-09-03T01:13:29.313408Z"
}