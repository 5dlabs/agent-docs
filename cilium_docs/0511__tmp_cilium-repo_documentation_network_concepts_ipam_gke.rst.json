{
  "url": "file:///tmp/cilium-repo/Documentation/network/concepts/ipam/gke.rst",
  "content": ".. only:: not (epub or latex or html)\n\n    WARNING: You are looking at unreleased Cilium documentation.\n    Please use the official rendered version released here:\n    https://docs.cilium.io\n\n.. _ipam_gke:\n\n########################\nGoogle Kubernetes Engine\n########################\n\nWhen running Cilium on Google GKE, the native networking layer of Google Cloud\nwill be utilized for address management and IP forwarding.\n\n************\nArchitecture\n************\n\n.. image:: gke_ipam_arch.png\n    :align: center\n\nCilium running in a GKE configuration mode utilizes the Kubernetes hostscope\nIPAM mode. It will configure the Cilium agent to wait until the Kubernetes node\nresource is populated with a ``spec.podCIDR`` or ``spec.podCIDRs`` as required\nby the enabled address families (IPv4/IPv6). See :ref:`k8s_hostscope` for\nadditional details of this IPAM mode.\n\nThe corresponding datapath is described in section :ref:`gke_datapath`.\n\nSee the getting started guide :ref:`k8s_install_quick` to install Cilium Google\nKubernetes Engine (GKE).\n\n*************\nConfiguration\n*************\n\nThe GKE IPAM mode can be enabled by setting the Helm option\n``ipam.mode=kubernetes`` or by setting the ConfigMap option ``ipam:\nkubernetes``.\n\n***************\nTroubleshooting\n***************\n\nValidate the exposed PodCIDR field\n==================================\n\nCheck if the Kubernetes nodes contain a value in the ``podCIDR`` field:\n\n.. code-block:: shell-session\n\n    $ kubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.spec.podCIDR}{\"\\n\"}{end}'\n    gke-cluster4-default-pool-b195a3f3-k431\t10.4.0.0/24\n    gke-cluster4-default-pool-b195a3f3-zv3p\t10.4.1.0/24\n\nCheck the Cilium status\n=======================\n\nRun ``cilium status`` on the node in question and validate that the CIDR used\nfor IPAM matches the PodCIDR announced in the Kubernetes node:\n\n.. code-block:: shell-session\n\n    $ kubectl -n kube-system get pods -o wide | grep gke-cluster4-default-pool-b195a3f3-k431\n    cilium-lv4xd                       1/1     Running   0          3h8m   10.164.0.112   gke-cluster4-default-pool-b195a3f3-k431   <none>           <none>\n\n    $ kubectl -n kube-system exec -ti cilium-lv4xd -- cilium-dbg status\n    KVStore:                Disabled\n    Kubernetes:             Ok   1.14+ (v1.14.10-gke.27) [linux/amd64]\n    Kubernetes APIs:        [\"CustomResourceDefinition\", \"cilium/v2::CiliumClusterwideNetworkPolicy\", \"cilium/v2::CiliumEndpoint\", \"cilium/v2::CiliumNetworkPolicy\", \"cilium/v2::CiliumNode\", \"core/v1::Endpoint\", \"core/v1::Namespace\", \"core/v1::Pods\", \"core/v1::Service\", \"networking.k8s.io/v1::NetworkPolicy\"]\n    KubeProxyReplacement:   Probe   []\n    Cilium:                 Ok      OK\n    NodeMonitor:            Disabled\n    Cilium health daemon:   Ok\n    IPAM:                   IPv4: 7/255 allocated from 10.4.0.0/24,\n    Controller Status:      36/36 healthy\n    Proxy Status:           OK, ip 10.4.0.190, 0 redirects active on ports 10000-20000\n    Hubble:                 Disabled\n    Cluster health:         2/2 reachable   (2020-04-23T13:46:36Z)\n",
  "item_type": "unknown",
  "module_path": "/tmp/cilium-repo/Documentation/network/concepts/ipam/gke.rst",
  "extracted_at": "2025-09-03T01:13:29.217741Z"
}