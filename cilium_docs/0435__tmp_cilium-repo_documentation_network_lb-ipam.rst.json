{
  "url": "file:///tmp/cilium-repo/Documentation/network/lb-ipam.rst",
  "content": ".. only:: not (epub or latex or html)\n\n    WARNING: You are looking at unreleased Cilium documentation.\n    Please use the official rendered version released here:\n    https://docs.cilium.io\n\n.. _lb_ipam:\n\n********************************************\nLoadBalancer IP Address Management (LB IPAM)\n********************************************\n\nLB IPAM is a feature that allows Cilium to assign IP addresses to Services of\ntype ``LoadBalancer``. This functionality is usually left up to a cloud provider,\nhowever, when deploying in a private cloud environment, these facilities are not\nalways available.\n\nLB IPAM works in conjunction with features such as :ref:`bgp_control_plane` and :ref:`l2_announcements`. Where\nLB IPAM is responsible for allocation and assigning of IPs to Service objects and\nother features are responsible for load balancing and/or advertisement of these\nIPs. \n\nUse :ref:`bgp_control_plane` to advertise the IP addresses assigned by LB IPAM over BGP and :ref:`l2_announcements` to advertise them locally.\n\nLB IPAM is always enabled but dormant. The controller is awoken when the first\nIP Pool is added to the cluster.\n\n.. _lb_ipam_pools:\n\nPools\n#####\n\nLB IPAM has the notion of IP Pools which the administrator can create to tell \nCilium which IP ranges can be used to allocate IPs from.\n\nA basic IP Pools with both an IPv4 and IPv6 range looks like this:\n\n.. code-block:: yaml\n\n    apiVersion: \"cilium.io/v2\"\n    kind: CiliumLoadBalancerIPPool\n    metadata:\n      name: \"blue-pool\"\n    spec:\n      blocks:\n      - cidr: \"10.0.10.0/24\"\n      - cidr: \"2004::0/112\"\n      - start: \"20.0.20.100\"\n        stop: \"20.0.20.200\"\n\nAfter adding the pool to the cluster, it appears like so.\n\n.. code-block:: shell-session\n\n    $ kubectl get ippools                           \n    NAME        DISABLED   CONFLICTING   IPS AVAILABLE   AGE\n    blue-pool   false      False         65892           2s\n\n.. warning::\n\n  Updating an IP pool can result in IP addresses being reassigned and service IPs\n  could change. See :gh-issue:`40358`\n\n\nCIDRs, Ranges and reserved IPs\n------------------------------\n\nAn IP pool can have multiple blocks of IPs. A block can be specified with CIDR\nnotation (<prefix>/<bits>) or a range notation with a start and stop IP. As\npictured in :ref:`lb_ipam_pools`.\n\nWhen CIDRs are used to specify routable IP ranges, you might not want to allocate\nthe first and the last IP of a CIDR. Typically the first IP is the \n\"network address\" and the last IP is the \"broadcast address\". In some networks\nthese IPs are not usable and they do not always play well with all network \nequipment. By default, LB-IPAM uses all IPs in a given CIDR.\n\nIf you wish to reserve the first and last IPs of CIDRs, you can set the \n``.spec.allowFirstLastIPs`` field to ``No``.\n\nThis option is ignored for /32 and /31 IPv4 CIDRs and /128 and /127 IPv6 CIDRs \nsince these only have 1 or 2 IPs respectively.\n\nThis setting only applies to blocks specified with ``.spec.blocks[].cidr`` and not to\nblocks specified with ``.spec.blocks[].start`` and ``.spec.blocks[].stop``.\n\nService Selectors\n-----------------\n\nIP Pools have an optional ``.spec.serviceSelector`` field which allows administrators\nto limit which services can get IPs from which pools using a `label selector <https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/>`__.\nThe pool will allocate to any service if no service selector is specified.\n\n.. code-block:: yaml\n\n    apiVersion: \"cilium.io/v2\"\n    kind: CiliumLoadBalancerIPPool\n    metadata:\n      name: \"blue-pool\"\n    spec:\n      blocks:\n      - cidr: \"20.0.10.0/24\"\n      serviceSelector:\n        matchExpressions:\n          - {key: color, operator: In, values: [blue, cyan]}\n    ---\n    apiVersion: \"cilium.io/v2\"\n    kind: CiliumLoadBalancerIPPool\n    metadata:\n      name: \"red-pool\"\n    spec:\n      blocks:\n      - cidr: \"20.0.10.0/24\"\n      serviceSelector:\n        matchLabels:\n          color: red\n\nThere are a few special purpose selector fields which don't match on labels but\ninstead on other metadata like ``.meta.name`` or ``.meta.namespace``.\n\n=============================== ===================\nSelector                        Field\n------------------------------- -------------------\nio.kubernetes.service.namespace ``.meta.namespace``\nio.kubernetes.service.name      ``.meta.name``\n=============================== ===================\n\nFor example:\n\n.. code-block:: yaml\n\n    apiVersion: \"cilium.io/v2\"\n    kind: CiliumLoadBalancerIPPool\n    metadata:\n      name: \"blue-pool\"\n    spec:\n      blocks:\n      - cidr: \"20.0.10.0/24\"\n      serviceSelector:\n        matchLabels:\n          \"io.kubernetes.service.namespace\": \"tenant-a\"\n\nConflicts\n---------\n\nIP Pools are not allowed to have overlapping CIDRs. When an administrator does\ncreate pools which overlap, a soft error is caused. The last added pool will be\nmarked as ``Conflicting`` and no further allocation will happen from that pool.\nTherefore, administrators should always check the status of all pools after making\nmodifications.\n\nFor example, if we add 2 pools (``blue-pool`` and ``red-pool``) both with the same\nCIDR, we will see the following:\n\n.. code-block:: shell-session\n\n    $ kubectl get ippools\n    NAME        DISABLED   CONFLICTING   IPS AVAILABLE   AGE\n    blue-pool   false      False         254             25m\n    red-pool    false      True          254             11s\n\nThe reason for the conflict is stated in the status and can be accessed like so\n\n.. code-block:: shell-session\n\n    $ kubectl get ippools/red-pool -o jsonpath='{.status.conditions[?(@.type==\"cilium.io/PoolConflict\")].message}'\n    Pool conflicts since CIDR '20.0.10.0/24' overlaps CIDR '20.0.10.0/24' from IP Pool 'blue-pool'\n\nor\n\n.. code-block:: shell-session\n\n    $ kubectl describe ippools/red-pool\n    Name:         red-pool\n    #[...]\n    Status:\n      Conditions:\n        #[...]\n            Last Transition Time:  2022-10-25T14:09:05Z\n            Message:               Pool conflicts since CIDR '20.0.10.0/24' overlaps CIDR '20.0.10.0/24' from IP Pool 'blue-pool'\n            Observed Generation:   1\n            Reason:                cidr_overlap\n            Status:                True\n            Type:                  cilium.io/PoolConflict\n        #[...]\n\nDisabling a Pool\n-----------------\n\nIP Pools can be disabled. Disabling a pool will stop LB IPAM from allocating\nnew IPs from the pool, but doesn't remove existing allocations. This allows\nan administrator to slowly drain pool or reserve a pool for future use.\n\n.. code-block:: yaml\n\n    apiVersion: \"cilium.io/v2\"\n    kind: CiliumLoadBalancerIPPool\n    metadata:\n      name: \"blue-pool\"\n    spec:\n      blocks:\n      - cidr: \"20.0.10.0/24\"\n      disabled: true\n\n.. code-block:: shell-session\n\n    $ kubectl get ippools          \n    NAME        DISABLED   CONFLICTING   IPS AVAILABLE   AGE\n    blue-pool   true       False         254             41m\n\nStatus\n------\n\nThe IP Pool's status contains additional counts which can be used to monitor\nthe amount of used and available IPs. A machine parsable output can be obtained like so.\n\n.. code-block:: shell-session\n\n    $ kubectl get ippools -o jsonpath='{.items[*].status.conditions[?(@.type!=\"cilium.io/PoolConflict\")]}' | jq\n    {\n      \"lastTransitionTime\": \"2022-10-25T14:08:55Z\",\n      \"message\": \"254\",\n      \"observedGeneration\": 1,\n      \"reason\": \"noreason\",\n      \"status\": \"Unknown\",\n      \"type\": \"cilium.io/IPsTotal\"\n    }\n    {\n      \"lastTransitionTime\": \"2022-10-25T14:08:55Z\",\n      \"message\": \"254\",\n      \"observedGeneration\": 1,\n      \"reason\": \"noreason\",\n      \"status\": \"Unknown\",\n      \"type\": \"cilium.io/IPsAvailable\"\n    }\n    {\n      \"lastTransitionTime\": \"2022-10-25T14:08:55Z\",\n      \"message\": \"0\",\n      \"observedGeneration\": 1,\n      \"reason\": \"noreason\",\n      \"status\": \"Unknown\",\n      \"type\": \"cilium.io/IPsUsed\"\n    }\n\nOr human readable output like so\n\n.. code-block:: shell-session\n\n    $ kubectl describe ippools/blue-pool\n    Name:         blue-pool\n    Namespace:    \n    Labels:       <none>\n    Annotations:  <none>\n    API Version:  cilium.io/v2\n    Kind:         CiliumLoadBalancerIPPool\n    #[...]\n    Status:\n      Conditions:\n        #[...]\n        Last Transition Time:  2022-10-25T14:08:55Z\n        Message:               254\n        Observed Generation:   1\n        Reason:                noreason\n        Status:                Unknown\n        Type:                  cilium.io/IPsTotal\n        Last Transition Time:  2022-10-25T14:08:55Z\n        Message:               254\n        Observed Generation:   1\n        Reason:                noreason\n        Status:                Unknown\n        Type:                  cilium.io/IPsAvailable\n        Last Transition Time:  2022-10-25T14:08:55Z\n        Message:               0\n        Observed Generation:   1\n        Reason:                noreason\n        Status:                Unknown\n        Type:                  cilium.io/IPsUsed\n\nServices\n########\n\nAny service with ``.spec.type=LoadBalancer`` can get IPs from any pool as long\nas the IP Pool's service selector matches the service.\n\nLets say we add a simple service.\n\n.. code-block:: yaml\n\n    apiVersion: v1\n    kind: Service\n    metadata:\n      name: service-red\n      namespace: example\n      labels:\n        color: red\n    spec:\n      type: LoadBalancer\n      ports:\n      - port: 1234\n\nThis service will appear like so.\n\n.. code-block:: shell-session\n\n    $ kubectl -n example get svc\n    NAME          TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE\n    service-red   LoadBalancer   10.96.192.212   <pending>     1234:30628/TCP   24s\n\nThe ExternalIP field has a value of ``<pending>`` which means no LB IPs have been assigned.\nWhen LB IPAM is unable to allocate or assign IPs for the service, it will update the service\nconditions in the status.\n\nThe service conditions can be checked like so:\n\n.. code-block:: shell-session\n\n    $ kubectl -n example get svc/service-red -o jsonpath='{.status.conditions}' | jq\n    [\n      {\n        \"lastTransitionTime\": \"2022-10-06T13:40:48Z\",\n        \"message\": \"There are no enabled CiliumLoadBalancerIPPools that match this service\",\n        \"reason\": \"no_pool\",\n        \"status\": \"False\",\n        \"type\": \"io.cilium/lb-ipam-request-satisfied\"\n      }\n    ]\n\nAfter updating the service labels to match our ``blue-pool`` from before we see:\n\n.. code-block:: shell-session\n\n    $ kubectl -n example get svc\n    NAME          TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE\n    service-red   LoadBalancer   10.96.192.212   20.0.10.163   1234:30628/TCP   12m\n\n    $ kubectl -n example get svc/service-red -o jsonpath='{.status.conditions}' | jq\n    [\n      {\n        \"lastTransitionTime\": \"2022-10-06T13:40:48Z\",\n        \"message\": \"There are no enabled CiliumLoadBalancerIPPools that match this service\",\n        \"reason\": \"no_pool\",\n        \"status\": \"False\",\n        \"type\": \"io.cilium/lb-ipam-request-satisfied\"\n      },\n      {\n        \"lastTransitionTime\": \"2022-10-06T13:52:55Z\",\n        \"message\": \"\",\n        \"reason\": \"satisfied\",\n        \"status\": \"True\",\n        \"type\": \"io.cilium/lb-ipam-request-satisfied\"\n      }\n    ]\n\nIPv4 / IPv6 families + policy\n-----------------------------\n\nLB IPAM supports IPv4 and/or IPv6 in SingleStack or `DualStack <https://kubernetes.io/docs/concepts/services-networking/dual-stack/>`__ mode. \nServices can use the ``.spec.ipFamilyPolicy`` and ``.spec.ipFamilies`` fields to change\nthe requested IPs.\n\nIf ``.spec.ipFamilyPolicy`` isn't specified, ``SingleStack`` mode is assumed. \nIf both IPv4 and IPv6 are enabled in ``SingleStack`` mode, an IPv4 address is allocated.\n\nIf ``.spec.ipFamilyPolicy`` is set to ``PreferDualStack``, LB IPAM will attempt to allocate \nboth an IPv4 and IPv6 address if both are enabled on the cluster. If only IPv4 or only IPv6 is\nenabled on the cluster, the service is still considered \"satisfied\".\n\nIf ``.spec.ipFamilyPolicy`` is set to ``RequireDualStack`` LB IPAM will attempt to allocate\nboth an IPv4 and IPv6 address. The service is considered \"unsatisfied\" If IPv4 \nor IPv6 is disabled on the cluster.\n\nThe order of ``.spec.ipFamilies`` has no effect on LB IPAM but is significant for cluster IP\nallocation which isn't handled by LB IPAM.\n\nLoadBalancerClass\n-----------------\n\nKubernetes >= v1.24 supports `multiple load balancers <https://kubernetes.io/docs/concepts/services-networking/service/#load-balancer-class>`_ \nin the same cluster. Picking between load balancers is done with the ``.spec.loadBalancerClass`` field. \nWhen LB IPAM is enabled it allocates and assigns IPs for services with \nno load balancer class set.\n\nLB IPAM only does IP allocation and doesn't provide load balancing services by itself. Therefore,\nusers should pick one of the following Cilium load balancer classes, all of which use LB IPAM\nfor allocation (if the feature is enabled):\n\n=============================== ========================\nloadBalancerClass               Feature\n------------------------------- ------------------------\n``io.cilium/bgp-control-plane`` :ref:`bgp_control_plane`\n------------------------------- ------------------------\n``io.cilium/l2-announcer``      :ref:`l2_announcements`\n=============================== ========================\n\nIf the ``.spec.loadBalancerClass`` is set to a class which isn't handled by Cilium's LB IPAM, \nthen Cilium's LB IPAM will ignore the service entirely, not even setting a condition in the status. \n\nBy default, if the ``.spec.loadBalancerClass`` field is not set, Cilium's LB IPAM will assume it can \nallocate IPs for the service from its configured pools. If this isn't the desired behavior, you can \nconfigure LB-IPAM to only allocate IPs for services from its configured pools when it has a recognized \nload balancer class by setting the following configuration in the Helm chart or ConfigMap:\n\n.. tabs::\n    .. group-tab:: Helm\n\n        .. parsed-literal::\n\n            $ helm upgrade cilium |CHART_RELEASE| \\\\\n               --namespace kube-system \\\\\n               --reuse-values \\\\\n               --set defaultLBServiceIPAM=none\n\n    .. group-tab:: ConfigMap\n\n        .. code-block:: yaml\n\n            default-lb-service-ipam: none\n\nRequesting IPs\n--------------\n\nServices can request specific IPs. The legacy way of doing so is via ``.spec.loadBalancerIP``\nwhich takes a single IP address. This method has been deprecated in k8s v1.24 but is supported\nuntil its future removal.\n\nThe new way of requesting specific IPs is to use annotations, ``lbipam.cilium.io/ips`` in the case\nof Cilium LB IPAM. This annotation takes a comma-separated list of IP addresses, allowing for\nmultiple IPs to be requested at once.\n\nThe service selector of the IP Pool still applies, requested IPs will not be allocated or assigned\nif the services don't match the pool's selector.\n\nDon't configure the annotation to request the first or last IP of an IP pool. They are reserved \nfor the network and broadcast addresses respectively.\n\n.. code-block:: yaml\n\n    apiVersion: v1\n    kind: Service\n    metadata:\n      name: service-blue\n      namespace: example\n      labels:\n        color: blue\n      annotations:\n        \"lbipam.cilium.io/ips\": \"20.0.10.100,20.0.10.200\"\n    spec:\n      type: LoadBalancer\n      ports:\n      - port: 1234\n\n.. code-block:: shell-session\n\n    $ kubectl -n example get svc                \n    NAME           TYPE           CLUSTER-IP     EXTERNAL-IP               PORT(S)          AGE\n    service-blue   LoadBalancer   10.96.26.105   20.0.10.100,20.0.10.200   1234:30363/TCP   43s\n\nSharing Keys\n------------\n\nServices can share the same IP or set of IPs with other services. This is done by setting the ``lbipam.cilium.io/sharing-key`` annotation on the service.\nServices that have the same sharing key annotation will share the same IP or set of IPs. The sharing key is a string that can be any value.\n\n.. code-block:: yaml\n\n  apiVersion: v1\n  kind: Service\n  metadata:\n    name: service-blue\n    namespace: example\n    labels:\n      color: blue\n    annotations:\n      \"lbipam.cilium.io/sharing-key\": \"1234\"\n  spec:\n    type: LoadBalancer\n    ports:\n    - port: 1234\n  ---\n  apiVersion: v1\n  kind: Service\n  metadata:\n    name: service-red\n    namespace: example\n    labels:\n      color: red\n    annotations:\n      \"lbipam.cilium.io/sharing-key\": \"1234\"\n  spec:\n    type: LoadBalancer\n    ports:\n    - port: 2345\n\n.. code-block:: shell-session\n\n  $ kubectl -n example get svc\n  NAME           TYPE           CLUSTER-IP     EXTERNAL-IP               PORT(S)          AGE\n  service-blue   LoadBalancer   10.96.26.105   20.0.10.100               1234:30363/TCP   43s\n  service-red    LoadBalancer   10.96.26.106   20.0.10.100               2345:30131/TCP   43s\n\nAs long as the services do not have conflicting ports, they will be allocated the same IP. If the services have conflicting ports, they will be allocated different IPs, which will be added to the set of IPs belonging to the sharing key.\nIf a service has a sharing key and also requests a specific IP, the service will be allocated the requested IP and it will be added to the set of IPs belonging to that sharing key.\n\nBy default, sharing IPs across namespaces is not allowed. To allow sharing across a namespace, set the ``lbipam.cilium.io/sharing-cross-namespace`` annotation to the namespaces the service can be shared with. The value must be a comma-separated list of namespaces. The annotation must be present on both services. You can allow all namespaces with ``*``.\n",
  "item_type": "unknown",
  "module_path": "/tmp/cilium-repo/Documentation/network/lb-ipam.rst",
  "extracted_at": "2025-09-03T01:13:29.138182Z"
}