{
  "url": "file:///tmp/cilium-repo/Documentation/network/clustermesh/clustermesh.rst",
  "content": ".. _clustermesh:\n.. _gs_clustermesh:\n\n***********************\nSetting up Cluster Mesh\n***********************\n\nThis is a step-by-step guide on how to build a mesh of Kubernetes clusters by\nconnecting them together, enable pod-to-pod connectivity across all clusters,\ndefine global services to load-balance between clusters and enforce security\npolicies to restrict access.\n\n.. admonition:: Video\n  :class: attention\n\n  Aside from this step-by-step guide, if you would like to watch how Cilium's\n  Clustermesh feature works, check out `eCHO Episode 41: Cilium Clustermesh <https://www.youtube.com/watch?v=VBOONHW65NU&t=342s>`__.\n\nPrerequisites\n#############\n\nCluster Addressing Requirements\n===============================\n\n* All clusters must be configured with the same datapath mode. Cilium install\n  may default to :ref:`arch_overlay` or :ref:`native_routing` mode depending on\n  the specific cloud environment.\n\n* PodCIDR ranges in all clusters and all nodes must be non-conflicting and\n  unique IP addresses.\n\n* Nodes in all clusters must have IP connectivity between each other using the \n  configured InternalIP for each node. This requirement is typically met by establishing \n  peering or VPN tunnels between the networks of the nodes of each cluster.\n\n* The network between clusters must allow the inter-cluster communication. The\n  exact ports are documented in the :ref:`firewall_requirements` section.\n\n.. note::\n  \n  For cloud-specific deployments, you can check out the :ref:`gs_clustermesh_aks_prep`\n  guide for Azure Kubernetes Service (AKS), the :ref:`gs_clustermesh_eks_prep`\n  guide for Amazon Elastic Kubernetes Service (EKS) or the :ref:`gs_clustermesh_gke_prep` \n  guide for Google Kubernetes Engine (GKE) clusters for instructions on\n  how to meet the above requirements.\n\nAdditional Requirements for Native-routed Datapath Modes\n--------------------------------------------------------\n\n* Cilium in each cluster must be configured with a native routing CIDR that\n  covers all the PodCIDR ranges across all connected clusters. Cluster CIDRs are\n  typically allocated from the ``10.0.0.0/8`` private address space. When this\n  is the case a native routing CIDR such as ``10.0.0.0/8`` should cover all\n  clusters:\n\n * ConfigMap option ``ipv4-native-routing-cidr=10.0.0.0/8``\n * Helm option ``--set ipv4NativeRoutingCIDR=10.0.0.0/8``\n * ``cilium install`` option ``--set ipv4NativeRoutingCIDR=10.0.0.0/8``\n\n* In addition to nodes, pods in all clusters must have IP connectivity between each other. This\n  requirement is typically met by establishing peering or VPN tunnels between\n  the networks of the nodes of each cluster\n\n* The network between clusters must allow pod-to-pod inter-cluster communication\n  across any ports that the pods may use. This is typically accomplished with\n  firewall rules allowing pods in different clusters to reach each other on all\n  ports.\n\nScaling Limitations\n=============================\n\n* By default, the maximum number of clusters that can be connected together using Cluster Mesh is\n  255. By using the option ``maxConnectedClusters`` this limit can be set to 511, at the expense of\n  lowering the maximum number of cluster-local identities. Reference the following table for valid\n  configurations and their corresponding cluster-local identity limits:\n\n+------------------------+------------+----------+----------+\n| MaxConnectedClusters   | Maximum cluster-local identities |\n+========================+============+==========+==========+\n| 255 (default)          | 65535                            |\n+------------------------+------------+----------+----------+\n| 511                    | 32767                            |\n+------------------------+------------+----------+----------+\n\n* All clusters across a Cluster Mesh must be configured with the same ``maxConnectedClusters``\n  value.\n\n * ConfigMap option ``max-connected-clusters=511``\n * Helm option ``--set clustermesh.maxConnectedClusters=511``\n * ``cilium install`` option ``--set clustermesh.maxConnectedClusters=511``\n\n.. note::\n\n   This option controls the bit allocation of numeric identities and will affect the maximum number\n   of cluster-local identities that can be allocated. By default, cluster-local\n   :ref:`security_identities` are limited to 65535, regardless of whether Cluster Mesh is used or\n   not.\n\n.. warning::\n  ``MaxConnectedClusters`` can only be set once during Cilium installation and should not be\n  changed for existing clusters. Changing this option on a live cluster may result in connection\n  disruption and possible incorrect enforcement of network policies\n\nInstall the Cilium CLI\n======================\n\n.. include:: ../../installation/cli-download.rst\n\n.. warning::\n\n  Don't use the Cilium CLI *helm* mode to enable Cluster Mesh or connect clusters\n  configured using the Cilium CLI operating in *classic* mode, as the two modes are\n  not compatible with each other.\n\nPrepare the Clusters\n####################\n\nFor the rest of this tutorial, we will assume that you intend to connect two\nclusters together with the kubectl configuration context stored in the\nenvironment variables ``$CLUSTER1`` and ``$CLUSTER2``. This context name is the\nsame as you typically pass to ``kubectl --context``.\n\nSpecify the Cluster Name and ID\n===============================\n\nCilium needs to be installed onto each cluster.\n\nEach cluster must be assigned a unique human-readable name as well as a numeric\ncluster ID (1-255). The cluster name must respect the following constraints:\n\n* It must contain at most 32 characters;\n* It must begin and end with a lower case alphanumeric character;\n* It may contain lower case alphanumeric characters and dashes between.\n\nIt is best to assign both the cluster name and the cluster ID at installation time:\n\n * ConfigMap options ``cluster-name`` and ``cluster-id``\n * Helm options ``cluster.name`` and ``cluster.id``\n * Cilium CLI install options ``--set cluster.name`` and ``--set cluster.id``\n\nReview :ref:`k8s_install_quick` for more details and use cases.\n\nExample install using the Cilium CLI:\n\n.. code-block:: shell-session\n\n  cilium install --set cluster.name=$CLUSTER1 --set cluster.id=1 --context $CLUSTER1\n  cilium install --set cluster.name=$CLUSTER2 --set cluster.id=2 --context $CLUSTER2\n\n.. important::\n\n   If you change the cluster ID and/or cluster name in a cluster with running\n   workloads, you will need to restart all workloads. The cluster ID is used to\n   generate the security identity and it will need to be re-created in order to\n   establish access across clusters.\n\nShared Certificate Authority\n============================\n\nIf you are planning to run Hubble Relay across clusters, it is best to share a\ncertificate authority (CA) between the clusters as it will enable mTLS across\nclusters to just work.\n\nYou can propagate the CA copying the Kubernetes secret containing the CA\nfrom one cluster to another:\n\n.. code-block:: shell-session\n\n  kubectl --context=$CLUSTER1 get secret -n kube-system cilium-ca -o yaml | \\\n    kubectl --context $CLUSTER2 create -f -\n\n.. _enable_clustermesh:\n\nEnable Cluster Mesh\n===================\n\nEnable all required components by running ``cilium clustermesh enable`` in the\ncontext of both clusters. This will deploy the ``clustermesh-apiserver`` into\nthe cluster and generate all required certificates and import them as\nKubernetes secrets. It will also attempt to auto-detect the best service type\nfor the LoadBalancer to expose the Cluster Mesh control plane to other\nclusters.\n\n.. code-block:: shell-session\n\n   cilium clustermesh enable --context $CLUSTER1\n   cilium clustermesh enable --context $CLUSTER2\n\n.. note::\n\n   Starting from v1.16 KVStoreMesh is enabled by default.\n   You can opt out of :ref:`kvstoremesh` when enabling the Cluster Mesh.\n\n   .. code-block:: shell-session\n\n     cilium clustermesh enable --context $CLUSTER1 --enable-kvstoremesh=false\n     cilium clustermesh enable --context $CLUSTER2 --enable-kvstoremesh=false\n\n.. important::\n\n   In some cases, the service type cannot be automatically detected and you need to specify it manually. This\n   can be done with the option ``--service-type``. The possible values are:\n\n   LoadBalancer:\n     A Kubernetes service of type LoadBalancer is used to expose the control\n     plane. This uses a stable LoadBalancer IP and is typically the best option. \n\n   NodePort:\n     A Kubernetes service of type NodePort is used to expose the control plane.\n     This requires stable Node IPs. If a node disappears, the Cluster Mesh may\n     have to reconnect to a different node. If all nodes have become\n     unavailable, you may have to re-connect the clusters to extract new node\n     IPs.\n\n   ClusterIP:\n     A Kubernetes service of type ClusterIP is used to expose the control\n     plane. This requires the ClusterIPs are routable between clusters.\n\nWait for the Cluster Mesh components to come up by invoking ``cilium\nclustermesh status --wait``. If you are using a service of type LoadBalancer\nthen this will also wait for the LoadBalancer to be assigned an IP.\n\n.. code-block:: shell-session\n\n   cilium clustermesh status --context $CLUSTER1 --wait\n   cilium clustermesh status --context $CLUSTER2 --wait\n\n.. code-block:: shell-session\n\n    âœ… Cluster access information is available:\n      - 10.168.0.89:2379\n    âœ… Service \"clustermesh-apiserver\" of type \"LoadBalancer\" found\n    ðŸ”Œ Cluster Connections:\n    ðŸ”€ Global services: [ min:0 / avg:0.0 / max:0 ]\n\n\nConnect Clusters\n================\n\nFinally, connect the clusters. This step only needs to be done in one\ndirection. The connection will automatically be established in both directions:\n\n.. code-block:: shell-session\n\n    cilium clustermesh connect --context $CLUSTER1 --destination-context $CLUSTER2\n\nIt may take a bit for the clusters to be connected. You can run ``cilium\nclustermesh status --wait`` to wait for the connection to be successful:\n\n.. code-block:: shell-session\n\n   cilium clustermesh status --context $CLUSTER1 --wait\n\nThe output will look something like this:\n\n.. code-block:: shell-session\n\n    âœ… Cluster access information is available:\n      - 10.168.0.89:2379\n    âœ… Service \"clustermesh-apiserver\" of type \"LoadBalancer\" found\n    âŒ› Waiting (12s) for clusters to be connected: 2 nodes are not ready\n    âŒ› Waiting (25s) for clusters to be connected: 2 nodes are not ready\n    âŒ› Waiting (38s) for clusters to be connected: 2 nodes are not ready\n    âŒ› Waiting (51s) for clusters to be connected: 2 nodes are not ready\n    âŒ› Waiting (1m4s) for clusters to be connected: 2 nodes are not ready\n    âŒ› Waiting (1m17s) for clusters to be connected: 1 nodes are not ready\n    âœ… All 2 nodes are connected to all clusters [min:1 / avg:1.0 / max:1]\n    ðŸ”Œ Cluster Connections:\n    - cilium-cli-ci-multicluster-2-168: 2/2 configured, 2/2 connected\n    ðŸ”€ Global services: [ min:6 / avg:6.0 / max:6 ]\n\nIf this step does not complete successfully, proceed to the troubleshooting\nsection.\n\nTest Pod Connectivity Between Clusters\n======================================\n\nCongratulations, you have successfully connected your clusters together. You\ncan validate the connectivity by running the connectivity test in multi cluster\nmode:\n\n.. code-block:: shell-session\n\n   cilium connectivity test --context $CLUSTER1 --multi-cluster $CLUSTER2\n\nNext Steps\n==========\n\nLogical next steps to explore from here are:\n\n * :ref:`gs_clustermesh_services`\n * :ref:`gs_clustermesh_network_policy`\n\nTroubleshooting\n###############\n\nUse the following list of steps to troubleshoot issues with ClusterMesh:\n\n #. Validate that Cilium pods are healthy and ready:\n\n    .. code-block:: shell-session\n\n       cilium status --context $CLUSTER1\n       cilium status --context $CLUSTER2\n\n #. Validate that Cluster Mesh is enabled and operational:\n\n    .. code-block:: shell-session\n\n       cilium clustermesh status --context $CLUSTER1\n       cilium clustermesh status --context $CLUSTER2\n\nIf you cannot resolve the issue with the above commands, see the\n:ref:`troubleshooting_clustermesh` for a more detailed troubleshooting guide.\n",
  "item_type": "unknown",
  "module_path": "/tmp/cilium-repo/Documentation/network/clustermesh/clustermesh.rst",
  "extracted_at": "2025-09-03T01:13:29.196375Z"
}