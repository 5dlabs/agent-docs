{
  "url": "file:///tmp/cilium-repo/Documentation/network/concepts/ipam/cluster-pool.rst",
  "content": ".. only:: not (epub or latex or html)\n\n    WARNING: You are looking at unreleased Cilium documentation.\n    Please use the official rendered version released here:\n    http://docs.cilium.io\n\n.. _ipam_crd_cluster_pool:\n\n#######################\nCluster Scope (Default)\n#######################\n\nThe cluster-scope IPAM mode assigns per-node PodCIDRs to each node and\nallocates IPs using a host-scope allocator on each node. It is thus similar to\nthe :ref:`k8s_hostscope` mode. The difference is that instead of Kubernetes\nassigning the per-node PodCIDRs via the Kubernetes ``v1.Node`` resource, the\nCilium operator will manage the per-node PodCIDRs via the ``v2.CiliumNode``\nresource. The advantage of this mode is that it does not depend on Kubernetes\nbeing configured to hand out per-node PodCIDRs.\n\n************\nArchitecture\n************\n\n.. image:: cluster_pool.png\n    :align: center\n\nThis is useful if Kubernetes cannot be configured to hand out PodCIDRs or if\nmore control is needed.\n\nIn this mode, the Cilium agent will wait on startup until the ``podCIDRs`` range\nare made available via the Cilium Node ``v2.CiliumNode`` object for all enabled\naddress families via the resource field set in the ``v2.CiliumNode``:\n\n====================== ==============================\nField                  Description\n====================== ==============================\n``spec.ipam.podCIDRs`` IPv4 and/or IPv6 PodCIDR range\n====================== ==============================\n\n*************\nConfiguration\n*************\n\nFor a practical tutorial on how to enable this mode in Cilium, see\n:ref:`gsg_ipam_crd_cluster_pool`.\n\nExpanding the cluster pool\n==========================\n\nDon't change any existing elements of the ``clusterPoolIPv4PodCIDRList`` list, as\nchanges cause unexpected behavior. If the pool is exhausted,\nadd a new element to the list instead. The minimum mask length is ``/30``, with a recommended minimum mask \nlength of at least ``/29``. The reason to add new elements rather than change existing elements is that\nthe allocator reserves 2 IPs per CIDR block for the network and broadcast addresses.\nChanging ``clusterPoolIPv4MaskSize`` is also not possible. \n\n***************\nTroubleshooting\n***************\n\nLook for allocation errors\n==========================\n\nCheck the ``Error`` field in the ``status.ipam.operator-status`` field:\n\n.. code-block:: shell-session\n\n    kubectl get ciliumnodes -o jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.status.ipam.operator-status}{\"\\n\"}{end}'\n    \nCheck for conflicting node CIDRs\n================================\n\n``10.0.0.0/8`` is the default pod CIDR. If your node network is in the same range\nyou will lose connectivity to other nodes. All egress traffic will be assumed\nto target pods on a given node rather than other nodes.\n\nYou can solve it in two ways:\n\n  - Explicitly set ``clusterPoolIPv4PodCIDRList`` to a non-conflicting CIDR\n  - Use a different CIDR for your nodes\n",
  "item_type": "unknown",
  "module_path": "/tmp/cilium-repo/Documentation/network/concepts/ipam/cluster-pool.rst",
  "extracted_at": "2025-09-03T01:13:29.217044Z"
}