# Product Requirements Document: Doc Server

## Executive Summary

The Doc Server is a comprehensive documentation platform that provides AI assistants with semantic search capabilities across diverse technical documentation sources. Originally designed for Rust crate documentation, it has evolved into a multi-type documentation server supporting infrastructure tools, blockchain platforms, and programming resources.

## Current State (As of Handoff)

### üîÑ Infrastructure Status (Requires Rebuild)
- **Database**: `docs` database with harmonized schema (needs validation and migration)
- **Content**: 4,000+ documents with embeddings across 3 documentation types (needs verification)
- **Architecture**: PostgreSQL with pgvector, Rust MCP server (requires Streamable HTTP transport migration)
- **Working Tools**: `rust_query` tool exists but needs validation with new transport
- **Production Environment**: Kubernetes cluster deployment required

### üìã Current Data Status
- **Rust Crates**: 40+ crates with documentation and embeddings (needs production validation)
- **BirdEye API**: 600+ endpoints with OpenAPI specifications (needs production deployment)
- **Solana Documentation**: 400+ documents including core docs, architecture diagrams, ZK cryptography PDFs (needs cluster validation)

### üîß Current Implementation Status
- **MCP Server**: Streamable HTTP server (migration to JSON-only MVP in progress)
- **Database Schema**: Unified tables supporting multiple documentation types (needs cluster deployment)
- **Production Database**: PostgreSQL with pgvector extension in dedicated cluster (ready for connection)
- **Data Migration**: 184MB database with 4,000+ documents ready for cluster deployment

## Target State

### Core Functionality
A unified documentation server providing:
- **Query Tools**: 10 type-specific query tools for precise documentation access
- **Management Tools**: Dynamic Rust crate management via MCP
- **Connection Reliability**: Streamable HTTP transport for stable Toolman integration  
- **Cost Efficiency**: 70% embedding cost reduction via OpenAI batching
- **Developer Experience**: Streamlined Kubernetes deployment and validation workflow

### Supported Documentation Types
1. üîÑ **Rust Crates** (needs production validation) - Dynamic management and querying
2. üîÑ **BirdEye API** (needs cluster deployment) - Blockchain API documentation
3. üîÑ **Solana** (needs cluster deployment) - Blockchain platform documentation
4. üìã **Jupyter Notebooks** (planned) - Interactive notebook documentation
5. üìã **Cilium** (planned) - Networking and security documentation
6. üìã **Talos** (planned) - Linux distribution documentation
7. üìã **Meteora** (planned) - DEX protocol documentation
8. üìã **Raydium** (planned) - DEX protocol documentation
9. üìã **eBPF** (planned) - Extended Berkeley Packet Filter documentation
10. üìã **Rust Best Practices** (planned) - Language best practices guide

## Functional Requirements

### 1. Query Tools (Primary MCP Interface)

Each documentation type must have its own specific query tool for optimal relevance:

#### üîÑ Requires Production Validation
- `rust_query` - Query Rust crate documentation with semantic search (needs Streamable HTTP migration)

#### üìã Planned Implementation  
- `birdeye_query` - Query BirdEye blockchain API documentation
- `solana_query` - Query Solana blockchain platform documentation
- `jupyter_query` - Query Jupyter notebook documentation
- `cilium_query` - Query Cilium networking/security documentation
- `talos_query` - Query Talos Linux documentation
- `meteora_query` - Query Meteora DEX documentation
- `raydium_query` - Query Raydium DEX documentation
- `ebpf_query` - Query eBPF documentation
- `rust_best_practices_query` - Query Rust best practices guide

#### Tool Requirements
- **Type-specific naming**: Clear indication of available documentation (no generic names)
- **Semantic search**: Vector similarity search using OpenAI text-embedding-3-large (3072 dimensions)
- **Metadata filtering**: JSONB metadata filtering for refined results and document categorization
- **Response formatting**: Structured responses with source attribution and relevance scoring
- **Performance targets**: Query response time < 2 seconds despite lack of vector indexing
- **Content handling**: Support for markdown, PDF, BOB diagrams, MSC sequence charts

#### Tool Architecture Details

Each query tool follows a standardized architecture pattern:

```rust
// Tool Implementation Pattern
struct DocumentTypeQueryTool {
    db_pool: SqlxPool,
    embeddings_client: OpenAIClient,
    doc_type: DocumentType,
}

impl Tool for DocumentTypeQueryTool {
    async fn execute(&self, query: String) -> ToolResult {
        // 1. Generate query embedding (3072 dimensions)
        // 2. Perform vector similarity search
        // 3. Apply metadata filters for doc_type
        // 4. Rank results by relevance score
        // 5. Format response with source attribution
    }
}
```

**Tool Naming Convention (CRITICAL)**:
- ‚úÖ **Correct**: `solana_query`, `birdeye_query`, `cilium_query`, `rust_query`
- ‚ùå **Incorrect**: `blockchain_query`, `api_query`, `docs_query`, `search_tool`

**Metadata Structure Per Documentation Type**:

**Rust Crates**:
```json
{
  "version": "1.0.0",
  "features": ["full", "macros"],
  "crate_type": "library",
  "docs_url": "https://docs.rs/crate/version",
  "module_path": "src/lib.rs",
  "item_type": "struct|function|trait|enum"
}
```

**BirdEye API**:
```json
{
  "api_version": "v1",
  "endpoint": "/defi/price",
  "method": "GET|POST|PUT|DELETE",
  "parameters": {"address": "string", "chain": "solana"},
  "response_schema": {"price": "number", "timestamp": "unix"},
  "category": "defi|nft|analytics"
}
```

**Solana Documentation**:
```json
{
  "category": "core|architecture|crypto|consensus",
  "format": "markdown|pdf|bob|msc",
  "section": "consensus|networking|validators|runtime",
  "complexity": "beginner|intermediate|advanced",
  "topic": "proof_of_stake|gossip|turbine|accounts"
}
```

### 2. Management Tools (Rust Only)

Only Rust crates support dynamic management via MCP:
- `add_rust_crate` - Add new Rust crate with automatic documentation extraction
- `remove_rust_crate` - Remove Rust crate and associated documentation
- `list_rust_crates` - List available crates with status information
- `check_rust_status` - Check population status and health of Rust documentation

### 3. Connection Reliability (Priority)

#### Current State
- Basic HTTP/SSE server implementation (using deprecated transport)
- Health check endpoint functional
- Cursor MCP integration working

#### Required Implementation - Streamable HTTP Transport (MCP 2025-06-18)
- **Single MCP Endpoint (MVP)**: POST only (GET returns 405)
- **Session Management**: Implement Mcp-Session-Id header support for stateful sessions
- **Protocol Version Headers**: MCP-Protocol-Version header compliance
- **Resumable Connections**: SSE event IDs for connection resumability and message redelivery
- **Security Requirements**: Origin header validation, localhost binding, proper authentication
- **Health Monitoring**: Enhanced health endpoints for system monitoring

### 4. Data Management

#### Current Implementation Status
- üîÑ Harmonized schema supporting multiple documentation types (needs cluster deployment)
- üîÑ JSONB metadata for type-specific information (needs production validation)
- üîÑ OpenAI text-embedding-3-large embeddings (3072 dimensions) (needs cluster connection)
- üîÑ Migration from original Rust-only schema (needs production verification)

#### Required Enhancements
- **Batch Processing**: OpenAI API batching for cost optimization
- **Rate Limiting**: Compliance with OpenAI rate limits (3000 RPM / 1M TPM)
- **Vector Indexing**: Optimize search performance as data grows
- **Cross-Type Search**: Unified search across all documentation types

## Deployment and Validation Process

### Mandatory Validation Workflow

**CRITICAL REQUIREMENT**: Every feature implementation and task completion **MUST** follow this exact validation process to reach acceptance criteria. No exceptions.

#### Step 1: Code Integration
- **Action**: Push feature branch to GitHub repository
- **Requirement**: All code changes, configuration updates, and documentation must be committed
- **Validation**: GitHub Actions triggers must execute successfully
- **Deliverable**: Feature branch ready for build pipeline

#### Step 2: Container Build
- **Action**: Wait for automated container image build to complete
- **Requirement**: Container image build must succeed without errors
- **Validation**: Container image tagged and available in container registry
- **Deliverable**: Deployable container image with proper versioning

#### Step 3: Kubernetes Deployment
- **Action**: Deploy the new image to production Kubernetes cluster
- **Requirement**: All Kubernetes manifests (deployments, services, ingress) must apply successfully
- **Validation**: Pod health checks pass, service endpoints respond
- **Environment**: Production cluster with dedicated PostgreSQL + pgvector
- **Deliverable**: Running service accessible via cluster networking

#### Step 4: Real-World Testing
- **Action**: Execute comprehensive testing against live deployment
- **Requirement**: All acceptance criteria must be validated in production environment
- **Validation**: 
  - Tool functionality tested via Cursor MCP integration
  - Query performance benchmarked against production data
  - Connection stability validated under realistic load
  - Database queries executed against live PostgreSQL cluster
- **Deliverable**: Test results demonstrating acceptance criteria fulfillment

#### Step 5: Acceptance Validation
- **Action**: Document validation results and mark task complete
- **Requirement**: All acceptance criteria met in production environment
- **Validation**: Stakeholder approval on real-world functionality
- **Deliverable**: Completed task with production validation evidence

### Infrastructure Requirements

#### Production Environment
- **Kubernetes Cluster**: Production-grade cluster with proper resource allocation
- **PostgreSQL Database**: Dedicated PostgreSQL instance with pgvector extension
- **Container Registry**: Automated image building and registry management
- **Monitoring**: Production monitoring and alerting for all deployments
- **Load Balancing**: Ingress controllers and service mesh for traffic management

#### Development Workflow
- **Source Control**: Git-based workflow with feature branches
- **CI/CD Pipeline**: GitHub Actions for automated building and testing
- **Infrastructure as Code**: Kubernetes manifests versioned with application code
- **Observability**: Structured logging and metrics collection in production

## Technical Requirements

### Database Schema (Implemented)

```sql
-- Primary documents table (implemented)
CREATE TABLE documents (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    doc_type VARCHAR(50) NOT NULL CHECK (doc_type IN (
        'rust', 'jupyter', 'birdeye', 'cilium', 'talos', 
        'meteora', 'solana', 'ebpf', 'raydium', 'rust_best_practices'
    )),
    source_name VARCHAR(255) NOT NULL,
    doc_path TEXT NOT NULL,
    content TEXT NOT NULL,
    metadata JSONB DEFAULT '{}',
    embedding vector(3072), -- OpenAI text-embedding-3-large
    token_count INTEGER,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(doc_type, source_name, doc_path)
);

-- Source configuration table (implemented)
CREATE TABLE document_sources (
    id SERIAL PRIMARY KEY,
    doc_type VARCHAR(50) NOT NULL,
    source_name VARCHAR(255) NOT NULL,
    config JSONB NOT NULL DEFAULT '{}',
    enabled BOOLEAN DEFAULT true,
    last_checked TIMESTAMPTZ,
    last_populated TIMESTAMPTZ,
    created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(doc_type, source_name)
);
```

### Performance Requirements

#### Current Performance
- Query response time: ~2-3 seconds (no vector index due to dimension limits)
- Concurrent connections: Tested with basic load
- Memory usage: Moderate with current dataset

#### Target Performance
- **Query Response**: < 2 seconds for semantic search
- **Concurrent Connections**: Support 100+ simultaneous connections
- **Batch Processing**: 100 embeddings per OpenAI request
- **Rate Limiting**: Compliant with OpenAI limits (3000 RPM / 1M TPM)
- **Uptime**: 99.9% availability target

### Integration Requirements

#### Current Integration Status
- üîÑ **Toolman**: Primary client with deprecated HTTP/SSE transport (MUST migrate to Streamable HTTP)
- üîÑ **Cursor**: MCP integration exists but needs cluster deployment validation
- üîÑ **PostgreSQL**: Database with pgvector extension (needs cluster connection setup)
- üîÑ **OpenAI**: Embedding generation and LLM queries (needs production environment configuration)

#### Required Enhancements
- **Kubernetes Deployment**: Production-ready container orchestration
- **Monitoring**: Health checks and system metrics with Prometheus/Grafana
- **Logging**: Structured logging with tracing for cluster environments
- **Error Handling**: Comprehensive error recovery and circuit breakers

## Implementation Priorities

### Phase 1: System Evaluation (Week 1)
**Priority**: Immediate (Task 35)
- Comprehensive system assessment by new implementation agent
- Verify existing functionality against production requirements
- Test Kubernetes cluster connectivity and database access
- Validate current MCP server implementation against Streamable HTTP requirements
- Document current state, identify migration requirements, and plan cluster deployment

### Phase 2: MCP Transport Migration (Week 2)
**Priority**: High (Task 17)
- Migrate from deprecated HTTP+SSE to Streamable HTTP transport (MCP 2025-06-18)
- Implement unified MCP endpoint with POST/GET support
- Add session management with Mcp-Session-Id headers
- Implement protocol version header compliance
- Add resumable connection support with SSE event IDs
- Enhance security with Origin header validation
- Test with Toolman under various network conditions

### Phase 3: Query Tool Implementation (Week 3-4)
**Priority**: Medium (Tasks 20-25)
- Implement `birdeye_query` and `solana_query` tools
- Complete Rust crate management tools
- Add remaining query tools for all documentation types
- Integration testing with Cursor and Toolman

### Phase 4: Optimization (Week 5-6)
**Priority**: Medium (Tasks 21, 28, 29)
- OpenAI batch processing implementation
- Rate limiting and API protection
- Performance optimization and scaling
- Documentation ingestion pipeline automation

### Phase 5: Advanced Features (Week 7+)
**Priority**: Low (Tasks 26, 30)
- Unified search across all documentation types
- Comprehensive integration testing
- Production deployment preparation
- Monitoring and alerting implementation

## Success Criteria

### Previous Work (Requires Production Validation)
1. üîÑ All 40 existing Rust crates data exists (needs cluster deployment validation)
2. üîÑ Schema migration completed locally (needs production database validation)
3. üîÑ MCP server exists with deprecated HTTP/SSE transport (MUST migrate to Streamable HTTP)
4. üîÑ Local development environment existed (needs Kubernetes deployment workflow)
5. üîÑ BirdEye and Solana documentation ingested locally (needs cluster data migration)

### Remaining Success Criteria
1. **MCP Transport Compliance**: Full migration to Streamable HTTP transport (MCP 2025-06-18)
2. **Connection Reliability**: Stable session management and resumable connections
3. **Cost Optimization**: 70% cost reduction via OpenAI batching
4. **Query Tools**: All 10 documentation types with working query tools
5. **Performance**: Query performance within 10% of original system
6. **Integration**: Stable Toolman integration under production load

## Constraints

### Technical Constraints
1. **Database**: Must maintain PostgreSQL with pgvector for embeddings
2. **Embeddings**: OpenAI text-embedding-3-large (3072 dimensions)
3. **Vector Index**: No pgvector index due to 2000-dimension limit
4. **Dynamic Management**: Only Rust crates support MCP-based management
5. **Tool Naming**: Must use specific names, not generic terms

### Business Constraints
1. **Backward Compatibility**: Existing Rust functionality must be preserved
2. **Data Migration**: No data loss during any future schema changes
3. **Cost Management**: Optimize OpenAI API usage through batching
4. **Development Speed**: Prioritize connection reliability over new features

## Dependencies

### Infrastructure Dependencies
- üîÑ PostgreSQL with pgvector extension (3072-dimension vectors) - cluster ready, needs connection
- üîÑ OpenAI API access and authentication (text-embedding-3-large) - needs production environment setup
- üîÑ Kubernetes cluster infrastructure - available, needs application deployment
- üîÑ Rust toolchain and cargo workspace (Axum-based HTTP server) - needs Streamable HTTP migration
- üîÑ Production container registry and CI/CD pipeline - ready for integration

### Additional Requirements
- **Toolman Production Integration**: Live testing with Toolman against cluster deployment
- **Monitoring Infrastructure**: Prometheus/Grafana stack for production observability
- **Database Performance**: Optimized PostgreSQL configuration for vector workloads
- **Load Balancing**: Ingress controllers for high-availability deployment

## Risks and Mitigation

### Technical Risks

1. **MCP Transport Compatibility Risk**
   - **Risk**: Current implementation uses deprecated HTTP+SSE transport, may cause compatibility issues
   - **Mitigation**: Priority migration to Streamable HTTP transport (MCP 2025-06-18) (Task 17)
   - **Status**: High priority for Phase 2

2. **Performance Risk**
   - **Risk**: Slower queries without vector indexing
   - **Mitigation**: Optimize query patterns and consider alternative indexing
   - **Status**: Monitor and optimize in Phase 4

3. **Cost Risk**
   - **Risk**: High OpenAI API costs without batching
   - **Mitigation**: Implement batch processing for 70% cost reduction
   - **Status**: Planned for Phase 4

### Business Risks

1. **Integration Risk**
   - **Risk**: Breaking changes affecting Toolman compatibility
   - **Mitigation**: Extensive testing with each change
   - **Status**: Continuous monitoring required

2. **Data Risk**
   - **Risk**: Data loss during future schema modifications
   - **Mitigation**: Comprehensive backup strategy and staging environment
   - **Status**: Database dump created for preservation

## Future Considerations

### Extensibility
- Architecture supports addition of new documentation types
- Loader pattern allows custom ingestion for various formats
- JSONB metadata provides flexibility for type-specific data

### Scalability
- Horizontal scaling capability with proper load balancing
- Vector search optimization as dataset grows
- Caching layer for frequently accessed content

### Advanced Features
- Cross-documentation linking and relationships
- Multi-language support for international documentation
- Real-time documentation updates and synchronization
- Advanced query features (filtering, faceting, aggregation)

## References and Documentation

### Technical Specifications

#### Model Context Protocol (MCP) Specification
- **MCP Protocol Version**: 2025-06-18
- **Transport Specification**: [.reference/transports.md](../.reference/transports.md)
- **Schema Definition**: [.reference/mcp-spec/modelcontextprotocol/schema/2025-06-18/schema.ts](../.reference/mcp-spec/modelcontextprotocol/schema/2025-06-18/schema.ts)
- **Official MCP Documentation**: https://spec.modelcontextprotocol.io/

#### Key Transport Requirements
- **Streamable HTTP Transport**: Replaces deprecated HTTP+SSE transport (protocol version 2024-11-05)
- **Single MCP Endpoint**: Unified endpoint supporting both POST and GET methods
- **Session Management**: `Mcp-Session-Id` header support for stateful sessions
- **Protocol Version Headers**: `MCP-Protocol-Version` header compliance required
- **Security Requirements**: Origin header validation, localhost binding, proper authentication

### Project Documentation

#### Current Project Documents
- **Product Requirements Document**: [.taskmaster/docs/prd.txt](./prd.txt) (this document)
- **Architecture Documentation**: [.taskmaster/docs/architecture.md](./architecture.md)
- **Deployment Workflow**: [.github/workflows/deploy-doc-server.yml](../.github/workflows/deploy-doc-server.yml)

#### Database and API Specifications
- **PostgreSQL with pgvector**: Vector database for 3072-dimension embeddings
- **OpenAI API**: text-embedding-3-large model for document embeddings
- **Vector Dimensions**: 3072 (OpenAI text-embedding-3-large standard)
- **Rate Limits**: 3,000 RPM / 1M TPM for OpenAI API compliance

### Compliance and Standards

-#### MCP Transport Compliance Checklist
- [ ] **Single MCP Endpoint (MVP)**: Implement unified `/mcp` endpoint supporting POST only (GET returns 405)
- [ ] **Protocol Version Header**: Include `MCP-Protocol-Version: 2025-06-18` in all responses
- [ ] **Session Management**: Support `Mcp-Session-Id` headers for stateful connections
- [ ] **SSE Event IDs**: (Post-MVP)
- [ ] **Security Headers**: Validate `Origin` header to prevent DNS rebinding attacks
- [ ] **Error Handling**: Return proper JSON-RPC error responses per MCP specification

#### Kubernetes Deployment Standards
- [ ] **Container Security**: Non-root user, minimal base image, security scanning
- [ ] **Resource Limits**: CPU and memory limits defined in Helm chart
- [ ] **Health Checks**: Liveness and readiness probes configured
- [ ] **Service Mesh**: Compatible with Istio/Linkerd if deployed
- [ ] **Monitoring**: Prometheus metrics endpoint exposed
- [ ] **Logging**: Structured JSON logging with correlation IDs

### Development and Testing References

#### Validation Process Documentation
- **GitHub Actions Workflow**: Automated build, test, and deployment pipeline
- **Clippy Configuration**: [clippy.toml](../clippy.toml) - Rust linting rules
- **Rust Formatting**: [rustfmt.toml](../rustfmt.toml) - Code formatting standards
- **Container Build**: [Dockerfile](../Dockerfile) - Multi-stage container build process

#### Testing Requirements
- **Unit Tests**: Cargo test suite with full feature coverage
- **Integration Tests**: MCP protocol compliance testing
- **Production Validation**: Real-world testing against Kubernetes cluster
- **Performance Benchmarks**: Query response time < 2 seconds target

### External Dependencies and APIs

#### Required External Services
- **OpenAI API**: Text embedding generation service
- **PostgreSQL Cluster**: Production database with pgvector extension
- **Container Registry**: GitHub Container Registry (GHCR) for image storage
- **Kubernetes Cluster**: Production deployment environment

#### Development Tools and Environment
- **Rust Toolchain**: Stable channel with Clippy and rustfmt components
- **Docker/Podman**: Container runtime for local development and CI/CD
- **Helm**: Kubernetes package manager for deployment orchestration
- **kubectl**: Kubernetes command-line tool for cluster management

## Definition of Done

### For Each Query Tool
- [ ] Tool implemented and registered in MCP server
- [ ] Semantic search working with vector embeddings
- [ ] Metadata filtering and result ranking
- [ ] Integration tested with Cursor
- [ ] Performance benchmarked and optimized
- [ ] Documentation and examples provided

### For System Reliability
- [ ] Streamable HTTP transport implemented and tested
- [ ] Connection recovery working under network stress
- [ ] Health monitoring providing detailed system status
- [ ] Error handling graceful and informative
- [ ] Load testing completed with 100+ concurrent connections

### For Production Readiness
- [ ] Kubernetes deployment optimized for production cluster
- [ ] Monitoring and alerting configured (Prometheus/Grafana)
- [ ] Security review completed (authentication, authorization, network policies)
- [ ] Performance benchmarks documented against production database
- [ ] CI/CD pipeline working (GitHub ‚Üí Build ‚Üí K8s ‚Üí Test)
- [ ] Backup and recovery procedures tested in cluster environment